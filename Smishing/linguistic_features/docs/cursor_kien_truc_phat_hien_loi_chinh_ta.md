# Kiáº¿n trÃºc phÃ¡t hiá»‡n lá»—i chÃ­nh táº£
_Exported on 12/17/2025 at 16:25:58 GMT+7 from Cursor (2.2.20)_

---

**User**

@data/dataset.csv Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng má»™t cÃ¡ch hiá»‡u quáº£ vÃ  chÃ­nh xÃ¡c Ä‘á»ƒ giáº£i quyáº¿t Ä‘áº·c trÆ°ng "lá»—i chÃ­nh táº£", báº¡n cÃ³ toÃ n quyá»n tham kháº£o vÃ  thiáº¿t káº¿ Ä‘á» xuáº¥t dá»±a trÃªn kiáº¿n trÃºc nÃ y: 
Kiáº¿n trÃºc PhÃ¢n táº§ng (Layered Architecture)
Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng theo mÃ´ hÃ¬nh cÃ¡i phá»…u 4 táº§ng. Dá»¯ liá»‡u Ä‘i tá»« trÃªn xuá»‘ng, táº§ng trÃªn xá»­ lÃ½ xong má»›i chuyá»n xuá»‘ng táº§ng dÆ°á»›i.

Táº§ng 1: Aggressive Masking (Che giáº¥u thá»±c thá»ƒ hung hÄƒng)
Nhiá»‡m vá»¥: TÃ¬m vÃ  thay tháº¿ cÃ¡c thá»±c thá»ƒ phi ngÃ´n ngá»¯ báº±ng token chung.

CÃ´ng cá»¥: Regex (Biá»ƒu thá»©c chÃ­nh quy).

YÃªu cáº§u Ä‘áº·c biá»‡t: Pháº£i "hung hÄƒng" (Aggressive) Ä‘á»ƒ báº¯t cáº£ cÃ¡c biáº¿n thá»ƒ cá»‘ tÃ¬nh viáº¿t sai cá»§a Spammer.

CÃ¡c token báº¯t buá»™c:

<URL>: Báº¯t cáº£ http, www, bit.ly, banca . com, [ domain . com ].

<PHONE>: Báº¯t cáº£ 09xxx, 84xxx, 09xx.xxx.xxx.

<CODE>: Báº¯t chuá»—i in hoa kÃ¨m sá»‘ (ST5K, OTP123, V120N).

<MONEY>: Báº¯t sá»‘ tiá»n (100k, 500.000d, 1 triá»‡u).

<DATE/TIME> (TÃ¹y chá»n): 15/05, 10h30.

Táº§ng 2: Tokenization & Normalization (Cáº¯t tá»« & Chuáº©n hÃ³a)
Nhiá»‡m vá»¥: Chia cÃ¢u thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ cÃ³ nghÄ©a Ä‘á»ƒ kiá»ƒm tra.

CÃ´ng cá»¥:

ThÆ° viá»‡n tÃ¡ch tá»« tiáº¿ng Viá»‡t (pyvi hoáº·c underthesea) Ä‘á»ƒ giá»¯ ngá»¯ nghÄ©a tá»« ghÃ©p (vÃ­ dá»¥: ngÃ¢n_hÃ ng thay vÃ¬ ngÃ¢n, hÃ ng).

HÃ m chuáº©n hÃ³a Unicode (Ä‘Æ°a vá» dá»±ng sáºµn NFC).

YÃªu cáº§u: Xá»­ lÃ½ cÃ¡c tá»« dÃ­nh nhau (Glue words) náº¿u Regex Táº§ng 1 chÆ°a báº¯t háº¿t (vÃ­ dá»¥: ngÃ y/Ä‘Ãªm -> ngÃ y / Ä‘Ãªm).

Táº§ng 3: Whitelist Filtering (Bá»™ lá»c ngoáº¡i lá»‡)
Nhiá»‡m vá»¥: Bá» qua cÃ¡c tá»« "Ä‘áº·c thÃ¹" khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn nhÆ°ng há»£p lá»‡ trong tin nháº¯n.

Cáº¥u trÃºc dá»¯ liá»‡u: Set (Hash Set) Ä‘á»ƒ tra cá»©u O(1).

3 Danh sÃ¡ch cáº§n cÃ³:

Brand List: TÃªn thÆ°Æ¡ng hiá»‡u, ngÃ¢n hÃ ng, vÃ­ Ä‘iá»‡n tá»­ (shopee, momo, vcb).

Jargon List: Thuáº­t ngá»¯ viá»…n thÃ´ng/marketing (data, 4g, voucher, deal).

Slang/Abbr List: Viáº¿t táº¯t, teencode (lh, qc, nt, ko, dc).

Táº§ng 4: Dual-Lookup Validation (Tra cá»©u kÃ©p)
Nhiá»‡m vá»¥: Kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a tá»« tiáº¿ng Viá»‡t, cháº¥p nháº­n sá»± nháº­p nháº±ng cÃ³ dáº¥u/khÃ´ng dáº¥u.

CÆ¡ cháº¿:

Check 1: CÃ³ trong Vietnamese_Full_Dict (CÃ³ dáº¥u) khÃ´ng?

Check 2: CÃ³ trong Vietnamese_Shadow_Dict (KhÃ´ng dáº¥u) khÃ´ng?

Káº¿t quáº£: Náº¿u trÆ°á»£t cáº£ 2 -> ÄÃ¡nh dáº¥u lÃ  Misspell / Unknown Token.


Vá» máº·t kiáº¿n trÃºc, cÃ³ thá»ƒ gá»£i Ã½ Ä‘iá»u chá»‰nh náº¿u cáº£m tháº¥y khÃ´ng phÃ¹ há»£p hoáº·c khÃ´ng cáº§n thiáº¿t. cÃ²n vá» tÃ i nguyÃªn yÃªu cáº§u bÃªn trong thÃ¬ khÃ´ng pháº£i váº¥n Ä‘á». 

---

**Cursor**

Äá»ƒ Ä‘Æ°a ra Ä‘á» xuáº¥t phÃ¹ há»£p, tÃ´i cáº§n xem qua cáº¥u trÃºc vÃ  ná»™i dung cá»§a dataset trÆ°á»›c.

Äá»ƒ hiá»ƒu rÃµ hÆ¡n vá» dá»¯ liá»‡u vÃ  cÃ¡c tÃ i nguyÃªn hiá»‡n cÃ³, tÃ´i cáº§n xem thÃªm má»™t sá»‘ file:

BÃ¢y giá» tÃ´i Ä‘Ã£ hiá»ƒu rÃµ dá»¯ liá»‡u vÃ  tÃ i nguyÃªn hiá»‡n cÃ³. Dá»±a trÃªn phÃ¢n tÃ­ch dataset vÃ  kiáº¿n trÃºc báº¡n Ä‘á» xuáº¥t, tÃ´i sáº½ Ä‘Æ°a ra **Ä‘Ã¡nh giÃ¡ chi tiáº¿t vÃ  Ä‘á» xuáº¥t Ä‘iá»u chá»‰nh**:

---

## ğŸ“Š PhÃ¢n TÃ­ch Äáº·c Äiá»ƒm Dá»¯ Liá»‡u

Tá»« dataset, tÃ´i nháº­n diá»‡n Ä‘Æ°á»£c **5 nhÃ³m lá»—i chÃ­nh táº£ chÃ­nh** trong tin nháº¯n spam/smishing:

| NhÃ³m | VÃ­ dá»¥ | Táº§n suáº¥t |
|------|-------|----------|
| **1. Leetspeak** | `kh0ng`, `d!eu k!en`, `DU0C` | Ráº¥t cao |
| **2. Viáº¿t khÃ´ng dáº¥u** | `dieu kien`, `vui long`, `tai khoan` | Ráº¥t cao |
| **3. Gibberish/Random** | `tORKiM! ay:Ma""n;N,ha7lXklq` | Trung bÃ¬nh |
| **4. Teencode/Viáº¿t táº¯t** | `lh`, `qc`, `dc`, `tt` | Trung bÃ¬nh |
| **5. Biáº¿n thá»ƒ domain giáº£** | `vcbielbiok`, `vniatinbanks` | Tháº¥p |

---

## âœ… ÄÃ¡nh GiÃ¡ Kiáº¿n TrÃºc 4 Táº§ng Äá» Xuáº¥t

### Táº§ng 1: Aggressive Masking âœ“ **Äá»’NG Ã - Cáº¦N THIáº¾T**
ÄÃ¢y lÃ  táº§ng **ráº¥t quan trá»ng** vÃ¬ nÃ³:
- Loáº¡i bá» nhiá»…u tá»« cÃ¡c thá»±c thá»ƒ phi ngÃ´n ngá»¯
- Giáº£m false positive (URL, sá»‘ tiá»n khÃ´ng pháº£i lá»—i chÃ­nh táº£)

**Äá» xuáº¥t bá»• sung regex patterns:**

```python
# Patterns cáº§n "hung hÄƒng" cho smishing Viá»‡t Nam
PATTERNS = {
    '<URL>': r'(?:https?://|www\.|bit\.ly|[a-z0-9-]+\s*\.\s*(?:com|vn|top|xyz|icu|vip|cc|life|club|cfd))',
    '<PHONE>': r'(?:\+?84|0)\s*\d[\d\s\.\-]{7,12}',
    '<CODE>': r'\b[A-Z0-9]{3,}[A-Z][0-9]+|[0-9]+[A-Z]{2,}\b',  # ST5K, OTP123
    '<MONEY>': r'\d{1,3}(?:[.,]\d{3})*\s*(?:k|tr|triá»‡u|trieu|VND|VNÄ|dong|Ä‘á»“ng|d)\b',
    '<ZALO>': r'zalo\.me/\d+|zalo:\s*\d+',
}
```

### Táº§ng 2: Tokenization & Normalization âœ“ **Äá»’NG Ã - Cáº¦N THIáº¾T**

**Tuy nhiÃªn**, tÃ´i Ä‘á» xuáº¥t **THÃŠM 1 bÆ°á»›c trÆ°á»›c khi tokenize**: **Leetspeak Normalization**

```
Luá»“ng Ä‘á» xuáº¥t:
Raw Text â†’ Leet Decode â†’ Unicode NFC â†’ Word Tokenization
```

LÃ½ do: Leetspeak xuáº¥t hiá»‡n **ráº¥t nhiá»u** trong dataset (`kh0ng`, `d!eu`, `h0 tro`). Náº¿u decode trÆ°á»›c khi tokenize sáº½ giÃºp:
- pyvi/underthesea nháº­n diá»‡n tá»« ghÃ©p tá»‘t hÆ¡n
- Giáº£m sá»‘ lÆ°á»£ng "unknown tokens" á»Ÿ Táº§ng 4

### Táº§ng 3: Whitelist Filtering âœ“ **Äá»’NG Ã - Cáº¦N THIáº¾T**

3 danh sÃ¡ch Ä‘á» xuáº¥t há»£p lÃ½. **Bá»• sung thÃªm**:

| Danh sÃ¡ch | VÃ­ dá»¥ cáº§n cÃ³ |
|-----------|--------------|
| **Bank/Finance** | `vcb`, `acb`, `bidv`, `vpbank`, `techcombank`, `momo`, `zalopay` |
| **Telco** | `viettel`, `vinaphone`, `mobifone`, `4g`, `5g`, `data`, `sim` |
| **E-commerce** | `shopee`, `lazada`, `tiki`, `grab`, `gojek` |
| **Common abbr** | `qc`, `lh`, `nt`, `ko`, `dc`, `ns`, `ib`, `ms`, `cskh` |

### Táº§ng 4: Dual-Lookup Validation âœ“ **Äá»’NG Ã NHÆ¯NG Cáº¦N ÄIá»€U CHá»ˆNH**

ÄÃ¢y lÃ  Ä‘iá»ƒm **cáº§n cÃ¢n nháº¯c ká»¹ nháº¥t**:

**Váº¥n Ä‘á» vá»›i cÃ¡ch tiáº¿p cáº­n hiá»‡n táº¡i:**
- Tá»« Ä‘iá»ƒn `words.txt` cÃ³ ~79,000 tá»« **cÃ³ dáº¥u** 
- SMS spam thÆ°á»ng **khÃ´ng cÃ³ dáº¥u** â†’ Háº§u háº¿t tá»« sáº½ "trÆ°á»£t" Check 1
- Shadow Dict (khÃ´ng dáº¥u) sáº½ cÃ³ **ráº¥t nhiá»u collision** (vd: `an` â†’ Äƒn, Ã¡n, Ã n, áº£n, Ã£n, áº¡n...)

---

## ğŸ”„ Äá»€ XUáº¤T KIáº¾N TRÃšC Cáº¢I TIáº¾N

TÃ´i Ä‘á» xuáº¥t **kiáº¿n trÃºc 5 táº§ng** vá»›i viá»‡c tÃ¡ch riÃªng xá»­ lÃ½ Leetspeak:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: Raw SMS Text                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 1: AGGRESSIVE MASKING                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Regex báº¯t URL, Phone, Money, Code, Date/Time              â”‚
â”‚ â€¢ Output: Text vá»›i <ENTITY> tokens                          â”‚
â”‚ â€¢ Bonus: Äáº¿m sá»‘ entity â†’ Feature cho ML                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 2: LEETSPEAK DETECTION & DECODE  â† [Má»šI]              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Detect patterns: 0â†’o, 1â†’i, !â†’i, 3â†’e, @â†’a, $â†’s            â”‚
â”‚ â€¢ Output 1: leet_count (sá»‘ kÃ½ tá»± bá»‹ thay)                   â”‚
â”‚ â€¢ Output 2: Decoded text (cho cÃ¡c táº§ng sau)                 â”‚
â”‚ â€¢ QUAN TRá»ŒNG: Giá»¯ láº¡i báº£n gá»‘c Ä‘á»ƒ Ä‘áº¿m leet riÃªng            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 3: TOKENIZATION + NORMALIZATION                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Unicode NFC normalization                                 â”‚
â”‚ â€¢ pyvi/underthesea word segmentation                        â”‚
â”‚ â€¢ Xá»­ lÃ½ tá»« dÃ­nh: "ngÃ y/Ä‘Ãªm" â†’ "ngÃ y / Ä‘Ãªm"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 4: WHITELIST FILTERING                                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Brand names (ngÃ¢n hÃ ng, vÃ­, telco, ecommerce)            â”‚
â”‚ â€¢ Technical jargon (otp, data, voucher)                     â”‚
â”‚ â€¢ Common abbreviations & teencode                           â”‚
â”‚ â€¢ Entity tokens (<URL>, <PHONE>, etc.)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 5: VIETNAMESE VALIDATION + ACCENT RESTORATION          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ STRATEGY A (Fast - Rule-based):                             â”‚
â”‚   â€¢ Check Vietnamese_Shadow_Dict (no-accent)                â”‚
â”‚   â€¢ Tá»« "cÃ³ thá»ƒ cÃ³ dáº¥u" â†’ VALID                              â”‚
â”‚   â€¢ Tá»« khÃ´ng match â†’ MISSPELL                               â”‚
â”‚                                                             â”‚
â”‚ STRATEGY B (Accurate - Hybrid):                             â”‚
â”‚   â€¢ DÃ¹ng XLM-RoBERTa restore accent trÆ°á»›c                   â”‚
â”‚   â€¢ Check Full_Dict vá»›i tá»« Ä‘Ã£ cÃ³ dáº¥u                        â”‚
â”‚   â€¢ So sÃ¡nh original vs restored â†’ Detect intentional       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT FEATURES:                                            â”‚
â”‚ â€¢ misspelled_count, misspelled_pct                          â”‚
â”‚ â€¢ leet_count, leet_pct                                      â”‚
â”‚ â€¢ unknown_token_count (gibberish)                           â”‚
â”‚ â€¢ entity_counts (url, phone, money, code)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Giáº£i ThÃ­ch Chi Tiáº¿t CÃ¡c Äiá»u Chá»‰nh

### 1. TÃ¡ch Leetspeak thÃ nh táº§ng riÃªng (Táº§ng 2)

**LÃ½ do:**
- Leetspeak lÃ  **Ä‘áº·c trÆ°ng máº¡nh** phÃ¢n biá»‡t spam vs ham (tin thÆ°á»ng khÃ´ng cÃ³)
- Cáº§n **Ä‘áº¿m riÃªng** leet_count nhÆ° feature cho ML model
- Decode trÆ°á»›c giÃºp cÃ¡c táº§ng sau hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c hÆ¡n

```python
LEET_MAP = {
    '0': 'o', '1': 'i', '!': 'i', '3': 'e', 
    '4': 'a', '@': 'a', '$': 's', '5': 's',
    '7': 't', '8': 'b', '9': 'g'
}

def decode_leet(text):
    leet_count = 0
    result = []
    for char in text:
        if char in LEET_MAP:
            result.append(LEET_MAP[char])
            leet_count += 1
        else:
            result.append(char)
    return ''.join(result), leet_count
```

### 2. Chiáº¿n lÆ°á»£c Dual-Dict cáº£i tiáº¿n (Táº§ng 5)

**Váº¥n Ä‘á» collision cá»§a Shadow Dict:**
```
"an" (khÃ´ng dáº¥u) â†’ cÃ³ thá»ƒ lÃ : Äƒn, Ã¡n, Ã n, áº£n, Ã£n, áº¡n, an
```

**Giáº£i phÃ¡p: "CÃ³ thá»ƒ há»£p lá»‡" thay vÃ¬ "Cháº¯c cháº¯n há»£p lá»‡"**

```python
def build_shadow_dict(full_dict):
    """
    Táº¡o shadow dict: no_accent_form â†’ set of possible accented forms
    """
    shadow = defaultdict(set)
    for word in full_dict:
        no_accent = remove_accents(word.lower())
        shadow[no_accent].add(word)
    return shadow

def validate_word(word, full_dict, shadow_dict, whitelist):
    word_lower = word.lower()
    no_accent = remove_accents(word_lower)
    
    # Check 1: Whitelist (brands, jargon, abbr)
    if word_lower in whitelist:
        return 'VALID_WHITELIST'
    
    # Check 2: Full dict (cÃ³ dáº¥u)
    if word_lower in full_dict:
        return 'VALID_FULL'
    
    # Check 3: Shadow dict (khÃ´ng dáº¥u nhÆ°ng CÃ“ THá»‚ há»£p lá»‡)
    if no_accent in shadow_dict:
        return 'VALID_SHADOW'  # Cháº¥p nháº­n - cÃ³ thá»ƒ lÃ  tá»« khÃ´ng dáº¥u
    
    return 'MISSPELL'  # KhÃ´ng match gÃ¬ cáº£
```

### 3. Xá»­ lÃ½ Gibberish (Random text)

Tá»« dataset, tÃ´i tháº¥y cÃ³ tin nháº¯n dáº¡ng:
```
"tORKiM! ay:Ma""n;N,ha7lXklq,uoacx.tech*;G^ja*nh$ap!"
```

**Äá» xuáº¥t thÃªm Gibberish Detector:**

```python
def is_gibberish(token):
    """
    PhÃ¡t hiá»‡n token gibberish dá»±a trÃªn:
    1. Tá»· lá»‡ kÃ½ tá»± Ä‘áº·c biá»‡t cao
    2. Pattern khÃ´ng tá»± nhiÃªn (consonant clusters báº¥t thÆ°á»ng)
    3. Äá»™ dÃ i báº¥t thÆ°á»ng
    """
    special_ratio = len(re.findall(r'[^a-zA-Z0-9\s]', token)) / max(len(token), 1)
    if special_ratio > 0.3:
        return True
    
    # Vietnamese khÃ´ng cÃ³ 3+ consonants liÃªn tiáº¿p
    if re.search(r'[bcdfghjklmnpqrstvwxz]{4,}', token.lower()):
        return True
    
    return False
```

---

## ğŸ“‹ TÃ³m Táº¯t Äá» Xuáº¥t

| Kiáº¿n trÃºc gá»‘c | Äá» xuáº¥t Ä‘iá»u chá»‰nh |
|---------------|-------------------|
| 4 táº§ng | **5 táº§ng** (thÃªm Leetspeak riÃªng) |
| Dual-Lookup binary | **Tri-state validation** (Full/Shadow/Misspell) |
| KhÃ´ng cÃ³ gibberish | **ThÃªm gibberish detector** |
| Tokenize trÆ°á»›c | **Leet decode â†’ Tokenize** |

**Output features Ä‘á» xuáº¥t:**
- `misspelled_count` / `misspelled_pct`
- `leet_count` / `leet_pct` 
- `gibberish_count`
- `entity_counts` (dictionary vá»›i tá»«ng loáº¡i entity)

---

Báº¡n muá»‘n tÃ´i giáº£i thÃ­ch chi tiáº¿t hÆ¡n pháº§n nÃ o hoáº·c cung cáº¥p pseudo-code/skeleton cho module cá»¥ thá»ƒ khÃ´ng?

```python
# Patterns cáº§n "hung hÄƒng" cho smishing Viá»‡t Nam
PATTERNS = {
    '<URL>': r'(?:https?://|www\.|bit\.ly|[a-z0-9-]+\s*\.\s*(?:com|vn|top|xyz|icu|vip|cc|life|club|cfd))',
    '<PHONE>': r'(?:\+?84|0)\s*\d[\d\s\.\-]{7,12}',
    '<CODE>': r'\b[A-Z0-9]{3,}[A-Z][0-9]+|[0-9]+[A-Z]{2,}\b',  # ST5K, OTP123
    '<MONEY>': r'\d{1,3}(?:[.,]\d{3})*\s*(?:k|tr|triá»‡u|trieu|VND|VNÄ|dong|Ä‘á»“ng|d)\b',
    '<ZALO>': r'zalo\.me/\d+|zalo:\s*\d+',
}
```

```plaintext
Luá»“ng Ä‘á» xuáº¥t:
Raw Text â†’ Leet Decode â†’ Unicode NFC â†’ Word Tokenization
```

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: Raw SMS Text                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 1: AGGRESSIVE MASKING                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Regex báº¯t URL, Phone, Money, Code, Date/Time              â”‚
â”‚ â€¢ Output: Text vá»›i <ENTITY> tokens                          â”‚
â”‚ â€¢ Bonus: Äáº¿m sá»‘ entity â†’ Feature cho ML                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 2: LEETSPEAK DETECTION & DECODE  â† [Má»šI]              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Detect patterns: 0â†’o, 1â†’i, !â†’i, 3â†’e, @â†’a, $â†’s            â”‚
â”‚ â€¢ Output 1: leet_count (sá»‘ kÃ½ tá»± bá»‹ thay)                   â”‚
â”‚ â€¢ Output 2: Decoded text (cho cÃ¡c táº§ng sau)                 â”‚
â”‚ â€¢ QUAN TRá»ŒNG: Giá»¯ láº¡i báº£n gá»‘c Ä‘á»ƒ Ä‘áº¿m leet riÃªng            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 3: TOKENIZATION + NORMALIZATION                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Unicode NFC normalization                                 â”‚
â”‚ â€¢ pyvi/underthesea word segmentation                        â”‚
â”‚ â€¢ Xá»­ lÃ½ tá»« dÃ­nh: "ngÃ y/Ä‘Ãªm" â†’ "ngÃ y / Ä‘Ãªm"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 4: WHITELIST FILTERING                                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Brand names (ngÃ¢n hÃ ng, vÃ­, telco, ecommerce)            â”‚
â”‚ â€¢ Technical jargon (otp, data, voucher)                     â”‚
â”‚ â€¢ Common abbreviations & teencode                           â”‚
â”‚ â€¢ Entity tokens (<URL>, <PHONE>, etc.)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº¦NG 5: VIETNAMESE VALIDATION + ACCENT RESTORATION          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ STRATEGY A (Fast - Rule-based):                             â”‚
â”‚   â€¢ Check Vietnamese_Shadow_Dict (no-accent)                â”‚
â”‚   â€¢ Tá»« "cÃ³ thá»ƒ cÃ³ dáº¥u" â†’ VALID                              â”‚
â”‚   â€¢ Tá»« khÃ´ng match â†’ MISSPELL                               â”‚
â”‚                                                             â”‚
â”‚ STRATEGY B (Accurate - Hybrid):                             â”‚
â”‚   â€¢ DÃ¹ng XLM-RoBERTa restore accent trÆ°á»›c                   â”‚
â”‚   â€¢ Check Full_Dict vá»›i tá»« Ä‘Ã£ cÃ³ dáº¥u                        â”‚
â”‚   â€¢ So sÃ¡nh original vs restored â†’ Detect intentional       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT FEATURES:                                            â”‚
â”‚ â€¢ misspelled_count, misspelled_pct                          â”‚
â”‚ â€¢ leet_count, leet_pct                                      â”‚
â”‚ â€¢ unknown_token_count (gibberish)                           â”‚
â”‚ â€¢ entity_counts (url, phone, money, code)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
LEET_MAP = {
    '0': 'o', '1': 'i', '!': 'i', '3': 'e', 
    '4': 'a', '@': 'a', '$': 's', '5': 's',
    '7': 't', '8': 'b', '9': 'g'
}

def decode_leet(text):
    leet_count = 0
    result = []
    for char in text:
        if char in LEET_MAP:
            result.append(LEET_MAP[char])
            leet_count += 1
        else:
            result.append(char)
    return ''.join(result), leet_count
```

```plaintext
"an" (khÃ´ng dáº¥u) â†’ cÃ³ thá»ƒ lÃ : Äƒn, Ã¡n, Ã n, áº£n, Ã£n, áº¡n, an
```

```python
def build_shadow_dict(full_dict):
    """
    Táº¡o shadow dict: no_accent_form â†’ set of possible accented forms
    """
    shadow = defaultdict(set)
    for word in full_dict:
        no_accent = remove_accents(word.lower())
        shadow[no_accent].add(word)
    return shadow

def validate_word(word, full_dict, shadow_dict, whitelist):
    word_lower = word.lower()
    no_accent = remove_accents(word_lower)
    
    # Check 1: Whitelist (brands, jargon, abbr)
    if word_lower in whitelist:
        return 'VALID_WHITELIST'
    
    # Check 2: Full dict (cÃ³ dáº¥u)
    if word_lower in full_dict:
        return 'VALID_FULL'
    
    # Check 3: Shadow dict (khÃ´ng dáº¥u nhÆ°ng CÃ“ THá»‚ há»£p lá»‡)
    if no_accent in shadow_dict:
        return 'VALID_SHADOW'  # Cháº¥p nháº­n - cÃ³ thá»ƒ lÃ  tá»« khÃ´ng dáº¥u
    
    return 'MISSPELL'  # KhÃ´ng match gÃ¬ cáº£
```

```plaintext
"tORKiM! ay:Ma""n;N,ha7lXklq,uoacx.tech*;G^ja*nh$ap!"
```

```python
def is_gibberish(token):
    """
    PhÃ¡t hiá»‡n token gibberish dá»±a trÃªn:
    1. Tá»· lá»‡ kÃ½ tá»± Ä‘áº·c biá»‡t cao
    2. Pattern khÃ´ng tá»± nhiÃªn (consonant clusters báº¥t thÆ°á»ng)
    3. Äá»™ dÃ i báº¥t thÆ°á»ng
    """
    special_ratio = len(re.findall(r'[^a-zA-Z0-9\s]', token)) / max(len(token), 1)
    if special_ratio > 0.3:
        return True
    
    # Vietnamese khÃ´ng cÃ³ 3+ consonants liÃªn tiáº¿p
    if re.search(r'[bcdfghjklmnpqrstvwxz]{4,}', token.lower()):
        return True
    
    return False
```

---

**User**

á» táº§ng 4, tÃ´i Ä‘á» xuáº¥t má»™t quy trÃ¬nh káº¿t há»£p giá»¯a cáº£ 2 loáº¡i tá»« Ä‘iá»ƒn, kiá»ƒu Ä‘áº§u tiÃªn sáº½ kiá»ƒm tra qua tá»« Ä‘iá»ƒn cÃ³ dáº¥u, náº¿u khÃ´ng cÃ³ thÃ¬ sáº½ kiá»ƒm tra tá»« Ä‘iá»ƒn khÃ´ng dáº¥u. 

---

**Cursor**

ÄÃºng váº­y, quy trÃ¬nh **sequential check** (cÃ³ dáº¥u trÆ°á»›c â†’ khÃ´ng dáº¥u sau) lÃ  há»£p lÃ½. Äá»ƒ tÃ´i phÃ¢n tÃ­ch chi tiáº¿t cÃ¡ch tiáº¿p cáº­n nÃ y:

---

## ğŸ”„ Quy TrÃ¬nh Dual-Lookup Äá» Xuáº¥t

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Token     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ CHECK 1: Full Dict     â”‚
              â”‚ (Tá»« Ä‘iá»ƒn CÃ“ Dáº¤U)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚  CÃ³ trong   â”‚
                    â”‚  Full Dict? â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     YES   â”‚   NO
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                         â”‚
              â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  âœ“ VALID  â”‚        â”‚ CHECK 2: Shadow Dict   â”‚
        â”‚  (Full)   â”‚        â”‚ (Tá»« Ä‘iá»ƒn KHÃ”NG Dáº¤U)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                   â”‚  CÃ³ trong   â”‚
                                   â”‚ Shadow Dict?â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                    YES   â”‚   NO
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚                         â”‚
                             â–¼                         â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  âœ“ VALID  â”‚           â”‚ âœ— MISSPELL  â”‚
                       â”‚ (Shadow)  â”‚           â”‚             â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ PhÃ¢n TÃ­ch Æ¯u/NhÆ°á»£c Äiá»ƒm

### âœ… Æ¯u Ä‘iá»ƒm

| Äiá»ƒm | Giáº£i thÃ­ch |
|------|------------|
| **ChÃ­nh xÃ¡c vá»›i tin cÃ³ dáº¥u** | Tin nháº¯n chÃ­nh thá»‘ng (ngÃ¢n hÃ ng, telco tháº­t) thÆ°á»ng cÃ³ dáº¥u Ä‘áº§y Ä‘á»§ â†’ match Full Dict ngay |
| **Linh hoáº¡t vá»›i tin khÃ´ng dáº¥u** | Tin spam viáº¿t khÃ´ng dáº¥u váº«n Ä‘Æ°á»£c cháº¥p nháº­n náº¿u lÃ  tá»« há»£p lá»‡ |
| **Hiá»‡u suáº¥t tá»‘t** | O(1) lookup cho cáº£ 2 dict náº¿u dÃ¹ng HashSet |
| **Logic rÃµ rÃ ng** | Dá»… debug, dá»… maintain |

### âš ï¸ Äiá»ƒm cáº§n lÆ°u Ã½

**Váº¥n Ä‘á» 1: Collision trong Shadow Dict**

```
VÃ­ dá»¥: "an" (khÃ´ng dáº¥u) cÃ³ thá»ƒ lÃ :
â”œâ”€â”€ Äƒn (to eat)
â”œâ”€â”€ Ã¡n (case/sentence)  
â”œâ”€â”€ Ã n (?)
â”œâ”€â”€ áº£n (?)
â”œâ”€â”€ Ã£n (?)
â”œâ”€â”€ áº¡n (?)
â””â”€â”€ an (peace/safe) â† cÅ©ng há»£p lá»‡!
```

**Giáº£i phÃ¡p**: Vá»›i má»¥c Ä‘Ã­ch phÃ¡t hiá»‡n lá»—i chÃ­nh táº£, **collision khÃ´ng pháº£i váº¥n Ä‘á» lá»›n**:
- Náº¿u "an" map Ä‘Æ°á»£c Ä‘áº¿n Báº¤T Ká»² tá»« cÃ³ dáº¥u há»£p lá»‡ nÃ o â†’ coi lÃ  VALID
- Má»¥c tiÃªu lÃ  **loáº¡i bá» tá»« hoÃ n toÃ n vÃ´ nghÄ©a**, khÃ´ng pháº£i xÃ¡c Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c

---

**Váº¥n Ä‘á» 2: Tá»« khÃ´ng dáº¥u KHÃ”NG tá»“n táº¡i dáº¡ng cÃ³ dáº¥u**

Má»™t sá»‘ tá»« vay mÆ°á»£n/thuáº­t ngá»¯ cÃ³ thá»ƒ chá»‰ tá»“n táº¡i dáº¡ng khÃ´ng dáº¥u:
- `wifi`, `internet`, `email`, `covid`...

**Giáº£i phÃ¡p**: ÄÃ¢y lÃ  lÃ½ do cáº§n **Whitelist (Táº§ng 3)** cháº¡y trÆ°á»›c Dual-Lookup

---

## ğŸ› ï¸ CÃ¡ch XÃ¢y Dá»±ng 2 Tá»« Äiá»ƒn

### Full Dict (CÃ³ dáº¥u)

```python
def build_full_dict(words_file_path):
    """
    XÃ¢y dá»±ng tá»« Ä‘iá»ƒn cÃ³ dáº¥u tá»« file words.txt
    Format file: {"text": "tá»«", "source": [...]}
    """
    full_dict = set()
    
    with open(words_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                entry = json.loads(line.strip())
                word = entry['text'].lower()
                full_dict.add(word)
                
                # ThÃªm cáº£ dáº¡ng cÃ³ gáº¡ch ná»‘i vÃ  khÃ´ng gáº¡ch ná»‘i
                # "ngÃ¢n-hÃ ng" â†’ thÃªm cáº£ "ngÃ¢n hÃ ng"
                if '-' in word:
                    full_dict.add(word.replace('-', ' '))
                    full_dict.add(word.replace('-', '_'))
                    
            except json.JSONDecodeError:
                continue
    
    return full_dict

# Káº¿t quáº£: ~79,000+ tá»« cÃ³ dáº¥u
```

### Shadow Dict (KhÃ´ng dáº¥u)

```python
import unicodedata
import re

def remove_vietnamese_accents(text):
    """
    Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i gá»‘c
    "ngÃ¢n hÃ ng" â†’ "ngan hang"
    "Ä‘iá»u kiá»‡n" â†’ "dieu kien"
    """
    # Báº£ng chuyá»ƒn Ä‘á»•i Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t
    ACCENT_MAP = {
        'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
        'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
        'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
        'Ä‘': 'd',
        'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
        'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
        'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
        'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
        'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
        'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
        'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
        'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
        'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
    }
    
    result = []
    for char in text.lower():
        result.append(ACCENT_MAP.get(char, char))
    
    return ''.join(result)


def build_shadow_dict(full_dict):
    """
    Táº¡o Shadow Dict tá»« Full Dict
    Má»—i dáº¡ng khÃ´ng dáº¥u â†’ set cÃ¡c dáº¡ng cÃ³ dáº¥u gá»‘c (Ä‘á»ƒ debug náº¿u cáº§n)
    """
    shadow_dict = set()
    
    for word in full_dict:
        no_accent = remove_vietnamese_accents(word)
        shadow_dict.add(no_accent)
    
    return shadow_dict

# Káº¿t quáº£: ~50,000-60,000 tá»« khÃ´ng dáº¥u (Ã­t hÆ¡n do collision)
```

---

## ğŸ“ Code HoÃ n Chá»‰nh cho Dual-Lookup

```python
class DualLookupValidator:
    """
    Táº§ng 4: Dual-Lookup Validation
    Kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a tá»« tiáº¿ng Viá»‡t
    """
    
    def __init__(self, words_file_path):
        print("Loading Vietnamese dictionaries...")
        self.full_dict = self._build_full_dict(words_file_path)
        self.shadow_dict = self._build_shadow_dict(self.full_dict)
        print(f"âœ“ Full Dict: {len(self.full_dict):,} words")
        print(f"âœ“ Shadow Dict: {len(self.shadow_dict):,} words")
    
    def _build_full_dict(self, path):
        full_dict = set()
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    word = entry['text'].lower().strip()
                    if word:
                        full_dict.add(word)
                        # Xá»­ lÃ½ biáº¿n thá»ƒ
                        if '-' in word:
                            full_dict.add(word.replace('-', ' '))
                        if ' ' in word:
                            full_dict.add(word.replace(' ', '_'))
                except:
                    continue
        return full_dict
    
    def _build_shadow_dict(self, full_dict):
        shadow = set()
        for word in full_dict:
            no_accent = remove_vietnamese_accents(word)
            shadow.add(no_accent)
        return shadow
    
    def validate(self, token):
        """
        Kiá»ƒm tra token cÃ³ há»£p lá»‡ khÃ´ng
        
        Returns:
            tuple: (is_valid: bool, match_type: str)
            match_type: 'full' | 'shadow' | 'misspell'
        """
        token_lower = token.lower().strip()
        
        # Bá» qua token quÃ¡ ngáº¯n hoáº·c chá»‰ cÃ³ sá»‘
        if len(token_lower) < 1 or token_lower.isdigit():
            return (True, 'skip')
        
        # CHECK 1: Tá»« Ä‘iá»ƒn cÃ³ dáº¥u
        if token_lower in self.full_dict:
            return (True, 'full')
        
        # CHECK 2: Tá»« Ä‘iá»ƒn khÃ´ng dáº¥u
        token_no_accent = remove_vietnamese_accents(token_lower)
        if token_no_accent in self.shadow_dict:
            return (True, 'shadow')
        
        # KhÃ´ng match â†’ Misspell
        return (False, 'misspell')
    
    def validate_tokens(self, tokens):
        """
        Kiá»ƒm tra danh sÃ¡ch tokens
        
        Returns:
            dict: {
                'misspelled_tokens': [...],
                'misspelled_count': int,
                'total_checked': int,
                'misspelled_pct': float
            }
        """
        misspelled = []
        total_checked = 0
        
        for token in tokens:
            is_valid, match_type = self.validate(token)
            
            if match_type == 'skip':
                continue
                
            total_checked += 1
            
            if not is_valid:
                misspelled.append(token)
        
        return {
            'misspelled_tokens': misspelled,
            'misspelled_count': len(misspelled),
            'total_checked': total_checked,
            'misspelled_pct': (len(misspelled) / total_checked * 100) if total_checked > 0 else 0
        }
```

---

## ğŸ”— TÃ­ch Há»£p Vá»›i CÃ¡c Táº§ng KhÃ¡c

```python
def extract_spelling_features(text, masker, leet_decoder, tokenizer, whitelist, validator):
    """
    Pipeline hoÃ n chá»‰nh 4 táº§ng
    """
    # Táº¦NG 1: Aggressive Masking
    masked_text, entity_counts = masker.mask(text)
    
    # Táº¦NG 2: Leetspeak Decode (náº¿u tÃ¡ch riÃªng)
    decoded_text, leet_count = leet_decoder.decode(masked_text)
    
    # Táº¦NG 3: Tokenization
    tokens = tokenizer.tokenize(decoded_text)
    
    # Táº¦NG 3.5: Whitelist Filter
    tokens_to_check = [t for t in tokens if t.lower() not in whitelist]
    
    # Táº¦NG 4: Dual-Lookup Validation
    validation_result = validator.validate_tokens(tokens_to_check)
    
    return {
        'misspelled_count': validation_result['misspelled_count'],
        'misspelled_pct': validation_result['misspelled_pct'],
        'leet_count': leet_count,
        'leet_pct': (leet_count / len(text) * 100) if len(text) > 0 else 0,
        'total_tokens': len(tokens),
        'entity_counts': entity_counts,
        'misspelled_tokens': validation_result['misspelled_tokens']  # Ä‘á»ƒ debug
    }
```

---

## âœ… Káº¿t Luáº­n

Quy trÃ¬nh **Full Dict â†’ Shadow Dict** lÃ  há»£p lÃ½ vÃ  tÃ´i **Ä‘á»“ng Ã½** vá»›i Ä‘á» xuáº¥t. Äiá»ƒm máº¥u chá»‘t:

| BÆ°á»›c | Má»¥c Ä‘Ã­ch |
|------|----------|
| Check Full Dict trÆ°á»›c | Æ¯u tiÃªn tá»« cÃ³ dáº¥u Ä‘Ãºng (tin chÃ­nh thá»‘ng) |
| Check Shadow Dict sau | Cháº¥p nháº­n tá»« khÃ´ng dáº¥u nhÆ°ng cÃ³ gá»‘c há»£p lá»‡ |
| Fail cáº£ 2 â†’ Misspell | Tá»« hoÃ n toÃ n khÃ´ng cÃ³ trong tiáº¿ng Viá»‡t |

CÃ¡ch tiáº¿p cáº­n nÃ y **cÃ¢n báº±ng** giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng xá»­ lÃ½ thá»±c táº¿ cá»§a dá»¯ liá»‡u SMS tiáº¿ng Viá»‡t (thÆ°á»ng khÃ´ng cÃ³ dáº¥u).

```plaintext
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Token     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ CHECK 1: Full Dict     â”‚
              â”‚ (Tá»« Ä‘iá»ƒn CÃ“ Dáº¤U)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚  CÃ³ trong   â”‚
                    â”‚  Full Dict? â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     YES   â”‚   NO
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                         â”‚
              â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  âœ“ VALID  â”‚        â”‚ CHECK 2: Shadow Dict   â”‚
        â”‚  (Full)   â”‚        â”‚ (Tá»« Ä‘iá»ƒn KHÃ”NG Dáº¤U)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                   â”‚  CÃ³ trong   â”‚
                                   â”‚ Shadow Dict?â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                    YES   â”‚   NO
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚                         â”‚
                             â–¼                         â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  âœ“ VALID  â”‚           â”‚ âœ— MISSPELL  â”‚
                       â”‚ (Shadow)  â”‚           â”‚             â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```plaintext
VÃ­ dá»¥: "an" (khÃ´ng dáº¥u) cÃ³ thá»ƒ lÃ :
â”œâ”€â”€ Äƒn (to eat)
â”œâ”€â”€ Ã¡n (case/sentence)  
â”œâ”€â”€ Ã n (?)
â”œâ”€â”€ áº£n (?)
â”œâ”€â”€ Ã£n (?)
â”œâ”€â”€ áº¡n (?)
â””â”€â”€ an (peace/safe) â† cÅ©ng há»£p lá»‡!
```

```python
def build_full_dict(words_file_path):
    """
    XÃ¢y dá»±ng tá»« Ä‘iá»ƒn cÃ³ dáº¥u tá»« file words.txt
    Format file: {"text": "tá»«", "source": [...]}
    """
    full_dict = set()
    
    with open(words_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                entry = json.loads(line.strip())
                word = entry['text'].lower()
                full_dict.add(word)
                
                # ThÃªm cáº£ dáº¡ng cÃ³ gáº¡ch ná»‘i vÃ  khÃ´ng gáº¡ch ná»‘i
                # "ngÃ¢n-hÃ ng" â†’ thÃªm cáº£ "ngÃ¢n hÃ ng"
                if '-' in word:
                    full_dict.add(word.replace('-', ' '))
                    full_dict.add(word.replace('-', '_'))
                    
            except json.JSONDecodeError:
                continue
    
    return full_dict

# Káº¿t quáº£: ~79,000+ tá»« cÃ³ dáº¥u
```

```python
import unicodedata
import re

def remove_vietnamese_accents(text):
    """
    Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i gá»‘c
    "ngÃ¢n hÃ ng" â†’ "ngan hang"
    "Ä‘iá»u kiá»‡n" â†’ "dieu kien"
    """
    # Báº£ng chuyá»ƒn Ä‘á»•i Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t
    ACCENT_MAP = {
        'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
        'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
        'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
        'Ä‘': 'd',
        'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
        'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
        'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
        'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
        'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
        'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
        'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
        'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
        'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
    }
    
    result = []
    for char in text.lower():
        result.append(ACCENT_MAP.get(char, char))
    
    return ''.join(result)


def build_shadow_dict(full_dict):
    """
    Táº¡o Shadow Dict tá»« Full Dict
    Má»—i dáº¡ng khÃ´ng dáº¥u â†’ set cÃ¡c dáº¡ng cÃ³ dáº¥u gá»‘c (Ä‘á»ƒ debug náº¿u cáº§n)
    """
    shadow_dict = set()
    
    for word in full_dict:
        no_accent = remove_vietnamese_accents(word)
        shadow_dict.add(no_accent)
    
    return shadow_dict

# Káº¿t quáº£: ~50,000-60,000 tá»« khÃ´ng dáº¥u (Ã­t hÆ¡n do collision)
```

```python
class DualLookupValidator:
    """
    Táº§ng 4: Dual-Lookup Validation
    Kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a tá»« tiáº¿ng Viá»‡t
    """
    
    def __init__(self, words_file_path):
        print("Loading Vietnamese dictionaries...")
        self.full_dict = self._build_full_dict(words_file_path)
        self.shadow_dict = self._build_shadow_dict(self.full_dict)
        print(f"âœ“ Full Dict: {len(self.full_dict):,} words")
        print(f"âœ“ Shadow Dict: {len(self.shadow_dict):,} words")
    
    def _build_full_dict(self, path):
        full_dict = set()
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    word = entry['text'].lower().strip()
                    if word:
                        full_dict.add(word)
                        # Xá»­ lÃ½ biáº¿n thá»ƒ
                        if '-' in word:
                            full_dict.add(word.replace('-', ' '))
                        if ' ' in word:
                            full_dict.add(word.replace(' ', '_'))
                except:
                    continue
        return full_dict
    
    def _build_shadow_dict(self, full_dict):
        shadow = set()
        for word in full_dict:
            no_accent = remove_vietnamese_accents(word)
            shadow.add(no_accent)
        return shadow
    
    def validate(self, token):
        """
        Kiá»ƒm tra token cÃ³ há»£p lá»‡ khÃ´ng
        
        Returns:
            tuple: (is_valid: bool, match_type: str)
            match_type: 'full' | 'shadow' | 'misspell'
        """
        token_lower = token.lower().strip()
        
        # Bá» qua token quÃ¡ ngáº¯n hoáº·c chá»‰ cÃ³ sá»‘
        if len(token_lower) < 1 or token_lower.isdigit():
            return (True, 'skip')
        
        # CHECK 1: Tá»« Ä‘iá»ƒn cÃ³ dáº¥u
        if token_lower in self.full_dict:
            return (True, 'full')
        
        # CHECK 2: Tá»« Ä‘iá»ƒn khÃ´ng dáº¥u
        token_no_accent = remove_vietnamese_accents(token_lower)
        if token_no_accent in self.shadow_dict:
            return (True, 'shadow')
        
        # KhÃ´ng match â†’ Misspell
        return (False, 'misspell')
    
    def validate_tokens(self, tokens):
        """
        Kiá»ƒm tra danh sÃ¡ch tokens
        
        Returns:
            dict: {
                'misspelled_tokens': [...],
                'misspelled_count': int,
                'total_checked': int,
                'misspelled_pct': float
            }
        """
        misspelled = []
        total_checked = 0
        
        for token in tokens:
            is_valid, match_type = self.validate(token)
            
            if match_type == 'skip':
                continue
                
            total_checked += 1
            
            if not is_valid:
                misspelled.append(token)
        
        return {
            'misspelled_tokens': misspelled,
            'misspelled_count': len(misspelled),
            'total_checked': total_checked,
            'misspelled_pct': (len(misspelled) / total_checked * 100) if total_checked > 0 else 0
        }
```

```python
def extract_spelling_features(text, masker, leet_decoder, tokenizer, whitelist, validator):
    """
    Pipeline hoÃ n chá»‰nh 4 táº§ng
    """
    # Táº¦NG 1: Aggressive Masking
    masked_text, entity_counts = masker.mask(text)
    
    # Táº¦NG 2: Leetspeak Decode (náº¿u tÃ¡ch riÃªng)
    decoded_text, leet_count = leet_decoder.decode(masked_text)
    
    # Táº¦NG 3: Tokenization
    tokens = tokenizer.tokenize(decoded_text)
    
    # Táº¦NG 3.5: Whitelist Filter
    tokens_to_check = [t for t in tokens if t.lower() not in whitelist]
    
    # Táº¦NG 4: Dual-Lookup Validation
    validation_result = validator.validate_tokens(tokens_to_check)
    
    return {
        'misspelled_count': validation_result['misspelled_count'],
        'misspelled_pct': validation_result['misspelled_pct'],
        'leet_count': leet_count,
        'leet_pct': (leet_count / len(text) * 100) if len(text) > 0 else 0,
        'total_tokens': len(tokens),
        'entity_counts': entity_counts,
        'misspelled_tokens': validation_result['misspelled_tokens']  # Ä‘á»ƒ debug
    }
```

---

**User**

Váº­y hÃ£y tá»•ng káº¿t láº¡i tá»« táº§ng 1 cÃ¡ch chi tiáº¿t. Háº¿t sá»©c chi tiáº¿t Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ tiáº¿n hÃ nh thá»±c hiá»‡n tá»«ng bÆ°á»›c, khÃ´ng nháº­p nháº±ng giá»¯a cÃ¡c pháº§n, rÃ nh máº¡ch rÃµ rÃ ng. 

---

**Cursor**

# ğŸ“‹ KIáº¾N TRÃšC PHÃ‚N Táº¦NG Xá»¬ LÃ Lá»–I CHÃNH Táº¢ - Tá»”NG Káº¾T CHI TIáº¾T

---

## ğŸ—ï¸ Tá»”NG QUAN KIáº¾N TRÃšC

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAW SMS TEXT                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 1: AGGRESSIVE MASKING                                        â”‚
â”‚  Che giáº¥u thá»±c thá»ƒ phi ngÃ´n ngá»¯ báº±ng Regex                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 2: NORMALIZATION & TOKENIZATION                              â”‚
â”‚  Chuáº©n hÃ³a Unicode + TÃ¡ch tá»« tiáº¿ng Viá»‡t                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 3: WHITELIST FILTERING                                       â”‚
â”‚  Lá»c bá» cÃ¡c tá»« ngoáº¡i lá»‡ há»£p lá»‡                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 4: DUAL-LOOKUP VALIDATION                                    â”‚
â”‚  Kiá»ƒm tra tá»« Ä‘iá»ƒn CÃ³ dáº¥u â†’ KhÃ´ng dáº¥u                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OUTPUT FEATURES                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

---

# Táº¦NG 1: AGGRESSIVE MASKING

---

## 1.1. Má»¥c TiÃªu

TÃ¬m vÃ  **thay tháº¿** cÃ¡c thá»±c thá»ƒ phi ngÃ´n ngá»¯ báº±ng token placeholder Ä‘á»ƒ:
- KhÃ´ng bá»‹ Ä‘áº¿m nháº§m lÃ  "lá»—i chÃ­nh táº£"
- Chuáº©n hÃ³a dá»¯ liá»‡u trÆ°á»›c khi xá»­ lÃ½ ngÃ´n ngá»¯
- Äáº¿m sá»‘ lÆ°á»£ng tá»«ng loáº¡i thá»±c thá»ƒ (feature phá»¥ cho ML)

---

## 1.2. Input / Output

| | MÃ´ táº£ |
|---|---|
| **Input** | Chuá»—i vÄƒn báº£n SMS gá»‘c |
| **Output 1** | Chuá»—i vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c mask (thay thá»±c thá»ƒ báº±ng token) |
| **Output 2** | Dictionary Ä‘áº¿m sá»‘ lÆ°á»£ng tá»«ng loáº¡i thá»±c thá»ƒ |

**VÃ­ dá»¥:**
```
Input:  "Dang nhap https://acb.vn-fake.com de nhan 500.000VND. LH: 0901234567"
Output: "Dang nhap <URL> de nhan <MONEY>. LH: <PHONE>"
Counts: {'url': 1, 'money': 1, 'phone': 1, 'code': 0, 'datetime': 0}
```

---

## 1.3. CÃ¡c Loáº¡i Thá»±c Thá»ƒ Cáº§n Mask

| Token | MÃ´ táº£ | VÃ­ dá»¥ cáº§n báº¯t |
|-------|-------|---------------|
| `<URL>` | ÄÆ°á»ng dáº«n web, domain | `https://...`, `www.`, `bit.ly/xxx`, `acb.vn-fake.com`, `abc . com` |
| `<PHONE>` | Sá»‘ Ä‘iá»‡n thoáº¡i VN | `0901234567`, `84901234567`, `0901.234.567`, `+84 901 234 567` |
| `<MONEY>` | Sá»‘ tiá»n | `500k`, `500.000Ä‘`, `2.000.000VND`, `5 trieu`, `100 ngÃ n` |
| `<CODE>` | MÃ£ code/OTP | `OTP123`, `ST5K`, `ABC123XYZ`, `V120N` |
| `<DATETIME>` | NgÃ y giá» | `15/05/2024`, `10h30`, `17:00`, `27/03` |
| `<ZALO>` | Link Zalo | `zalo.me/84xxx`, `Zalo: 0901234567` |
| `<EMAIL>` | Email | `abc@gmail.com` |

---

## 1.4. Regex Patterns Chi Tiáº¿t

```python
import re
from collections import OrderedDict

class AggressiveMasker:
    """
    Táº§ng 1: Aggressive Masking
    Thay tháº¿ cÃ¡c thá»±c thá»ƒ phi ngÃ´n ngá»¯ báº±ng token placeholder
    """
    
    def __init__(self):
        # OrderedDict Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»± xá»­ lÃ½ (URL trÆ°á»›c, sau Ä‘Ã³ má»›i Ä‘áº¿n cÃ¡c loáº¡i khÃ¡c)
        self.patterns = OrderedDict([
            # 1. URL - Pháº£i xá»­ lÃ½ TRÆ¯á»šC TIÃŠN (aggressive)
            ('url', re.compile(
                r'('
                # Pattern 1: URL chuáº©n vá»›i protocol
                r'(?:https?://)[^\s<>\"\']+'
                r'|'
                # Pattern 2: www
                r'(?:www\.)[^\s<>\"\']+'
                r'|'
                # Pattern 3: Shortener links
                r'(?:bit\.ly|tinyurl\.com|goo\.gl|t\.me|zalo\.me)/[^\s<>\"\']+'
                r'|'
                # Pattern 4: Domain cÃ³ khoáº£ng tráº¯ng cá»‘ Ã½ (spammer trick)
                # VÃ­ dá»¥: "acb . com", "vcb .vn"
                r'[a-zA-Z0-9][-a-zA-Z0-9]*\s*\.\s*(?:com|vn|net|org|top|xyz|icu|vip|cc|life|club|cfd|info|biz|site|online|tech|app|win|shop)(?:/[^\s]*)?'
                r'|'
                # Pattern 5: Domain khÃ´ng cÃ³ protocol nhÆ°ng cÃ³ path
                r'[a-zA-Z0-9][-a-zA-Z0-9]*\.(?:com|vn|net|org|top|xyz|icu|vip|cc|life|club|cfd)/[^\s<>\"\']+'
                r')',
                re.IGNORECASE
            )),
            
            # 2. ZALO - Link Zalo Ä‘áº·c biá»‡t
            ('zalo', re.compile(
                r'('
                r'zalo\.me/\d+'
                r'|'
                r'zalo\s*[:\-]?\s*\d{9,12}'
                r')',
                re.IGNORECASE
            )),
            
            # 3. EMAIL
            ('email', re.compile(
                r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
                re.IGNORECASE
            )),
            
            # 4. PHONE - Sá»‘ Ä‘iá»‡n thoáº¡i Viá»‡t Nam
            ('phone', re.compile(
                r'('
                # Pattern 1: Báº¯t Ä‘áº§u báº±ng +84 hoáº·c 84
                r'(?:\+?84)\s*\d[\d\s\.\-]{8,12}'
                r'|'
                # Pattern 2: Báº¯t Ä‘áº§u báº±ng 0
                r'(?<!\d)0\d{2}[\s\.\-]?\d{3}[\s\.\-]?\d{3,4}(?!\d)'
                r'|'
                # Pattern 3: Sá»‘ tá»•ng Ä‘Ã i 1800, 1900, 9xxx
                r'(?:1800|1900|19\d{2})\s*[\d\s\.\-]{4,8}'
                r'|'
                # Pattern 4: Äáº§u sá»‘ ngáº¯n (brandname SMS)
                r'(?<!\d)\d{4,6}(?!\d)'
                r')'
            )),
            
            # 5. MONEY - Sá»‘ tiá»n
            ('money', re.compile(
                r'('
                # Pattern 1: Sá»‘ + Ä‘Æ¡n vá»‹ (500k, 500K, 5tr, 5TR)
                r'\d{1,3}(?:[.,]\d{3})*\s*(?:k|K|tr|TR|trieu|triá»‡u|Trieu|Triá»‡u)\b'
                r'|'
                # Pattern 2: Sá»‘ + VND/VNÄ/Ä‘/d/dong/Ä‘á»“ng
                r'\d{1,3}(?:[.,]\d{3})*\s*(?:VND|VNÄ|vnÄ‘|vnd|Ä‘á»“ng|dong|Ä‘|d)\b'
                r'|'
                # Pattern 3: Sá»‘ lá»›n cÃ³ dáº¥u cháº¥m/pháº©y (500.000, 1,000,000)
                r'\d{1,3}(?:[.,]\d{3}){1,4}(?!\d)'
                r'|'
                # Pattern 4: Sá»‘ + ngÃ n/nghÃ¬n/triá»‡u/tá»·
                r'\d+\s*(?:ngÃ n|nghÃ¬n|ngan|nghin|triá»‡u|trieu|tá»·|ty)\b'
                r')',
                re.IGNORECASE
            )),
            
            # 6. DATETIME - NgÃ y thÃ¡ng, giá»
            ('datetime', re.compile(
                r'('
                # Pattern 1: NgÃ y thÃ¡ng DD/MM hoáº·c DD/MM/YYYY
                r'\d{1,2}[/\-\.]\d{1,2}(?:[/\-\.]\d{2,4})?'
                r'|'
                # Pattern 2: Giá» phÃºt HH:MM hoáº·c HHhMM hoáº·c HH giá»
                r'\d{1,2}\s*[hgG:]\s*\d{0,2}(?:\s*(?:phÃºt|phut|p))?' 
                r'|'
                # Pattern 3: Giá» + AM/PM
                r'\d{1,2}\s*(?:am|pm|AM|PM)'
                r')'
            )),
            
            # 7. CODE - MÃ£ OTP, mÃ£ giao dá»‹ch, mÃ£ khuyáº¿n mÃ£i
            ('code', re.compile(
                r'('
                # Pattern 1: Chá»¯ in hoa + sá»‘ (OTP123, ABC123)
                r'\b[A-Z]{2,}[0-9]+[A-Z0-9]*\b'
                r'|'
                # Pattern 2: Sá»‘ + chá»¯ in hoa (123ABC, 5K50)
                r'\b[0-9]+[A-Z]{2,}[A-Z0-9]*\b'
                r'|'
                # Pattern 3: MÃ£ cÃ³ dáº¡ng XXX-XXX hoáº·c XXX_XXX
                r'\b[A-Z0-9]{2,}[-_][A-Z0-9]{2,}\b'
                r')'
            )),
        ])
    
    def mask(self, text):
        """
        Thá»±c hiá»‡n masking trÃªn vÄƒn báº£n
        
        Args:
            text: Chuá»—i vÄƒn báº£n gá»‘c
            
        Returns:
            tuple: (masked_text, entity_counts)
        """
        if not isinstance(text, str) or not text.strip():
            return text, {name: 0 for name in self.patterns.keys()}
        
        masked_text = text
        entity_counts = {name: 0 for name in self.patterns.keys()}
        
        # Ãp dá»¥ng tá»«ng pattern theo thá»© tá»±
        for entity_name, pattern in self.patterns.items():
            # Äáº¿m sá»‘ match
            matches = pattern.findall(masked_text)
            entity_counts[entity_name] = len(matches)
            
            # Thay tháº¿ báº±ng token
            token = f'<{entity_name.upper()}>'
            masked_text = pattern.sub(token, masked_text)
        
        return masked_text, entity_counts
    
    def mask_batch(self, texts):
        """
        Mask má»™t batch vÄƒn báº£n
        
        Args:
            texts: List cÃ¡c chuá»—i vÄƒn báº£n
            
        Returns:
            tuple: (list of masked_texts, list of entity_counts)
        """
        results = [self.mask(text) for text in texts]
        masked_texts = [r[0] for r in results]
        entity_counts = [r[1] for r in results]
        return masked_texts, entity_counts
```

---

## 1.5. Test Cases cho Táº§ng 1

```python
def test_aggressive_masker():
    masker = AggressiveMasker()
    
    test_cases = [
        # Test URL
        (
            "Dang nhap https://acb.vn-fake.com ngay",
            "Dang nhap <URL> ngay",
            {'url': 1}
        ),
        (
            "Truy cap www.vietcombank . vip de nhan",
            "Truy cap <URL> de nhan",
            {'url': 1}
        ),
        (
            "Link: bit.ly/abc123 hoac tinyurl.com/xyz",
            "Link: <URL> hoac <URL>",
            {'url': 2}
        ),
        
        # Test Phone
        (
            "LH 0901234567 hoac 84 901 234 567",
            "LH <PHONE> hoac <PHONE>",
            {'phone': 2}
        ),
        (
            "Goi 1900 1234 de duoc ho tro",
            "Goi <PHONE> de duoc ho tro",
            {'phone': 1}
        ),
        
        # Test Money
        (
            "Nhan 500k hoac 500.000 VND",
            "Nhan <MONEY> hoac <MONEY>",
            {'money': 2}
        ),
        (
            "Thanh toan 2.000.000Ä‘ truoc 5 trieu",
            "Thanh toan <MONEY> truoc <MONEY>",
            {'money': 2}
        ),
        
        # Test Code
        (
            "Ma OTP: ABC123 hoac ST5K",
            "Ma OTP: <CODE> hoac <CODE>",
            {'code': 2}
        ),
        
        # Test DateTime
        (
            "Truoc 17h30 ngay 15/05/2024",
            "Truoc <DATETIME> ngay <DATETIME>",
            {'datetime': 2}
        ),
        
        # Test Combined
        (
            "ACB: Dang nhap https://acb-fake.com de nhan 500.000VND. LH: 0901234567",
            "ACB: Dang nhap <URL> de nhan <MONEY>. LH: <PHONE>",
            {'url': 1, 'money': 1, 'phone': 1}
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 1: AGGRESSIVE MASKING")
    print("=" * 60)
    
    for i, (input_text, expected_masked, expected_counts) in enumerate(test_cases):
        masked, counts = masker.mask(input_text)
        
        # Kiá»ƒm tra tá»«ng entity count
        passed = True
        for entity, expected_count in expected_counts.items():
            if counts.get(entity, 0) != expected_count:
                passed = False
                break
        
        status = "âœ“ PASS" if passed else "âœ— FAIL"
        print(f"\nTest {i+1}: {status}")
        print(f"  Input:    {input_text}")
        print(f"  Output:   {masked}")
        print(f"  Counts:   {counts}")

# Cháº¡y test
# test_aggressive_masker()
```

---

## 1.6. File Structure cho Táº§ng 1

```
misspelled_detection/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ layer1_masking.py          # Code Táº§ng 1
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_layer1.py         # Test cases
```

---

## 1.7. Checklist HoÃ n ThÃ nh Táº§ng 1

- [ ] Táº¡o class `AggressiveMasker`
- [ ] Implement Ä‘á»§ 7 loáº¡i pattern (URL, ZALO, EMAIL, PHONE, MONEY, DATETIME, CODE)
- [ ] Test vá»›i cÃ¡c trÆ°á»ng há»£p edge case:
  - [ ] URL cÃ³ khoáº£ng tráº¯ng cá»‘ Ã½ (`acb . com`)
  - [ ] Phone cÃ³ dáº¥u cháº¥m/gáº¡ch (`0901.234.567`)
  - [ ] Money nhiá»u format (`500k`, `500.000Ä‘`, `5 triá»‡u`)
  - [ ] VÄƒn báº£n khÃ´ng cÃ³ thá»±c thá»ƒ
  - [ ] VÄƒn báº£n rá»—ng/None
- [ ] Äáº£m báº£o thá»© tá»± xá»­ lÃ½ Ä‘Ãºng (URL trÆ°á»›c)

---

---

# Táº¦NG 2: NORMALIZATION & TOKENIZATION

---

## 2.1. Má»¥c TiÃªu

1. **Chuáº©n hÃ³a Unicode**: ÄÆ°a vÄƒn báº£n vá» dáº¡ng NFC chuáº©n
2. **Decode Leetspeak**: Chuyá»ƒn Ä‘á»•i kÃ½ tá»± thay tháº¿ vá» dáº¡ng gá»‘c (`0`â†’`o`, `!`â†’`i`)
3. **TÃ¡ch tá»« (Tokenization)**: Chia cÃ¢u thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ tá»« cÃ³ nghÄ©a

---

## 2.2. Input / Output

| | MÃ´ táº£ |
|---|---|
| **Input** | Chuá»—i vÄƒn báº£n Ä‘Ã£ mask tá»« Táº§ng 1 |
| **Output 1** | Danh sÃ¡ch tokens (tá»« Ä‘Ã£ tÃ¡ch) |
| **Output 2** | `leet_count` - Sá»‘ kÃ½ tá»± leetspeak Ä‘Ã£ chuyá»ƒn Ä‘á»•i |
| **Output 3** | VÄƒn báº£n Ä‘Ã£ chuáº©n hÃ³a (Ä‘á»ƒ debug) |

**VÃ­ dá»¥:**
```
Input:  "Kh0ng pha! ban dang nhap vu! l0ng xac nhan"
Output: 
  - tokens: ["khÃ´ng", "pháº£i", "báº¡n", "Ä‘Äƒng", "nháº­p", "vui", "lÃ²ng", "xÃ¡c", "nháº­n"]
  - leet_count: 4  (0â†’o: 2 láº§n, !â†’i: 2 láº§n)
```

---

## 2.3. Sub-module 2A: Unicode Normalization

```python
import unicodedata

def normalize_unicode(text):
    """
    Chuáº©n hÃ³a Unicode vá» dáº¡ng NFC (Composed)
    
    Xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p:
    - KÃ½ tá»± tá»• há»£p (combining characters)
    - CÃ¡c biáº¿n thá»ƒ Unicode cá»§a cÃ¹ng má»™t kÃ½ tá»±
    - Khoáº£ng tráº¯ng Ä‘áº·c biá»‡t
    
    Args:
        text: Chuá»—i vÄƒn báº£n gá»‘c
        
    Returns:
        Chuá»—i Ä‘Ã£ chuáº©n hÃ³a
    """
    if not isinstance(text, str):
        return text
    
    # BÆ°á»›c 1: Chuáº©n hÃ³a NFC
    text = unicodedata.normalize('NFC', text)
    
    # BÆ°á»›c 2: Thay tháº¿ cÃ¡c loáº¡i khoáº£ng tráº¯ng Ä‘áº·c biá»‡t báº±ng space thÆ°á»ng
    # \u00A0 = Non-breaking space
    # \u2000-\u200B = Various Unicode spaces
    text = re.sub(r'[\u00A0\u2000-\u200B\u202F\u205F\u3000]', ' ', text)
    
    # BÆ°á»›c 3: Loáº¡i bá» zero-width characters
    text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)
    
    # BÆ°á»›c 4: Chuáº©n hÃ³a multiple spaces thÃ nh single space
    text = re.sub(r' +', ' ', text)
    
    return text.strip()
```

---

## 2.4. Sub-module 2B: Leetspeak Decoder

```python
class LeetSpeakDecoder:
    """
    Giáº£i mÃ£ Leetspeak - chuyá»ƒn Ä‘á»•i kÃ½ tá»± thay tháº¿ vá» dáº¡ng gá»‘c
    
    Phá»• biáº¿n trong SMS spam tiáº¿ng Viá»‡t:
    - 0 â†’ o (kh0ng â†’ khÃ´ng)
    - 1 â†’ i (d1ch vu â†’ dá»‹ch vá»¥)  
    - ! â†’ i (d!eu k!en â†’ Ä‘iá»u kiá»‡n)
    - 3 â†’ e (tr3n â†’ trÃªn)
    - 4 â†’ a (kho4n â†’ khoáº£n)
    - @ â†’ a (@n â†’ Äƒn)
    - $ â†’ s ($o â†’ sá»‘)
    """
    
    def __init__(self):
        # Báº£ng chuyá»ƒn Ä‘á»•i Leetspeak â†’ Chá»¯ cÃ¡i
        # Chá»‰ Ä‘á»‹nh nghÄ©a cÃ¡c mapping phá»• biáº¿n vÃ  Ã­t gÃ¢y nháº§m láº«n
        self.leet_map = {
            '0': 'o',   # kh0ng â†’ khÃ´ng
            '1': 'i',   # d1ch â†’ dá»‹ch
            '!': 'i',   # d!eu â†’ Ä‘iá»u
            '3': 'e',   # tr3n â†’ trÃªn (Ã­t phá»• biáº¿n)
            '@': 'a',   # @n â†’ Äƒn (Ã­t phá»• biáº¿n)
        }
        
        # Pattern Ä‘á»ƒ phÃ¡t hiá»‡n leetspeak trong ngá»¯ cáº£nh tá»«
        # Chá»‰ decode khi kÃ½ tá»± náº±m trong má»™t tá»« (cÃ³ chá»¯ cÃ¡i xung quanh)
        self.leet_pattern = re.compile(
            r'(?<=[a-zA-ZÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘])'
            r'([0!13@])'
            r'(?=[a-zA-ZÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘])',
            re.IGNORECASE
        )
    
    def decode(self, text):
        """
        Giáº£i mÃ£ leetspeak trong vÄƒn báº£n
        
        Args:
            text: Chuá»—i vÄƒn báº£n
            
        Returns:
            tuple: (decoded_text, leet_count)
        """
        if not isinstance(text, str) or not text:
            return text, 0
        
        leet_count = 0
        result = []
        i = 0
        
        while i < len(text):
            char = text[i]
            
            # Kiá»ƒm tra xem cÃ³ pháº£i leetspeak khÃ´ng
            if char in self.leet_map:
                # Kiá»ƒm tra ngá»¯ cáº£nh: cÃ³ chá»¯ cÃ¡i trÆ°á»›c VÃ€ sau khÃ´ng?
                has_letter_before = (i > 0 and self._is_vietnamese_letter(text[i-1]))
                has_letter_after = (i < len(text)-1 and self._is_vietnamese_letter(text[i+1]))
                
                if has_letter_before and has_letter_after:
                    # ÄÃ¢y lÃ  leetspeak â†’ decode
                    result.append(self.leet_map[char])
                    leet_count += 1
                else:
                    # KhÃ´ng pháº£i leetspeak (vÃ­ dá»¥: sá»‘ Ä‘á»©ng má»™t mÃ¬nh)
                    result.append(char)
            else:
                result.append(char)
            
            i += 1
        
        return ''.join(result), leet_count
    
    def _is_vietnamese_letter(self, char):
        """Kiá»ƒm tra kÃ½ tá»± cÃ³ pháº£i chá»¯ cÃ¡i tiáº¿ng Viá»‡t khÃ´ng"""
        vietnamese_chars = (
            'aÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­bcdÄ‘eÃ¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡fghiÃ¬Ã­á»‰Ä©á»‹jklmno'
            'Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£pqrstuÃ¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±vwxyá»³Ã½á»·á»¹á»µz'
        )
        return char.lower() in vietnamese_chars
    
    def get_leet_positions(self, text):
        """
        Tráº£ vá» vá»‹ trÃ­ cÃ¡c kÃ½ tá»± leetspeak (Ä‘á»ƒ highlight/debug)
        
        Returns:
            list of tuples: [(position, original_char, decoded_char), ...]
        """
        positions = []
        for i, char in enumerate(text):
            if char in self.leet_map:
                has_letter_before = (i > 0 and self._is_vietnamese_letter(text[i-1]))
                has_letter_after = (i < len(text)-1 and self._is_vietnamese_letter(text[i+1]))
                
                if has_letter_before and has_letter_after:
                    positions.append((i, char, self.leet_map[char]))
        
        return positions
```

---

## 2.5. Sub-module 2C: Vietnamese Tokenizer

```python
# Cáº§n cÃ i Ä‘áº·t: pip install underthesea hoáº·c pip install pyvi

# Option 1: Sá»­ dá»¥ng underthesea (recommended)
from underthesea import word_tokenize

# Option 2: Sá»­ dá»¥ng pyvi
# from pyvi import ViTokenizer

class VietnameseTokenizer:
    """
    TÃ¡ch tá»« tiáº¿ng Viá»‡t sá»­ dá»¥ng underthesea/pyvi
    """
    
    def __init__(self, library='underthesea'):
        """
        Args:
            library: 'underthesea' hoáº·c 'pyvi'
        """
        self.library = library
        
        if library == 'underthesea':
            from underthesea import word_tokenize
            self._tokenize_func = word_tokenize
        elif library == 'pyvi':
            from pyvi import ViTokenizer
            self._tokenize_func = lambda x: ViTokenizer.tokenize(x).split()
        else:
            # Fallback: simple split
            self._tokenize_func = lambda x: x.split()
    
    def tokenize(self, text):
        """
        TÃ¡ch tá»« tiáº¿ng Viá»‡t
        
        Args:
            text: Chuá»—i vÄƒn báº£n
            
        Returns:
            list: Danh sÃ¡ch tokens
        """
        if not isinstance(text, str) or not text.strip():
            return []
        
        try:
            # TÃ¡ch tá»«
            if self.library == 'underthesea':
                # underthesea tráº£ vá» list
                tokens = self._tokenize_func(text)
            else:
                tokens = self._tokenize_func(text)
            
            # Chuáº©n hÃ³a tokens
            cleaned_tokens = []
            for token in tokens:
                # Loáº¡i bá» token rá»—ng
                token = token.strip()
                if token:
                    # Giá»¯ nguyÃªn cÃ¡c token Ä‘áº·c biá»‡t (<URL>, <PHONE>, etc.)
                    if token.startswith('<') and token.endswith('>'):
                        cleaned_tokens.append(token)
                    else:
                        # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ underscore (ngÃ¢n_hÃ ng â†’ ngÃ¢n_hÃ ng)
                        cleaned_tokens.append(token)
            
            return cleaned_tokens
            
        except Exception as e:
            # Fallback náº¿u lá»—i
            return text.split()
    
    def tokenize_with_positions(self, text):
        """
        TÃ¡ch tá»« vÃ  giá»¯ vá»‹ trÃ­
        
        Returns:
            list of tuples: [(token, start_pos, end_pos), ...]
        """
        tokens = self.tokenize(text)
        result = []
        current_pos = 0
        
        for token in tokens:
            # TÃ¬m vá»‹ trÃ­ token trong text gá»‘c
            start = text.find(token.replace('_', ' '), current_pos)
            if start == -1:
                start = text.find(token, current_pos)
            if start == -1:
                start = current_pos
            
            end = start + len(token.replace('_', ' '))
            result.append((token, start, end))
            current_pos = end
        
        return result
```

---

## 2.6. TÃ­ch Há»£p Táº§ng 2 HoÃ n Chá»‰nh

```python
class NormalizationTokenizer:
    """
    Táº§ng 2: Normalization & Tokenization
    Káº¿t há»£p Unicode normalization + Leetspeak decode + Word tokenization
    """
    
    def __init__(self, tokenizer_library='underthesea'):
        self.leet_decoder = LeetSpeakDecoder()
        self.tokenizer = VietnameseTokenizer(library=tokenizer_library)
    
    def process(self, text):
        """
        Xá»­ lÃ½ vÄƒn báº£n qua Táº§ng 2
        
        Args:
            text: Chuá»—i vÄƒn báº£n (Ä‘Ã£ qua Táº§ng 1 masking)
            
        Returns:
            dict: {
                'tokens': list of tokens,
                'leet_count': sá»‘ kÃ½ tá»± leetspeak,
                'normalized_text': vÄƒn báº£n Ä‘Ã£ chuáº©n hÃ³a (debug)
            }
        """
        if not isinstance(text, str) or not text.strip():
            return {
                'tokens': [],
                'leet_count': 0,
                'normalized_text': text
            }
        
        # BÆ°á»›c 1: Unicode normalization
        normalized = normalize_unicode(text)
        
        # BÆ°á»›c 2: Leetspeak decode
        decoded, leet_count = self.leet_decoder.decode(normalized)
        
        # BÆ°á»›c 3: Word tokenization
        tokens = self.tokenizer.tokenize(decoded)
        
        return {
            'tokens': tokens,
            'leet_count': leet_count,
            'normalized_text': decoded
        }
```

---

## 2.7. Test Cases cho Táº§ng 2

```python
def test_layer2():
    processor = NormalizationTokenizer(tokenizer_library='underthesea')
    
    test_cases = [
        # Test Leetspeak
        (
            "Kh0ng pha! ban dang nhap",
            {'expected_leet': 2}  # 0â†’o, !â†’i
        ),
        (
            "D!eu k!en nh4n t!en",
            {'expected_leet': 4}  # 4 láº§n !â†’i, 4â†’a
        ),
        
        # Test Unicode normalization
        (
            "NgÃ¢n  hÃ ng   ACB",  # Multiple spaces
            {'expected_tokens_contain': ['ngÃ¢n', 'hÃ ng', 'ACB']}
        ),
        
        # Test Tokenization vá»›i tá»« ghÃ©p
        (
            "Tai khoan ngan hang",
            {'expected_tokens_contain': ['tÃ i_khoáº£n', 'ngÃ¢n_hÃ ng']}  # hoáº·c ['tai', 'khoan', 'ngan', 'hang'] tÃ¹y tokenizer
        ),
        
        # Test giá»¯ nguyÃªn entity tokens
        (
            "Dang nhap <URL> de nhan <MONEY>",
            {'expected_tokens_contain': ['<URL>', '<MONEY>']}
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 2: NORMALIZATION & TOKENIZATION")
    print("=" * 60)
    
    for i, (input_text, expected) in enumerate(test_cases):
        result = processor.process(input_text)
        
        print(f"\nTest {i+1}:")
        print(f"  Input:      {input_text}")
        print(f"  Tokens:     {result['tokens']}")
        print(f"  Leet count: {result['leet_count']}")
        
        if 'expected_leet' in expected:
            status = "âœ“" if result['leet_count'] == expected['expected_leet'] else "âœ—"
            print(f"  Leet check: {status}")

# test_layer2()
```

---

## 2.8. Dependencies cho Táº§ng 2

```bash
# CÃ i Ä‘áº·t thÆ° viá»‡n tÃ¡ch tá»« tiáº¿ng Viá»‡t
pip install underthesea

# Hoáº·c dÃ¹ng pyvi (nháº¹ hÆ¡n)
pip install pyvi
```

---

## 2.9. Checklist HoÃ n ThÃ nh Táº§ng 2

- [ ] Implement `normalize_unicode()` 
- [ ] Implement class `LeetSpeakDecoder`
  - [ ] Test vá»›i cÃ¡c pattern: `0`, `!`, `1`, `3`, `@`
  - [ ] Äáº£m báº£o khÃ´ng decode sá»‘ Ä‘á»©ng má»™t mÃ¬nh
- [ ] Implement class `VietnameseTokenizer`
  - [ ] CÃ i Ä‘áº·t underthesea hoáº·c pyvi
  - [ ] Test tÃ¡ch tá»« ghÃ©p tiáº¿ng Viá»‡t
- [ ] Implement class tÃ­ch há»£p `NormalizationTokenizer`
- [ ] Test vá»›i vÄƒn báº£n cÃ³ entity tokens (`<URL>`, `<PHONE>`)

---

---

# Táº¦NG 3: WHITELIST FILTERING

---

## 3.1. Má»¥c TiÃªu

Lá»c bá» cÃ¡c token **Ä‘áº·c thÃ¹** khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn tiáº¿ng Viá»‡t nhÆ°ng **há»£p lá»‡** trong ngá»¯ cáº£nh SMS:
- TÃªn thÆ°Æ¡ng hiá»‡u (brand names)
- Thuáº­t ngá»¯ chuyÃªn ngÃ nh (jargon)
- Viáº¿t táº¯t, teencode phá»• biáº¿n
- Entity tokens tá»« Táº§ng 1

---

## 3.2. Input / Output

| | MÃ´ táº£ |
|---|---|
| **Input** | Danh sÃ¡ch tokens tá»« Táº§ng 2 |
| **Output** | Danh sÃ¡ch tokens cáº§n kiá»ƒm tra chÃ­nh táº£ (Ä‘Ã£ loáº¡i whitelist) |

**VÃ­ dá»¥:**
```
Input tokens:  ["vcb", "thong", "bao", "tai", "khoan", "otp", "123456", "<URL>"]
After filter:  ["thong", "bao", "tai", "khoan"]
Filtered out:  ["vcb" (brand), "otp" (jargon), "123456" (number), "<URL>" (entity)]
```

---

## 3.3. Cáº¥u TrÃºc Whitelist

```python
class WhitelistFilter:
    """
    Táº§ng 3: Whitelist Filtering
    Lá»c bá» cÃ¡c token há»£p lá»‡ khÃ´ng cáº§n kiá»ƒm tra chÃ­nh táº£
    """
    
    def __init__(self, custom_whitelist_path=None):
        # Khá»Ÿi táº¡o cÃ¡c danh sÃ¡ch whitelist
        self.brand_list = self._build_brand_list()
        self.jargon_list = self._build_jargon_list()
        self.slang_abbr_list = self._build_slang_abbr_list()
        self.entity_tokens = self._build_entity_tokens()
        
        # Gá»™p táº¥t cáº£ thÃ nh má»™t set Ä‘á»ƒ tra cá»©u O(1)
        self.whitelist = set()
        self.whitelist.update(self.brand_list)
        self.whitelist.update(self.jargon_list)
        self.whitelist.update(self.slang_abbr_list)
        self.whitelist.update(self.entity_tokens)
        
        # Load custom whitelist náº¿u cÃ³
        if custom_whitelist_path:
            self._load_custom_whitelist(custom_whitelist_path)
        
        print(f"âœ“ Whitelist loaded: {len(self.whitelist)} items")
    
    def _build_brand_list(self):
        """Danh sÃ¡ch tÃªn thÆ°Æ¡ng hiá»‡u, ngÃ¢n hÃ ng, vÃ­ Ä‘iá»‡n tá»­, telco, ecommerce"""
        return {
            # === NGÃ‚N HÃ€NG ===
            'vcb', 'vietcombank', 'acb', 'bidv', 'vietinbank', 'viettinbank',
            'techcombank', 'tcb', 'mbbank', 'mb', 'tpbank', 'vpbank',
            'sacombank', 'scb', 'shb', 'hdbank', 'ocb', 'msb',
            'seabank', 'abbank', 'vib', 'eximbank', 'namabank',
            'agribank', 'lienvietpostbank', 'pvcombank', 'bacabank',
            'vietabank', 'ncb', 'pgbank', 'baovietbank', 'cbbank',
            'gpbank', 'kienlongbank', 'vietbank', 'wooribank',
            'uob', 'hsbc', 'citibank', 'standardchartered', 'shinhan',
            'digibank', 'smartbanking', 'ebanking', 'ibanking', 'mbanking',
            
            # === VÃ ÄIá»†N Tá»¬ / FINTECH ===
            'momo', 'zalopay', 'viettelpay', 'vnpay', 'moca', 'airpay',
            'shoppeepay', 'grabpay', 'payoo', 'vtcpay', 'appotapay',
            'fecredit', 'homecredit', 'tpfinance', 'fegreenfinance',
            'mirae', 'credilo', 'tima', 'cash24', 'doctordong', 'robocash',
            
            # === VIá»„N THÃ”NG ===
            'viettel', 'vinaphone', 'mobifone', 'vnmobile', 'gmobile',
            'vietnamobile', 'itel', 'reddi', 'wintel', 'indochina',
            
            # === THÆ¯Æ NG Máº I ÄIá»†N Tá»¬ ===
            'shopee', 'lazada', 'tiki', 'sendo', 'thegioididong', 'tgdd',
            'fptshop', 'cellphones', 'dienmayxanh', 'dmx', 'bachhoaxanh',
            'concung', 'guardian', 'watsons', 'lotte', 'aeon', 'bigc',
            'coopmart', 'vinmart', 'winmart', 'baemin', 'grabfood',
            'gojek', 'beamin', 'loship', 'ahamove', 'lalamove',
            'grab', 'be', 'gojek', 'xanh sm', 'mai linh', 'vinasun',
            
            # === Máº NG XÃƒ Há»˜I / APP ===
            'facebook', 'fb', 'zalo', 'telegram', 'viber', 'whatsapp',
            'tiktok', 'youtube', 'instagram', 'twitter', 'linkedin',
            'google', 'gmail', 'yahoo', 'outlook', 'icloud',
            
            # === THÆ¯Æ NG HIá»†U KHÃC ===
            'apple', 'iphone', 'samsung', 'oppo', 'xiaomi', 'vivo',
            'huawei', 'realme', 'nokia', 'sony', 'lg', 'asus',
            'vingroup', 'vinfast', 'vinhomes', 'vincom', 'vinmec',
            'sungroup', 'novaland', 'fpt', 'vnpt', 'evn',
        }
    
    def _build_jargon_list(self):
        """Thuáº­t ngá»¯ chuyÃªn ngÃ nh: viá»…n thÃ´ng, tÃ i chÃ­nh, marketing, IT"""
        return {
            # === VIá»„N THÃ”NG ===
            'sim', '4g', '5g', '3g', 'lte', 'data', 'wifi', 'internet',
            'roaming', 'sms', 'mms', 'ussd', 'gsm', 'cdma',
            'prepaid', 'postpaid', 'topup', 'náº¡p', 'goi', 'cuoc',
            
            # === TÃ€I CHÃNH ===
            'otp', 'pin', 'cvv', 'atm', 'pos', 'qr', 'nfc',
            'visa', 'mastercard', 'jcb', 'napas', 'swift', 'iban',
            'credit', 'debit', 'loan', 'mortgage', 'interest',
            'transfer', 'withdraw', 'deposit', 'balance',
            'transaction', 'payment', 'refund', 'cashback',
            
            # === MARKETING / KHUYáº¾N MÃƒI ===
            'voucher', 'coupon', 'deal', 'sale', 'flash', 'discount',
            'promo', 'promotion', 'km', 'khuyen', 'mai', 'freeship',
            'cod', 'ghn', 'ghtk', 'jt', 'ninja', 'best', 'viettel post',
            
            # === IT / Ká»¸ THUáº¬T ===
            'app', 'web', 'link', 'url', 'click', 'download', 'update',
            'login', 'logout', 'signup', 'register', 'verify', 'confirm',
            'password', 'username', 'email', 'account', 'profile',
            'online', 'offline', 'server', 'system', 'database',
            'id', 'ip', 'api', 'sdk', 'ios', 'android', 'windows',
            
            # === ÄÆ N Vá»Š / VIáº¾T Táº®T ===
            'vnd', 'vnÄ‘', 'usd', 'eur', 'btc', 'usdt', 'eth',
            'kg', 'km', 'cm', 'mm', 'm2', 'm3', 'gb', 'mb', 'kb',
            'tp', 'hcm', 'hn', 'dn', 'hp', 'ct', 'bd', 'bt',
            'bhxh', 'bhyt', 'bhtn', 'cmnd', 'cccd', 'gplx',
        }
    
    def _build_slang_abbr_list(self):
        """Viáº¿t táº¯t, teencode phá»• biáº¿n trong SMS tiáº¿ng Viá»‡t"""
        return {
            # === VIáº¾T Táº®T PHá»” BIáº¾N ===
            'lh', 'dt', 'sdt', 'Ä‘t', 'sÄ‘t',  # liÃªn há»‡, Ä‘iá»‡n thoáº¡i
            'tk', 'stk',  # tÃ i khoáº£n, sá»‘ tÃ i khoáº£n
            'qc', 'tb', 'tt',  # quáº£ng cÃ¡o, thÃ´ng bÃ¡o, thanh toÃ¡n
            'ck', 'hs', 'hd',  # chuyá»ƒn khoáº£n, há»“ sÆ¡, há»£p Ä‘á»“ng
            'kh', 'nv', 'ql',  # khÃ¡ch hÃ ng, nhÃ¢n viÃªn, quáº£n lÃ½
            'cty', 'cong ty', 'cskh',  # cÃ´ng ty, chÄƒm sÃ³c khÃ¡ch hÃ ng
            'pv', 'gd', 'kd',  # phá»¥c vá»¥, giao dá»‹ch, kinh doanh
            'nd', 'mgd', 'ref',  # ná»™i dung, mÃ£ giao dá»‹ch, reference
            
            # === TEENCODE / SLANG ===
            'ko', 'k', 'khong',  # khÃ´ng
            'dc', 'Ä‘c', 'duoc',  # Ä‘Æ°á»£c
            'r', 'roi', 'rui',  # rá»“i
            'nha', 'nhe', 'nhÃ©',  # nhÃ©
            'a', 'e', 'em', 'anh', 'chi', 'chá»‹',  # xÆ°ng hÃ´
            'mk', 'mik', 'minh',  # mÃ¬nh
            'bn', 'báº¡n',  # báº¡n
            'ns', 'noi', 'nÃ³i',  # nÃ³i
            'nt', 'nhan', 'nháº¯n',  # nháº¯n tin
            'ib', 'inbox',  # inbox
            'ok', 'okie', 'oki', 'okÃª',
            'tks', 'thanks', 'thank', 'cam', 'on', 'Æ¡n',
            'sr', 'sorry', 'xin', 'loi', 'lá»—i',
            'pls', 'please', 'vui', 'long', 'lÃ²ng',
            'btw', 'fyi', 'asap', 'aka',
            
            # === CÃCH VIáº¾T KHÃ”NG Dáº¤U PHá»” BIáº¾N ===
            'vui', 'long', 'vao', 'de', 'duoc', 'khong',
            'ngay', 'hom', 'nay', 'mai', 'qua',
            'tien', 'nhan', 'gui', 'chuyen',
        }
    
    def _build_entity_tokens(self):
        """CÃ¡c token entity tá»« Táº§ng 1"""
        return {
            '<url>', '<phone>', '<money>', '<code>', 
            '<datetime>', '<zalo>', '<email>',
            # ThÃªm cáº£ dáº¡ng viáº¿t hoa
            '<URL>', '<PHONE>', '<MONEY>', '<CODE>',
            '<DATETIME>', '<ZALO>', '<EMAIL>',
        }
    
    def _load_custom_whitelist(self, path):
        """Load whitelist tÃ¹y chá»‰nh tá»« file"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip().lower()
                    if word and not word.startswith('#'):
                        self.whitelist.add(word)
        except FileNotFoundError:
            print(f"Warning: Custom whitelist not found: {path}")
    
    def is_whitelisted(self, token):
        """
        Kiá»ƒm tra token cÃ³ trong whitelist khÃ´ng
        
        Args:
            token: Má»™t token Ä‘Æ¡n láº»
            
        Returns:
            bool: True náº¿u token trong whitelist
        """
        token_lower = token.lower().strip()
        
        # Check 1: Trong whitelist set
        if token_lower in self.whitelist:
            return True
        
        # Check 2: Token lÃ  sá»‘ thuáº§n tÃºy
        if token_lower.isdigit():
            return True
        
        # Check 3: Token lÃ  entity (báº¯t Ä‘áº§u vÃ  káº¿t thÃºc báº±ng <>)
        if token.startswith('<') and token.endswith('>'):
            return True
        
        # Check 4: Token quÃ¡ ngáº¯n (1 kÃ½ tá»±) - thÆ°á»ng lÃ  noise
        if len(token_lower) <= 1:
            return True
        
        # Check 5: Token chá»‰ chá»©a kÃ½ tá»± Ä‘áº·c biá»‡t
        if not any(c.isalnum() for c in token):
            return True
        
        return False
    
    def filter(self, tokens):
        """
        Lá»c danh sÃ¡ch tokens, tráº£ vá» tokens cáº§n kiá»ƒm tra chÃ­nh táº£
        
        Args:
            tokens: List cÃ¡c tokens tá»« Táº§ng 2
            
        Returns:
            dict: {
                'tokens_to_check': tokens cáº§n kiá»ƒm tra,
                'whitelisted_tokens': tokens Ä‘Ã£ bá»‹ lá»c,
                'whitelist_count': sá»‘ tokens bá»‹ lá»c
            }
        """
        tokens_to_check = []
        whitelisted_tokens = []
        
        for token in tokens:
            if self.is_whitelisted(token):
                whitelisted_tokens.append(token)
            else:
                tokens_to_check.append(token)
        
        return {
            'tokens_to_check': tokens_to_check,
            'whitelisted_tokens': whitelisted_tokens,
            'whitelist_count': len(whitelisted_tokens)
        }
```

---

## 3.4. File Whitelist TÃ¹y Chá»‰nh (Optional)

Táº¡o file `custom_whitelist.txt` Ä‘á»ƒ thÃªm tá»« má»›i dá»… dÃ ng:

```text
# custom_whitelist.txt
# ThÃªm cÃ¡c tá»« Ä‘áº·c thÃ¹ cá»§a dá»± Ã¡n
# Má»—i tá»« má»™t dÃ²ng, comment báº¯t Ä‘áº§u báº±ng #

# TÃªn riÃªng
huit
ueh
uit
hcmus

# Thuáº­t ngá»¯ má»›i
blockchain
nft
defi
metaverse

# Brand má»›i
cake
timo
tnex
```

---

## 3.5. Test Cases cho Táº§ng 3

```python
def test_whitelist_filter():
    whitelist = WhitelistFilter()
    
    test_cases = [
        # Test brand filtering
        (
            ["vcb", "thong", "bao", "tai", "khoan"],
            {"should_filter": ["vcb"], "should_keep": ["thong", "bao", "tai", "khoan"]}
        ),
        
        # Test jargon filtering
        (
            ["nhap", "otp", "de", "xac", "nhan"],
            {"should_filter": ["otp"], "should_keep": ["nhap", "de", "xac", "nhan"]}
        ),
        
        # Test entity filtering
        (
            ["dang", "nhap", "<URL>", "de", "nhan", "<MONEY>"],
            {"should_filter": ["<URL>", "<MONEY>"], "should_keep": ["dang", "nhap", "de", "nhan"]}
        ),
        
        # Test number filtering
        (
            ["ma", "123456", "het", "han", "24", "gio"],
            {"should_filter": ["123456", "24"], "should_keep": ["ma", "het", "han", "gio"]}
        ),
        
        # Test slang filtering
        (
            ["lh", "ngay", "de", "dc", "ho", "tro"],
            {"should_filter": ["lh", "dc"], "should_keep": ["ngay", "de", "ho", "tro"]}
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 3: WHITELIST FILTERING")
    print("=" * 60)
    
    for i, (tokens, expected) in enumerate(test_cases):
        result = whitelist.filter(tokens)
        
        print(f"\nTest {i+1}:")
        print(f"  Input tokens:      {tokens}")
        print(f"  Tokens to check:   {result['tokens_to_check']}")
        print(f"  Whitelisted:       {result['whitelisted_tokens']}")

# test_whitelist_filter()
```

---

## 3.6. Checklist HoÃ n ThÃ nh Táº§ng 3

- [ ] Implement class `WhitelistFilter`
- [ ] XÃ¢y dá»±ng `brand_list` (~100+ brands)
- [ ] XÃ¢y dá»±ng `jargon_list` (~100+ thuáº­t ngá»¯)
- [ ] XÃ¢y dá»±ng `slang_abbr_list` (~50+ viáº¿t táº¯t)
- [ ] Implement logic `is_whitelisted()` vá»›i 5 checks
- [ ] Há»— trá»£ custom whitelist tá»« file
- [ ] Test vá»›i cÃ¡c trÆ°á»ng há»£p edge case

---

---

# Táº¦NG 4: DUAL-LOOKUP VALIDATION

---

## 4.1. Má»¥c TiÃªu

Kiá»ƒm tra tÃ­nh há»£p lá»‡ cá»§a tá»« tiáº¿ng Viá»‡t theo quy trÃ¬nh:
1. **Check 1**: Kiá»ƒm tra tá»« Ä‘iá»ƒn **CÃ“ Dáº¤U** (Full Dict)
2. **Check 2**: Náº¿u khÃ´ng cÃ³ â†’ Kiá»ƒm tra tá»« Ä‘iá»ƒn **KHÃ”NG Dáº¤U** (Shadow Dict)
3. **Káº¿t quáº£**: Pass cáº£ 2 check â†’ `MISSPELL`

---

## 4.2. Input / Output

| | MÃ´ táº£ |
|---|---|
| **Input** | Danh sÃ¡ch tokens Ä‘Ã£ qua Táº§ng 3 (cáº§n kiá»ƒm tra) |
| **Output** | Dictionary chá»©a káº¿t quáº£ validation |

**VÃ­ dá»¥:**
```
Input tokens: ["thong", "bao", "ta!", "khoan", "xyz123abc"]

Validation:
- "thong" â†’ Shadow Dict cÃ³ "thÃ´ng" â†’ VALID
- "bao"   â†’ Shadow Dict cÃ³ "bÃ¡o", "bao" â†’ VALID  
- "ta!"   â†’ KhÃ´ng cÃ³ trong cáº£ 2 â†’ MISSPELL
- "khoan" â†’ Shadow Dict cÃ³ "khoáº£n" â†’ VALID
- "xyz123abc" â†’ KhÃ´ng cÃ³ trong cáº£ 2 â†’ MISSPELL

Output: {
    'misspelled_tokens': ["ta!", "xyz123abc"],
    'misspelled_count': 2,
    'total_checked': 5,
    'misspelled_pct': 40.0
}
```

---

## 4.3. XÃ¢y Dá»±ng Tá»« Äiá»ƒn

### 4.3.1. HÃ m Loáº¡i Bá» Dáº¥u Tiáº¿ng Viá»‡t

```python
def remove_vietnamese_accents(text):
    """
    Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i Latin gá»‘c
    
    "ngÃ¢n hÃ ng" â†’ "ngan hang"
    "Ä‘iá»u kiá»‡n" â†’ "dieu kien"
    "Ä‘Æ°á»£c"      â†’ "duoc"
    """
    if not isinstance(text, str):
        return text
    
    # Báº£ng chuyá»ƒn Ä‘á»•i Ä‘áº§y Ä‘á»§ cho tiáº¿ng Viá»‡t
    VIETNAMESE_ACCENT_MAP = {
        # a vá»›i dáº¥u
        'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
        'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
        'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
        'Ã€': 'A', 'Ã': 'A', 'áº¢': 'A', 'Ãƒ': 'A', 'áº ': 'A',
        'Ä‚': 'A', 'áº°': 'A', 'áº®': 'A', 'áº²': 'A', 'áº´': 'A', 'áº¶': 'A',
        'Ã‚': 'A', 'áº¦': 'A', 'áº¤': 'A', 'áº¨': 'A', 'áºª': 'A', 'áº¬': 'A',
        
        # Ä‘
        'Ä‘': 'd', 'Ä': 'D',
        
        # e vá»›i dáº¥u
        'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
        'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
        'Ãˆ': 'E', 'Ã‰': 'E', 'áºº': 'E', 'áº¼': 'E', 'áº¸': 'E',
        'ÃŠ': 'E', 'á»€': 'E', 'áº¾': 'E', 'á»‚': 'E', 'á»„': 'E', 'á»†': 'E',
        
        # i vá»›i dáº¥u
        'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
        'ÃŒ': 'I', 'Ã': 'I', 'á»ˆ': 'I', 'Ä¨': 'I', 'á»Š': 'I',
        
        # o vá»›i dáº¥u
        'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
        'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
        'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
        'Ã’': 'O', 'Ã“': 'O', 'á»': 'O', 'Ã•': 'O', 'á»Œ': 'O',
        'Ã”': 'O', 'á»’': 'O', 'á»': 'O', 'á»”': 'O', 'á»–': 'O', 'á»˜': 'O',
        'Æ ': 'O', 'á»œ': 'O', 'á»š': 'O', 'á»': 'O', 'á» ': 'O', 'á»¢': 'O',
        
        # u vá»›i dáº¥u
        'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
        'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
        'Ã™': 'U', 'Ãš': 'U', 'á»¦': 'U', 'Å¨': 'U', 'á»¤': 'U',
        'Æ¯': 'U', 'á»ª': 'U', 'á»¨': 'U', 'á»¬': 'U', 'á»®': 'U', 'á»°': 'U',
        
        # y vá»›i dáº¥u
        'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
        'á»²': 'Y', 'Ã': 'Y', 'á»¶': 'Y', 'á»¸': 'Y', 'á»´': 'Y',
    }
    
    result = []
    for char in text:
        result.append(VIETNAMESE_ACCENT_MAP.get(char, char))
    
    return ''.join(result)
```

### 4.3.2. Class DualLookupValidator

```python
import json

class DualLookupValidator:
    """
    Táº§ng 4: Dual-Lookup Validation
    Kiá»ƒm tra tá»« Ä‘iá»ƒn CÃ³ dáº¥u â†’ KhÃ´ng dáº¥u
    """
    
    def __init__(self, words_file_path):
        """
        Args:
            words_file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file words.txt
                            Format: {"text": "tá»«", "source": [...]}
        """
        print("Loading Vietnamese dictionaries...")
        
        # BÆ°á»›c 1: Load vÃ  build Full Dict (cÃ³ dáº¥u)
        self.full_dict = self._build_full_dict(words_file_path)
        print(f"  âœ“ Full Dict: {len(self.full_dict):,} words")
        
        # BÆ°á»›c 2: Build Shadow Dict (khÃ´ng dáº¥u) tá»« Full Dict
        self.shadow_dict = self._build_shadow_dict()
        print(f"  âœ“ Shadow Dict: {len(self.shadow_dict):,} words")
        
        print("âœ“ Dictionaries loaded successfully!")
    
    def _build_full_dict(self, path):
        """
        XÃ¢y dá»±ng tá»« Ä‘iá»ƒn cÃ³ dáº¥u tá»« file words.txt
        """
        full_dict = set()
        
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                try:
                    # Parse JSON
                    entry = json.loads(line)
                    word = entry.get('text', '').strip()
                    
                    if word:
                        # ThÃªm dáº¡ng lowercase
                        full_dict.add(word.lower())
                        
                        # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ gáº¡ch ná»‘i
                        # "ngÃ¢n-hÃ ng" â†’ thÃªm cáº£ "ngÃ¢n hÃ ng" vÃ  "ngÃ¢n_hÃ ng"
                        if '-' in word:
                            full_dict.add(word.lower().replace('-', ' '))
                            full_dict.add(word.lower().replace('-', '_'))
                        
                        # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ khoáº£ng tráº¯ng
                        # "ngÃ¢n hÃ ng" â†’ thÃªm cáº£ "ngÃ¢n_hÃ ng"
                        if ' ' in word:
                            full_dict.add(word.lower().replace(' ', '_'))
                            
                except json.JSONDecodeError:
                    # Náº¿u khÃ´ng pháº£i JSON, thá»­ Ä‘á»c nhÆ° plain text
                    word = line.strip()
                    if word:
                        full_dict.add(word.lower())
        
        return full_dict
    
    def _build_shadow_dict(self):
        """
        XÃ¢y dá»±ng Shadow Dict (khÃ´ng dáº¥u) tá»« Full Dict
        """
        shadow_dict = set()
        
        for word in self.full_dict:
            # Chuyá»ƒn vá» dáº¡ng khÃ´ng dáº¥u
            no_accent = remove_vietnamese_accents(word.lower())
            shadow_dict.add(no_accent)
        
        return shadow_dict
    
    def validate_token(self, token):
        """
        Kiá»ƒm tra má»™t token
        
        Args:
            token: Má»™t token Ä‘Æ¡n láº»
            
        Returns:
            tuple: (is_valid, match_type)
                - is_valid: bool
                - match_type: 'full' | 'shadow' | 'misspell'
        """
        token_lower = token.lower().strip()
        
        # Bá» qua token rá»—ng
        if not token_lower:
            return (True, 'skip')
        
        # CHECK 1: Tá»« Ä‘iá»ƒn cÃ³ dáº¥u (Full Dict)
        if token_lower in self.full_dict:
            return (True, 'full')
        
        # Xá»­ lÃ½ tá»« ghÃ©p vá»›i underscore
        # "ngan_hang" â†’ check cáº£ "ngan hang"
        if '_' in token_lower:
            token_with_space = token_lower.replace('_', ' ')
            if token_with_space in self.full_dict:
                return (True, 'full')
        
        # CHECK 2: Tá»« Ä‘iá»ƒn khÃ´ng dáº¥u (Shadow Dict)
        token_no_accent = remove_vietnamese_accents(token_lower)
        
        if token_no_accent in self.shadow_dict:
            return (True, 'shadow')
        
        # Xá»­ lÃ½ tá»« ghÃ©p
        if '_' in token_no_accent:
            token_with_space = token_no_accent.replace('_', ' ')
            if token_with_space in self.shadow_dict:
                return (True, 'shadow')
        
        # KHÃ”NG match cáº£ 2 â†’ MISSPELL
        return (False, 'misspell')
    
    def validate_tokens(self, tokens):
        """
        Kiá»ƒm tra danh sÃ¡ch tokens
        
        Args:
            tokens: List cÃ¡c tokens cáº§n kiá»ƒm tra
            
        Returns:
            dict: {
                'misspelled_tokens': list cÃ¡c tá»« sai chÃ­nh táº£,
                'misspelled_count': sá»‘ tá»« sai,
                'valid_full_count': sá»‘ tá»« match Full Dict,
                'valid_shadow_count': sá»‘ tá»« match Shadow Dict,
                'total_checked': tá»•ng sá»‘ tá»« Ä‘Ã£ check,
                'misspelled_pct': % tá»« sai
            }
        """
        misspelled_tokens = []
        valid_full_count = 0
        valid_shadow_count = 0
        total_checked = 0
        
        for token in tokens:
            is_valid, match_type = self.validate_token(token)
            
            if match_type == 'skip':
                continue
            
            total_checked += 1
            
            if match_type == 'full':
                valid_full_count += 1
            elif match_type == 'shadow':
                valid_shadow_count += 1
            else:  # misspell
                misspelled_tokens.append(token)
        
        misspelled_count = len(misspelled_tokens)
        misspelled_pct = (misspelled_count / total_checked * 100) if total_checked > 0 else 0.0
        
        return {
            'misspelled_tokens': misspelled_tokens,
            'misspelled_count': misspelled_count,
            'valid_full_count': valid_full_count,
            'valid_shadow_count': valid_shadow_count,
            'total_checked': total_checked,
            'misspelled_pct': round(misspelled_pct, 2)
        }
```

---

## 4.4. Test Cases cho Táº§ng 4

```python
def test_dual_lookup():
    # Giáº£ sá»­ Ä‘Ã£ cÃ³ file words.txt
    validator = DualLookupValidator('data/dicts/words.txt')
    
    test_cases = [
        # Test tá»« cÃ³ dáº¥u â†’ match Full Dict
        (
            ["ngÃ¢n", "hÃ ng", "thÃ´ng", "bÃ¡o"],
            {'expected_full': 4, 'expected_misspell': 0}
        ),
        
        # Test tá»« khÃ´ng dáº¥u â†’ match Shadow Dict
        (
            ["ngan", "hang", "thong", "bao"],
            {'expected_shadow': 4, 'expected_misspell': 0}
        ),
        
        # Test tá»« sai chÃ­nh táº£
        (
            ["ngÃ¢n", "hÃ ng", "xyzabc", "abcdef"],
            {'expected_misspell': 2}
        ),
        
        # Test há»—n há»£p
        (
            ["tai", "khoan", "abc123xyz", "da", "bi", "khoa"],
            {'expected_misspell': 1}  # chá»‰ "abc123xyz" sai
        ),
        
        # Test tá»« ghÃ©p
        (
            ["ngÃ¢n_hÃ ng", "tai_khoan"],
            {'expected_valid': 2}  # cáº£ 2 Ä‘á»u há»£p lá»‡
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 4: DUAL-LOOKUP VALIDATION")
    print("=" * 60)
    
    for i, (tokens, expected) in enumerate(test_cases):
        result = validator.validate_tokens(tokens)
        
        print(f"\nTest {i+1}:")
        print(f"  Input tokens:     {tokens}")
        print(f"  Misspelled:       {result['misspelled_tokens']}")
        print(f"  Full Dict match:  {result['valid_full_count']}")
        print(f"  Shadow Dict match:{result['valid_shadow_count']}")
        print(f"  Misspelled %:     {result['misspelled_pct']}%")

# test_dual_lookup()
```

---

## 4.5. Checklist HoÃ n ThÃ nh Táº§ng 4

- [ ] Implement `remove_vietnamese_accents()` vá»›i Ä‘áº§y Ä‘á»§ báº£ng chuyá»ƒn Ä‘á»•i
- [ ] Implement class `DualLookupValidator`
  - [ ] `_build_full_dict()` - load tá»« file words.txt
  - [ ] `_build_shadow_dict()` - táº¡o tá»« Full Dict
  - [ ] `validate_token()` - kiá»ƒm tra 1 token
  - [ ] `validate_tokens()` - kiá»ƒm tra batch
- [ ] Test vá»›i cÃ¡c trÆ°á»ng há»£p:
  - [ ] Tá»« cÃ³ dáº¥u
  - [ ] Tá»« khÃ´ng dáº¥u  
  - [ ] Tá»« ghÃ©p (cÃ³ `_` hoáº·c `-`)
  - [ ] Tá»« sai chÃ­nh táº£ / gibberish
- [ ] Verify dict sizes (Full: ~79k, Shadow: ~50-60k)

---

---

# TÃCH Há»¢P TOÃ€N Bá»˜ PIPELINE

---

## Pipeline Class HoÃ n Chá»‰nh

```python
class MisspellingDetector:
    """
    Pipeline hoÃ n chá»‰nh phÃ¡t hiá»‡n lá»—i chÃ­nh táº£ trong SMS tiáº¿ng Viá»‡t
    Kiáº¿n trÃºc 4 táº§ng:
        Táº§ng 1: Aggressive Masking
        Táº§ng 2: Normalization & Tokenization
        Táº§ng 3: Whitelist Filtering
        Táº§ng 4: Dual-Lookup Validation
    """
    
    def __init__(self, words_file_path, custom_whitelist_path=None):
        """
        Khá»Ÿi táº¡o pipeline
        
        Args:
            words_file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file tá»« Ä‘iá»ƒn tiáº¿ng Viá»‡t
            custom_whitelist_path: (Optional) ÄÆ°á»ng dáº«n Ä‘áº¿n whitelist tÃ¹y chá»‰nh
        """
        print("=" * 60)
        print("INITIALIZING MISSPELLING DETECTOR")
        print("=" * 60)
        
        # Táº§ng 1
        print("\n[Layer 1] Loading Aggressive Masker...")
        self.masker = AggressiveMasker()
        
        # Táº§ng 2
        print("\n[Layer 2] Loading Normalizer & Tokenizer...")
        self.normalizer = NormalizationTokenizer(tokenizer_library='underthesea')
        
        # Táº§ng 3
        print("\n[Layer 3] Loading Whitelist Filter...")
        self.whitelist = WhitelistFilter(custom_whitelist_path)
        
        # Táº§ng 4
        print("\n[Layer 4] Loading Dual-Lookup Validator...")
        self.validator = DualLookupValidator(words_file_path)
        
        print("\n" + "=" * 60)
        print("âœ“ MISSPELLING DETECTOR READY!")
        print("=" * 60)
    
    def extract_features(self, text):
        """
        TrÃ­ch xuáº¥t features lá»—i chÃ­nh táº£ tá»« vÄƒn báº£n
        
        Args:
            text: Chuá»—i vÄƒn báº£n SMS
            
        Returns:
            dict: {
                # Tá»« Táº§ng 1
                'entity_counts': {...},
                
                # Tá»« Táº§ng 2
                'leet_count': int,
                'leet_pct': float,
                
                # Tá»« Táº§ng 4
                'misspelled_count': int,
                'misspelled_pct': float,
                'misspelled_tokens': [...],
                
                # Tá»•ng há»£p
                'total_tokens': int,
            }
        """
        if not isinstance(text, str) or not text.strip():
            return self._empty_result()
        
        # ===== Táº¦NG 1: AGGRESSIVE MASKING =====
        masked_text, entity_counts = self.masker.mask(text)
        
        # ===== Táº¦NG 2: NORMALIZATION & TOKENIZATION =====
        norm_result = self.normalizer.process(masked_text)
        tokens = norm_result['tokens']
        leet_count = norm_result['leet_count']
        
        # ===== Táº¦NG 3: WHITELIST FILTERING =====
        filter_result = self.whitelist.filter(tokens)
        tokens_to_check = filter_result['tokens_to_check']
        
        # ===== Táº¦NG 4: DUAL-LOOKUP VALIDATION =====
        validation_result = self.validator.validate_tokens(tokens_to_check)
        
        # ===== Tá»”NG Há»¢P Káº¾T QUáº¢ =====
        total_tokens = len(tokens)
        leet_pct = (leet_count / len(text) * 100) if len(text) > 0 else 0.0
        
        return {
            # Entity counts tá»« Táº§ng 1
            'entity_counts': entity_counts,
            'url_count': entity_counts.get('url', 0),
            'phone_count': entity_counts.get('phone', 0),
            'money_count': entity_counts.get('money', 0),
            'code_count': entity_counts.get('code', 0),
            
            # Leetspeak tá»« Táº§ng 2
            'leet_count': leet_count,
            'leet_pct': round(leet_pct, 2),
            
            # Whitelist tá»« Táº§ng 3
            'whitelist_count': filter_result['whitelist_count'],
            
            # Misspelling tá»« Táº§ng 4
            'misspelled_count': validation_result['misspelled_count'],
            'misspelled_pct': validation_result['misspelled_pct'],
            'misspelled_tokens': validation_result['misspelled_tokens'],
            
            # Tá»•ng há»£p
            'total_tokens': total_tokens,
            'tokens_checked': validation_result['total_checked'],
        }
    
    def _empty_result(self):
        """Tráº£ vá» káº¿t quáº£ rá»—ng cho vÄƒn báº£n khÃ´ng há»£p lá»‡"""
        return {
            'entity_counts': {},
            'url_count': 0,
            'phone_count': 0,
            'money_count': 0,
            'code_count': 0,
            'leet_count': 0,
            'leet_pct': 0.0,
            'whitelist_count': 0,
            'misspelled_count': 0,
            'misspelled_pct': 0.0,
            'misspelled_tokens': [],
            'total_tokens': 0,
            'tokens_checked': 0,
        }
    
    def process_dataframe(self, df, text_column='content'):
        """
        Xá»­ lÃ½ toÃ n bá»™ DataFrame
        
        Args:
            df: pandas DataFrame
            text_column: TÃªn cá»™t chá»©a vÄƒn báº£n
            
        Returns:
            DataFrame vá»›i cÃ¡c cá»™t features má»›i
        """
        import pandas as pd
        from tqdm import tqdm
        
        print(f"\nProcessing {len(df)} records...")
        
        # Ãp dá»¥ng cho tá»«ng dÃ²ng
        results = []
        for text in tqdm(df[text_column], desc="Extracting features"):
            results.append(self.extract_features(text))
        
        # Chuyá»ƒn thÃ nh DataFrame
        features_df = pd.DataFrame(results)
        
        # Gá»™p vá»›i DataFrame gá»‘c
        result_df = pd.concat([df.reset_index(drop=True), features_df], axis=1)
        
        print(f"âœ“ Processed {len(df)} records")
        
        return result_df
```

---

## CÃ¡ch Sá»­ Dá»¥ng

```python
# === KHá»I Táº O ===
detector = MisspellingDetector(
    words_file_path='data/dicts/words.txt',
    custom_whitelist_path='data/dicts/custom_whitelist.txt'  # optional
)

# === Xá»¬ LÃ 1 TIN NHáº®N ===
text = "Kh0ng pha! ban dang nhap https://fake-bank.com de nhan 500k. LH: 0901234567"
features = detector.extract_features(text)

print(f"Leet count: {features['leet_count']}")
print(f"Misspelled: {features['misspelled_tokens']}")
print(f"Misspelled %: {features['misspelled_pct']}%")

# === Xá»¬ LÃ DATAFRAME ===
import pandas as pd

df = pd.read_csv('data/dataset.csv')
df_with_features = detector.process_dataframe(df, text_column='content')
df_with_features.to_csv('data/dataset_with_spelling_features.csv', index=False)
```

---

## File Structure Äá» Xuáº¥t

```
misspelling_detection/
â”‚
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ layer1_masking.py           # Táº§ng 1: AggressiveMasker
â”‚
â”œâ”€â”€ layer2_normalization.py     # Táº§ng 2: NormalizationTokenizer
â”‚   â”œâ”€â”€ normalize_unicode()
â”‚   â”œâ”€â”€ LeetSpeakDecoder
â”‚   â””â”€â”€ VietnameseTokenizer
â”‚
â”œâ”€â”€ layer3_whitelist.py         # Táº§ng 3: WhitelistFilter
â”‚
â”œâ”€â”€ layer4_validation.py        # Táº§ng 4: DualLookupValidator
â”‚   â”œâ”€â”€ remove_vietnamese_accents()
â”‚   â””â”€â”€ DualLookupValidator
â”‚
â”œâ”€â”€ detector.py                 # Pipeline tÃ­ch há»£p: MisspellingDetector
â”‚
â”œâ”€â”€ utils.py                    # CÃ¡c hÃ m tiá»‡n Ã­ch chung
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_layer1.py
    â”œâ”€â”€ test_layer2.py
    â”œâ”€â”€ test_layer3.py
    â”œâ”€â”€ test_layer4.py
    â””â”€â”€ test_integration.py
```

---

## Checklist Tá»•ng Thá»ƒ

### Táº§ng 1: Aggressive Masking
- [ ] Class `AggressiveMasker`
- [ ] 7 loáº¡i patterns (URL, ZALO, EMAIL, PHONE, MONEY, DATETIME, CODE)
- [ ] Test cases

### Táº§ng 2: Normalization & Tokenization
- [ ] `normalize_unicode()`
- [ ] Class `LeetSpeakDecoder`
- [ ] Class `VietnameseTokenizer`
- [ ] Class tÃ­ch há»£p `NormalizationTokenizer`
- [ ] Test cases

### Táº§ng 3: Whitelist Filtering
- [ ] Class `WhitelistFilter`
- [ ] Brand list (~100+ items)
- [ ] Jargon list (~100+ items)
- [ ] Slang/Abbr list (~50+ items)
- [ ] Support custom whitelist file
- [ ] Test cases

### Táº§ng 4: Dual-Lookup Validation
- [ ] `remove_vietnamese_accents()`
- [ ] Class `DualLookupValidator`
- [ ] Build Full Dict tá»« words.txt
- [ ] Build Shadow Dict tá»« Full Dict
- [ ] Test cases

### TÃ­ch Há»£p
- [ ] Class `MisspellingDetector`
- [ ] Method `extract_features()`
- [ ] Method `process_dataframe()`
- [ ] Integration tests

---

ÄÃ¢y lÃ  tÃ i liá»‡u chi tiáº¿t hoÃ n chá»‰nh Ä‘á»ƒ báº¡n cÃ³ thá»ƒ tiáº¿n hÃ nh **implement tá»«ng táº§ng má»™t cÃ¡ch Ä‘á»™c láº­p** vÃ  sau Ä‘Ã³ **tÃ­ch há»£p láº¡i**. Má»—i táº§ng cÃ³ Input/Output rÃµ rÃ ng, code máº«u, test cases vÃ  checklist riÃªng.

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAW SMS TEXT                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 1: AGGRESSIVE MASKING                                        â”‚
â”‚  Che giáº¥u thá»±c thá»ƒ phi ngÃ´n ngá»¯ báº±ng Regex                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 2: NORMALIZATION & TOKENIZATION                              â”‚
â”‚  Chuáº©n hÃ³a Unicode + TÃ¡ch tá»« tiáº¿ng Viá»‡t                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 3: WHITELIST FILTERING                                       â”‚
â”‚  Lá»c bá» cÃ¡c tá»« ngoáº¡i lá»‡ há»£p lá»‡                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Táº¦NG 4: DUAL-LOOKUP VALIDATION                                    â”‚
â”‚  Kiá»ƒm tra tá»« Ä‘iá»ƒn CÃ³ dáº¥u â†’ KhÃ´ng dáº¥u                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OUTPUT FEATURES                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```plaintext
Input:  "Dang nhap https://acb.vn-fake.com de nhan 500.000VND. LH: 0901234567"
Output: "Dang nhap <URL> de nhan <MONEY>. LH: <PHONE>"
Counts: {'url': 1, 'money': 1, 'phone': 1, 'code': 0, 'datetime': 0}
```

```python
import re
from collections import OrderedDict

class AggressiveMasker:
    """
    Táº§ng 1: Aggressive Masking
    Thay tháº¿ cÃ¡c thá»±c thá»ƒ phi ngÃ´n ngá»¯ báº±ng token placeholder
    """
    
    def __init__(self):
        # OrderedDict Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»± xá»­ lÃ½ (URL trÆ°á»›c, sau Ä‘Ã³ má»›i Ä‘áº¿n cÃ¡c loáº¡i khÃ¡c)
        self.patterns = OrderedDict([
            # 1. URL - Pháº£i xá»­ lÃ½ TRÆ¯á»šC TIÃŠN (aggressive)
            ('url', re.compile(
                r'('
                # Pattern 1: URL chuáº©n vá»›i protocol
                r'(?:https?://)[^\s<>\"\']+'
                r'|'
                # Pattern 2: www
                r'(?:www\.)[^\s<>\"\']+'
                r'|'
                # Pattern 3: Shortener links
                r'(?:bit\.ly|tinyurl\.com|goo\.gl|t\.me|zalo\.me)/[^\s<>\"\']+'
                r'|'
                # Pattern 4: Domain cÃ³ khoáº£ng tráº¯ng cá»‘ Ã½ (spammer trick)
                # VÃ­ dá»¥: "acb . com", "vcb .vn"
                r'[a-zA-Z0-9][-a-zA-Z0-9]*\s*\.\s*(?:com|vn|net|org|top|xyz|icu|vip|cc|life|club|cfd|info|biz|site|online|tech|app|win|shop)(?:/[^\s]*)?'
                r'|'
                # Pattern 5: Domain khÃ´ng cÃ³ protocol nhÆ°ng cÃ³ path
                r'[a-zA-Z0-9][-a-zA-Z0-9]*\.(?:com|vn|net|org|top|xyz|icu|vip|cc|life|club|cfd)/[^\s<>\"\']+'
                r')',
                re.IGNORECASE
            )),
            
            # 2. ZALO - Link Zalo Ä‘áº·c biá»‡t
            ('zalo', re.compile(
                r'('
                r'zalo\.me/\d+'
                r'|'
                r'zalo\s*[:\-]?\s*\d{9,12}'
                r')',
                re.IGNORECASE
            )),
            
            # 3. EMAIL
            ('email', re.compile(
                r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
                re.IGNORECASE
            )),
            
            # 4. PHONE - Sá»‘ Ä‘iá»‡n thoáº¡i Viá»‡t Nam
            ('phone', re.compile(
                r'('
                # Pattern 1: Báº¯t Ä‘áº§u báº±ng +84 hoáº·c 84
                r'(?:\+?84)\s*\d[\d\s\.\-]{8,12}'
                r'|'
                # Pattern 2: Báº¯t Ä‘áº§u báº±ng 0
                r'(?<!\d)0\d{2}[\s\.\-]?\d{3}[\s\.\-]?\d{3,4}(?!\d)'
                r'|'
                # Pattern 3: Sá»‘ tá»•ng Ä‘Ã i 1800, 1900, 9xxx
                r'(?:1800|1900|19\d{2})\s*[\d\s\.\-]{4,8}'
                r'|'
                # Pattern 4: Äáº§u sá»‘ ngáº¯n (brandname SMS)
                r'(?<!\d)\d{4,6}(?!\d)'
                r')'
            )),
            
            # 5. MONEY - Sá»‘ tiá»n
            ('money', re.compile(
                r'('
                # Pattern 1: Sá»‘ + Ä‘Æ¡n vá»‹ (500k, 500K, 5tr, 5TR)
                r'\d{1,3}(?:[.,]\d{3})*\s*(?:k|K|tr|TR|trieu|triá»‡u|Trieu|Triá»‡u)\b'
                r'|'
                # Pattern 2: Sá»‘ + VND/VNÄ/Ä‘/d/dong/Ä‘á»“ng
                r'\d{1,3}(?:[.,]\d{3})*\s*(?:VND|VNÄ|vnÄ‘|vnd|Ä‘á»“ng|dong|Ä‘|d)\b'
                r'|'
                # Pattern 3: Sá»‘ lá»›n cÃ³ dáº¥u cháº¥m/pháº©y (500.000, 1,000,000)
                r'\d{1,3}(?:[.,]\d{3}){1,4}(?!\d)'
                r'|'
                # Pattern 4: Sá»‘ + ngÃ n/nghÃ¬n/triá»‡u/tá»·
                r'\d+\s*(?:ngÃ n|nghÃ¬n|ngan|nghin|triá»‡u|trieu|tá»·|ty)\b'
                r')',
                re.IGNORECASE
            )),
            
            # 6. DATETIME - NgÃ y thÃ¡ng, giá»
            ('datetime', re.compile(
                r'('
                # Pattern 1: NgÃ y thÃ¡ng DD/MM hoáº·c DD/MM/YYYY
                r'\d{1,2}[/\-\.]\d{1,2}(?:[/\-\.]\d{2,4})?'
                r'|'
                # Pattern 2: Giá» phÃºt HH:MM hoáº·c HHhMM hoáº·c HH giá»
                r'\d{1,2}\s*[hgG:]\s*\d{0,2}(?:\s*(?:phÃºt|phut|p))?' 
                r'|'
                # Pattern 3: Giá» + AM/PM
                r'\d{1,2}\s*(?:am|pm|AM|PM)'
                r')'
            )),
            
            # 7. CODE - MÃ£ OTP, mÃ£ giao dá»‹ch, mÃ£ khuyáº¿n mÃ£i
            ('code', re.compile(
                r'('
                # Pattern 1: Chá»¯ in hoa + sá»‘ (OTP123, ABC123)
                r'\b[A-Z]{2,}[0-9]+[A-Z0-9]*\b'
                r'|'
                # Pattern 2: Sá»‘ + chá»¯ in hoa (123ABC, 5K50)
                r'\b[0-9]+[A-Z]{2,}[A-Z0-9]*\b'
                r'|'
                # Pattern 3: MÃ£ cÃ³ dáº¡ng XXX-XXX hoáº·c XXX_XXX
                r'\b[A-Z0-9]{2,}[-_][A-Z0-9]{2,}\b'
                r')'
            )),
        ])
    
    def mask(self, text):
        """
        Thá»±c hiá»‡n masking trÃªn vÄƒn báº£n
        
        Args:
            text: Chuá»—i vÄƒn báº£n gá»‘c
            
        Returns:
            tuple: (masked_text, entity_counts)
        """
        if not isinstance(text, str) or not text.strip():
            return text, {name: 0 for name in self.patterns.keys()}
        
        masked_text = text
        entity_counts = {name: 0 for name in self.patterns.keys()}
        
        # Ãp dá»¥ng tá»«ng pattern theo thá»© tá»±
        for entity_name, pattern in self.patterns.items():
            # Äáº¿m sá»‘ match
            matches = pattern.findall(masked_text)
            entity_counts[entity_name] = len(matches)
            
            # Thay tháº¿ báº±ng token
            token = f'<{entity_name.upper()}>'
            masked_text = pattern.sub(token, masked_text)
        
        return masked_text, entity_counts
    
    def mask_batch(self, texts):
        """
        Mask má»™t batch vÄƒn báº£n
        
        Args:
            texts: List cÃ¡c chuá»—i vÄƒn báº£n
            
        Returns:
            tuple: (list of masked_texts, list of entity_counts)
        """
        results = [self.mask(text) for text in texts]
        masked_texts = [r[0] for r in results]
        entity_counts = [r[1] for r in results]
        return masked_texts, entity_counts
```

```python
def test_aggressive_masker():
    masker = AggressiveMasker()
    
    test_cases = [
        # Test URL
        (
            "Dang nhap https://acb.vn-fake.com ngay",
            "Dang nhap <URL> ngay",
            {'url': 1}
        ),
        (
            "Truy cap www.vietcombank . vip de nhan",
            "Truy cap <URL> de nhan",
            {'url': 1}
        ),
        (
            "Link: bit.ly/abc123 hoac tinyurl.com/xyz",
            "Link: <URL> hoac <URL>",
            {'url': 2}
        ),
        
        # Test Phone
        (
            "LH 0901234567 hoac 84 901 234 567",
            "LH <PHONE> hoac <PHONE>",
            {'phone': 2}
        ),
        (
            "Goi 1900 1234 de duoc ho tro",
            "Goi <PHONE> de duoc ho tro",
            {'phone': 1}
        ),
        
        # Test Money
        (
            "Nhan 500k hoac 500.000 VND",
            "Nhan <MONEY> hoac <MONEY>",
            {'money': 2}
        ),
        (
            "Thanh toan 2.000.000Ä‘ truoc 5 trieu",
            "Thanh toan <MONEY> truoc <MONEY>",
            {'money': 2}
        ),
        
        # Test Code
        (
            "Ma OTP: ABC123 hoac ST5K",
            "Ma OTP: <CODE> hoac <CODE>",
            {'code': 2}
        ),
        
        # Test DateTime
        (
            "Truoc 17h30 ngay 15/05/2024",
            "Truoc <DATETIME> ngay <DATETIME>",
            {'datetime': 2}
        ),
        
        # Test Combined
        (
            "ACB: Dang nhap https://acb-fake.com de nhan 500.000VND. LH: 0901234567",
            "ACB: Dang nhap <URL> de nhan <MONEY>. LH: <PHONE>",
            {'url': 1, 'money': 1, 'phone': 1}
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 1: AGGRESSIVE MASKING")
    print("=" * 60)
    
    for i, (input_text, expected_masked, expected_counts) in enumerate(test_cases):
        masked, counts = masker.mask(input_text)
        
        # Kiá»ƒm tra tá»«ng entity count
        passed = True
        for entity, expected_count in expected_counts.items():
            if counts.get(entity, 0) != expected_count:
                passed = False
                break
        
        status = "âœ“ PASS" if passed else "âœ— FAIL"
        print(f"\nTest {i+1}: {status}")
        print(f"  Input:    {input_text}")
        print(f"  Output:   {masked}")
        print(f"  Counts:   {counts}")

# Cháº¡y test
# test_aggressive_masker()
```

```plaintext
misspelled_detection/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ layer1_masking.py          # Code Táº§ng 1
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_layer1.py         # Test cases
```

```plaintext
Input:  "Kh0ng pha! ban dang nhap vu! l0ng xac nhan"
Output: 
  - tokens: ["khÃ´ng", "pháº£i", "báº¡n", "Ä‘Äƒng", "nháº­p", "vui", "lÃ²ng", "xÃ¡c", "nháº­n"]
  - leet_count: 4  (0â†’o: 2 láº§n, !â†’i: 2 láº§n)
```

```python
import unicodedata

def normalize_unicode(text):
    """
    Chuáº©n hÃ³a Unicode vá» dáº¡ng NFC (Composed)
    
    Xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p:
    - KÃ½ tá»± tá»• há»£p (combining characters)
    - CÃ¡c biáº¿n thá»ƒ Unicode cá»§a cÃ¹ng má»™t kÃ½ tá»±
    - Khoáº£ng tráº¯ng Ä‘áº·c biá»‡t
    
    Args:
        text: Chuá»—i vÄƒn báº£n gá»‘c
        
    Returns:
        Chuá»—i Ä‘Ã£ chuáº©n hÃ³a
    """
    if not isinstance(text, str):
        return text
    
    # BÆ°á»›c 1: Chuáº©n hÃ³a NFC
    text = unicodedata.normalize('NFC', text)
    
    # BÆ°á»›c 2: Thay tháº¿ cÃ¡c loáº¡i khoáº£ng tráº¯ng Ä‘áº·c biá»‡t báº±ng space thÆ°á»ng
    # \u00A0 = Non-breaking space
    # \u2000-\u200B = Various Unicode spaces
    text = re.sub(r'[\u00A0\u2000-\u200B\u202F\u205F\u3000]', ' ', text)
    
    # BÆ°á»›c 3: Loáº¡i bá» zero-width characters
    text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)
    
    # BÆ°á»›c 4: Chuáº©n hÃ³a multiple spaces thÃ nh single space
    text = re.sub(r' +', ' ', text)
    
    return text.strip()
```

```python
class LeetSpeakDecoder:
    """
    Giáº£i mÃ£ Leetspeak - chuyá»ƒn Ä‘á»•i kÃ½ tá»± thay tháº¿ vá» dáº¡ng gá»‘c
    
    Phá»• biáº¿n trong SMS spam tiáº¿ng Viá»‡t:
    - 0 â†’ o (kh0ng â†’ khÃ´ng)
    - 1 â†’ i (d1ch vu â†’ dá»‹ch vá»¥)  
    - ! â†’ i (d!eu k!en â†’ Ä‘iá»u kiá»‡n)
    - 3 â†’ e (tr3n â†’ trÃªn)
    - 4 â†’ a (kho4n â†’ khoáº£n)
    - @ â†’ a (@n â†’ Äƒn)
    - $ â†’ s ($o â†’ sá»‘)
    """
    
    def __init__(self):
        # Báº£ng chuyá»ƒn Ä‘á»•i Leetspeak â†’ Chá»¯ cÃ¡i
        # Chá»‰ Ä‘á»‹nh nghÄ©a cÃ¡c mapping phá»• biáº¿n vÃ  Ã­t gÃ¢y nháº§m láº«n
        self.leet_map = {
            '0': 'o',   # kh0ng â†’ khÃ´ng
            '1': 'i',   # d1ch â†’ dá»‹ch
            '!': 'i',   # d!eu â†’ Ä‘iá»u
            '3': 'e',   # tr3n â†’ trÃªn (Ã­t phá»• biáº¿n)
            '@': 'a',   # @n â†’ Äƒn (Ã­t phá»• biáº¿n)
        }
        
        # Pattern Ä‘á»ƒ phÃ¡t hiá»‡n leetspeak trong ngá»¯ cáº£nh tá»«
        # Chá»‰ decode khi kÃ½ tá»± náº±m trong má»™t tá»« (cÃ³ chá»¯ cÃ¡i xung quanh)
        self.leet_pattern = re.compile(
            r'(?<=[a-zA-ZÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘])'
            r'([0!13@])'
            r'(?=[a-zA-ZÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘])',
            re.IGNORECASE
        )
    
    def decode(self, text):
        """
        Giáº£i mÃ£ leetspeak trong vÄƒn báº£n
        
        Args:
            text: Chuá»—i vÄƒn báº£n
            
        Returns:
            tuple: (decoded_text, leet_count)
        """
        if not isinstance(text, str) or not text:
            return text, 0
        
        leet_count = 0
        result = []
        i = 0
        
        while i < len(text):
            char = text[i]
            
            # Kiá»ƒm tra xem cÃ³ pháº£i leetspeak khÃ´ng
            if char in self.leet_map:
                # Kiá»ƒm tra ngá»¯ cáº£nh: cÃ³ chá»¯ cÃ¡i trÆ°á»›c VÃ€ sau khÃ´ng?
                has_letter_before = (i > 0 and self._is_vietnamese_letter(text[i-1]))
                has_letter_after = (i < len(text)-1 and self._is_vietnamese_letter(text[i+1]))
                
                if has_letter_before and has_letter_after:
                    # ÄÃ¢y lÃ  leetspeak â†’ decode
                    result.append(self.leet_map[char])
                    leet_count += 1
                else:
                    # KhÃ´ng pháº£i leetspeak (vÃ­ dá»¥: sá»‘ Ä‘á»©ng má»™t mÃ¬nh)
                    result.append(char)
            else:
                result.append(char)
            
            i += 1
        
        return ''.join(result), leet_count
    
    def _is_vietnamese_letter(self, char):
        """Kiá»ƒm tra kÃ½ tá»± cÃ³ pháº£i chá»¯ cÃ¡i tiáº¿ng Viá»‡t khÃ´ng"""
        vietnamese_chars = (
            'aÃ Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­bcdÄ‘eÃ¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡fghiÃ¬Ã­á»‰Ä©á»‹jklmno'
            'Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£pqrstuÃ¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±vwxyá»³Ã½á»·á»¹á»µz'
        )
        return char.lower() in vietnamese_chars
    
    def get_leet_positions(self, text):
        """
        Tráº£ vá» vá»‹ trÃ­ cÃ¡c kÃ½ tá»± leetspeak (Ä‘á»ƒ highlight/debug)
        
        Returns:
            list of tuples: [(position, original_char, decoded_char), ...]
        """
        positions = []
        for i, char in enumerate(text):
            if char in self.leet_map:
                has_letter_before = (i > 0 and self._is_vietnamese_letter(text[i-1]))
                has_letter_after = (i < len(text)-1 and self._is_vietnamese_letter(text[i+1]))
                
                if has_letter_before and has_letter_after:
                    positions.append((i, char, self.leet_map[char]))
        
        return positions
```

```python
# Cáº§n cÃ i Ä‘áº·t: pip install underthesea hoáº·c pip install pyvi

# Option 1: Sá»­ dá»¥ng underthesea (recommended)
from underthesea import word_tokenize

# Option 2: Sá»­ dá»¥ng pyvi
# from pyvi import ViTokenizer

class VietnameseTokenizer:
    """
    TÃ¡ch tá»« tiáº¿ng Viá»‡t sá»­ dá»¥ng underthesea/pyvi
    """
    
    def __init__(self, library='underthesea'):
        """
        Args:
            library: 'underthesea' hoáº·c 'pyvi'
        """
        self.library = library
        
        if library == 'underthesea':
            from underthesea import word_tokenize
            self._tokenize_func = word_tokenize
        elif library == 'pyvi':
            from pyvi import ViTokenizer
            self._tokenize_func = lambda x: ViTokenizer.tokenize(x).split()
        else:
            # Fallback: simple split
            self._tokenize_func = lambda x: x.split()
    
    def tokenize(self, text):
        """
        TÃ¡ch tá»« tiáº¿ng Viá»‡t
        
        Args:
            text: Chuá»—i vÄƒn báº£n
            
        Returns:
            list: Danh sÃ¡ch tokens
        """
        if not isinstance(text, str) or not text.strip():
            return []
        
        try:
            # TÃ¡ch tá»«
            if self.library == 'underthesea':
                # underthesea tráº£ vá» list
                tokens = self._tokenize_func(text)
            else:
                tokens = self._tokenize_func(text)
            
            # Chuáº©n hÃ³a tokens
            cleaned_tokens = []
            for token in tokens:
                # Loáº¡i bá» token rá»—ng
                token = token.strip()
                if token:
                    # Giá»¯ nguyÃªn cÃ¡c token Ä‘áº·c biá»‡t (<URL>, <PHONE>, etc.)
                    if token.startswith('<') and token.endswith('>'):
                        cleaned_tokens.append(token)
                    else:
                        # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ underscore (ngÃ¢n_hÃ ng â†’ ngÃ¢n_hÃ ng)
                        cleaned_tokens.append(token)
            
            return cleaned_tokens
            
        except Exception as e:
            # Fallback náº¿u lá»—i
            return text.split()
    
    def tokenize_with_positions(self, text):
        """
        TÃ¡ch tá»« vÃ  giá»¯ vá»‹ trÃ­
        
        Returns:
            list of tuples: [(token, start_pos, end_pos), ...]
        """
        tokens = self.tokenize(text)
        result = []
        current_pos = 0
        
        for token in tokens:
            # TÃ¬m vá»‹ trÃ­ token trong text gá»‘c
            start = text.find(token.replace('_', ' '), current_pos)
            if start == -1:
                start = text.find(token, current_pos)
            if start == -1:
                start = current_pos
            
            end = start + len(token.replace('_', ' '))
            result.append((token, start, end))
            current_pos = end
        
        return result
```

```python
class NormalizationTokenizer:
    """
    Táº§ng 2: Normalization & Tokenization
    Káº¿t há»£p Unicode normalization + Leetspeak decode + Word tokenization
    """
    
    def __init__(self, tokenizer_library='underthesea'):
        self.leet_decoder = LeetSpeakDecoder()
        self.tokenizer = VietnameseTokenizer(library=tokenizer_library)
    
    def process(self, text):
        """
        Xá»­ lÃ½ vÄƒn báº£n qua Táº§ng 2
        
        Args:
            text: Chuá»—i vÄƒn báº£n (Ä‘Ã£ qua Táº§ng 1 masking)
            
        Returns:
            dict: {
                'tokens': list of tokens,
                'leet_count': sá»‘ kÃ½ tá»± leetspeak,
                'normalized_text': vÄƒn báº£n Ä‘Ã£ chuáº©n hÃ³a (debug)
            }
        """
        if not isinstance(text, str) or not text.strip():
            return {
                'tokens': [],
                'leet_count': 0,
                'normalized_text': text
            }
        
        # BÆ°á»›c 1: Unicode normalization
        normalized = normalize_unicode(text)
        
        # BÆ°á»›c 2: Leetspeak decode
        decoded, leet_count = self.leet_decoder.decode(normalized)
        
        # BÆ°á»›c 3: Word tokenization
        tokens = self.tokenizer.tokenize(decoded)
        
        return {
            'tokens': tokens,
            'leet_count': leet_count,
            'normalized_text': decoded
        }
```

```python
def test_layer2():
    processor = NormalizationTokenizer(tokenizer_library='underthesea')
    
    test_cases = [
        # Test Leetspeak
        (
            "Kh0ng pha! ban dang nhap",
            {'expected_leet': 2}  # 0â†’o, !â†’i
        ),
        (
            "D!eu k!en nh4n t!en",
            {'expected_leet': 4}  # 4 láº§n !â†’i, 4â†’a
        ),
        
        # Test Unicode normalization
        (
            "NgÃ¢n  hÃ ng   ACB",  # Multiple spaces
            {'expected_tokens_contain': ['ngÃ¢n', 'hÃ ng', 'ACB']}
        ),
        
        # Test Tokenization vá»›i tá»« ghÃ©p
        (
            "Tai khoan ngan hang",
            {'expected_tokens_contain': ['tÃ i_khoáº£n', 'ngÃ¢n_hÃ ng']}  # hoáº·c ['tai', 'khoan', 'ngan', 'hang'] tÃ¹y tokenizer
        ),
        
        # Test giá»¯ nguyÃªn entity tokens
        (
            "Dang nhap <URL> de nhan <MONEY>",
            {'expected_tokens_contain': ['<URL>', '<MONEY>']}
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 2: NORMALIZATION & TOKENIZATION")
    print("=" * 60)
    
    for i, (input_text, expected) in enumerate(test_cases):
        result = processor.process(input_text)
        
        print(f"\nTest {i+1}:")
        print(f"  Input:      {input_text}")
        print(f"  Tokens:     {result['tokens']}")
        print(f"  Leet count: {result['leet_count']}")
        
        if 'expected_leet' in expected:
            status = "âœ“" if result['leet_count'] == expected['expected_leet'] else "âœ—"
            print(f"  Leet check: {status}")

# test_layer2()
```

```shellscript
# CÃ i Ä‘áº·t thÆ° viá»‡n tÃ¡ch tá»« tiáº¿ng Viá»‡t
pip install underthesea

# Hoáº·c dÃ¹ng pyvi (nháº¹ hÆ¡n)
pip install pyvi
```

```plaintext
Input tokens:  ["vcb", "thong", "bao", "tai", "khoan", "otp", "123456", "<URL>"]
After filter:  ["thong", "bao", "tai", "khoan"]
Filtered out:  ["vcb" (brand), "otp" (jargon), "123456" (number), "<URL>" (entity)]
```

```python
class WhitelistFilter:
    """
    Táº§ng 3: Whitelist Filtering
    Lá»c bá» cÃ¡c token há»£p lá»‡ khÃ´ng cáº§n kiá»ƒm tra chÃ­nh táº£
    """
    
    def __init__(self, custom_whitelist_path=None):
        # Khá»Ÿi táº¡o cÃ¡c danh sÃ¡ch whitelist
        self.brand_list = self._build_brand_list()
        self.jargon_list = self._build_jargon_list()
        self.slang_abbr_list = self._build_slang_abbr_list()
        self.entity_tokens = self._build_entity_tokens()
        
        # Gá»™p táº¥t cáº£ thÃ nh má»™t set Ä‘á»ƒ tra cá»©u O(1)
        self.whitelist = set()
        self.whitelist.update(self.brand_list)
        self.whitelist.update(self.jargon_list)
        self.whitelist.update(self.slang_abbr_list)
        self.whitelist.update(self.entity_tokens)
        
        # Load custom whitelist náº¿u cÃ³
        if custom_whitelist_path:
            self._load_custom_whitelist(custom_whitelist_path)
        
        print(f"âœ“ Whitelist loaded: {len(self.whitelist)} items")
    
    def _build_brand_list(self):
        """Danh sÃ¡ch tÃªn thÆ°Æ¡ng hiá»‡u, ngÃ¢n hÃ ng, vÃ­ Ä‘iá»‡n tá»­, telco, ecommerce"""
        return {
            # === NGÃ‚N HÃ€NG ===
            'vcb', 'vietcombank', 'acb', 'bidv', 'vietinbank', 'viettinbank',
            'techcombank', 'tcb', 'mbbank', 'mb', 'tpbank', 'vpbank',
            'sacombank', 'scb', 'shb', 'hdbank', 'ocb', 'msb',
            'seabank', 'abbank', 'vib', 'eximbank', 'namabank',
            'agribank', 'lienvietpostbank', 'pvcombank', 'bacabank',
            'vietabank', 'ncb', 'pgbank', 'baovietbank', 'cbbank',
            'gpbank', 'kienlongbank', 'vietbank', 'wooribank',
            'uob', 'hsbc', 'citibank', 'standardchartered', 'shinhan',
            'digibank', 'smartbanking', 'ebanking', 'ibanking', 'mbanking',
            
            # === VÃ ÄIá»†N Tá»¬ / FINTECH ===
            'momo', 'zalopay', 'viettelpay', 'vnpay', 'moca', 'airpay',
            'shoppeepay', 'grabpay', 'payoo', 'vtcpay', 'appotapay',
            'fecredit', 'homecredit', 'tpfinance', 'fegreenfinance',
            'mirae', 'credilo', 'tima', 'cash24', 'doctordong', 'robocash',
            
            # === VIá»„N THÃ”NG ===
            'viettel', 'vinaphone', 'mobifone', 'vnmobile', 'gmobile',
            'vietnamobile', 'itel', 'reddi', 'wintel', 'indochina',
            
            # === THÆ¯Æ NG Máº I ÄIá»†N Tá»¬ ===
            'shopee', 'lazada', 'tiki', 'sendo', 'thegioididong', 'tgdd',
            'fptshop', 'cellphones', 'dienmayxanh', 'dmx', 'bachhoaxanh',
            'concung', 'guardian', 'watsons', 'lotte', 'aeon', 'bigc',
            'coopmart', 'vinmart', 'winmart', 'baemin', 'grabfood',
            'gojek', 'beamin', 'loship', 'ahamove', 'lalamove',
            'grab', 'be', 'gojek', 'xanh sm', 'mai linh', 'vinasun',
            
            # === Máº NG XÃƒ Há»˜I / APP ===
            'facebook', 'fb', 'zalo', 'telegram', 'viber', 'whatsapp',
            'tiktok', 'youtube', 'instagram', 'twitter', 'linkedin',
            'google', 'gmail', 'yahoo', 'outlook', 'icloud',
            
            # === THÆ¯Æ NG HIá»†U KHÃC ===
            'apple', 'iphone', 'samsung', 'oppo', 'xiaomi', 'vivo',
            'huawei', 'realme', 'nokia', 'sony', 'lg', 'asus',
            'vingroup', 'vinfast', 'vinhomes', 'vincom', 'vinmec',
            'sungroup', 'novaland', 'fpt', 'vnpt', 'evn',
        }
    
    def _build_jargon_list(self):
        """Thuáº­t ngá»¯ chuyÃªn ngÃ nh: viá»…n thÃ´ng, tÃ i chÃ­nh, marketing, IT"""
        return {
            # === VIá»„N THÃ”NG ===
            'sim', '4g', '5g', '3g', 'lte', 'data', 'wifi', 'internet',
            'roaming', 'sms', 'mms', 'ussd', 'gsm', 'cdma',
            'prepaid', 'postpaid', 'topup', 'náº¡p', 'goi', 'cuoc',
            
            # === TÃ€I CHÃNH ===
            'otp', 'pin', 'cvv', 'atm', 'pos', 'qr', 'nfc',
            'visa', 'mastercard', 'jcb', 'napas', 'swift', 'iban',
            'credit', 'debit', 'loan', 'mortgage', 'interest',
            'transfer', 'withdraw', 'deposit', 'balance',
            'transaction', 'payment', 'refund', 'cashback',
            
            # === MARKETING / KHUYáº¾N MÃƒI ===
            'voucher', 'coupon', 'deal', 'sale', 'flash', 'discount',
            'promo', 'promotion', 'km', 'khuyen', 'mai', 'freeship',
            'cod', 'ghn', 'ghtk', 'jt', 'ninja', 'best', 'viettel post',
            
            # === IT / Ká»¸ THUáº¬T ===
            'app', 'web', 'link', 'url', 'click', 'download', 'update',
            'login', 'logout', 'signup', 'register', 'verify', 'confirm',
            'password', 'username', 'email', 'account', 'profile',
            'online', 'offline', 'server', 'system', 'database',
            'id', 'ip', 'api', 'sdk', 'ios', 'android', 'windows',
            
            # === ÄÆ N Vá»Š / VIáº¾T Táº®T ===
            'vnd', 'vnÄ‘', 'usd', 'eur', 'btc', 'usdt', 'eth',
            'kg', 'km', 'cm', 'mm', 'm2', 'm3', 'gb', 'mb', 'kb',
            'tp', 'hcm', 'hn', 'dn', 'hp', 'ct', 'bd', 'bt',
            'bhxh', 'bhyt', 'bhtn', 'cmnd', 'cccd', 'gplx',
        }
    
    def _build_slang_abbr_list(self):
        """Viáº¿t táº¯t, teencode phá»• biáº¿n trong SMS tiáº¿ng Viá»‡t"""
        return {
            # === VIáº¾T Táº®T PHá»” BIáº¾N ===
            'lh', 'dt', 'sdt', 'Ä‘t', 'sÄ‘t',  # liÃªn há»‡, Ä‘iá»‡n thoáº¡i
            'tk', 'stk',  # tÃ i khoáº£n, sá»‘ tÃ i khoáº£n
            'qc', 'tb', 'tt',  # quáº£ng cÃ¡o, thÃ´ng bÃ¡o, thanh toÃ¡n
            'ck', 'hs', 'hd',  # chuyá»ƒn khoáº£n, há»“ sÆ¡, há»£p Ä‘á»“ng
            'kh', 'nv', 'ql',  # khÃ¡ch hÃ ng, nhÃ¢n viÃªn, quáº£n lÃ½
            'cty', 'cong ty', 'cskh',  # cÃ´ng ty, chÄƒm sÃ³c khÃ¡ch hÃ ng
            'pv', 'gd', 'kd',  # phá»¥c vá»¥, giao dá»‹ch, kinh doanh
            'nd', 'mgd', 'ref',  # ná»™i dung, mÃ£ giao dá»‹ch, reference
            
            # === TEENCODE / SLANG ===
            'ko', 'k', 'khong',  # khÃ´ng
            'dc', 'Ä‘c', 'duoc',  # Ä‘Æ°á»£c
            'r', 'roi', 'rui',  # rá»“i
            'nha', 'nhe', 'nhÃ©',  # nhÃ©
            'a', 'e', 'em', 'anh', 'chi', 'chá»‹',  # xÆ°ng hÃ´
            'mk', 'mik', 'minh',  # mÃ¬nh
            'bn', 'báº¡n',  # báº¡n
            'ns', 'noi', 'nÃ³i',  # nÃ³i
            'nt', 'nhan', 'nháº¯n',  # nháº¯n tin
            'ib', 'inbox',  # inbox
            'ok', 'okie', 'oki', 'okÃª',
            'tks', 'thanks', 'thank', 'cam', 'on', 'Æ¡n',
            'sr', 'sorry', 'xin', 'loi', 'lá»—i',
            'pls', 'please', 'vui', 'long', 'lÃ²ng',
            'btw', 'fyi', 'asap', 'aka',
            
            # === CÃCH VIáº¾T KHÃ”NG Dáº¤U PHá»” BIáº¾N ===
            'vui', 'long', 'vao', 'de', 'duoc', 'khong',
            'ngay', 'hom', 'nay', 'mai', 'qua',
            'tien', 'nhan', 'gui', 'chuyen',
        }
    
    def _build_entity_tokens(self):
        """CÃ¡c token entity tá»« Táº§ng 1"""
        return {
            '<url>', '<phone>', '<money>', '<code>', 
            '<datetime>', '<zalo>', '<email>',
            # ThÃªm cáº£ dáº¡ng viáº¿t hoa
            '<URL>', '<PHONE>', '<MONEY>', '<CODE>',
            '<DATETIME>', '<ZALO>', '<EMAIL>',
        }
    
    def _load_custom_whitelist(self, path):
        """Load whitelist tÃ¹y chá»‰nh tá»« file"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip().lower()
                    if word and not word.startswith('#'):
                        self.whitelist.add(word)
        except FileNotFoundError:
            print(f"Warning: Custom whitelist not found: {path}")
    
    def is_whitelisted(self, token):
        """
        Kiá»ƒm tra token cÃ³ trong whitelist khÃ´ng
        
        Args:
            token: Má»™t token Ä‘Æ¡n láº»
            
        Returns:
            bool: True náº¿u token trong whitelist
        """
        token_lower = token.lower().strip()
        
        # Check 1: Trong whitelist set
        if token_lower in self.whitelist:
            return True
        
        # Check 2: Token lÃ  sá»‘ thuáº§n tÃºy
        if token_lower.isdigit():
            return True
        
        # Check 3: Token lÃ  entity (báº¯t Ä‘áº§u vÃ  káº¿t thÃºc báº±ng <>)
        if token.startswith('<') and token.endswith('>'):
            return True
        
        # Check 4: Token quÃ¡ ngáº¯n (1 kÃ½ tá»±) - thÆ°á»ng lÃ  noise
        if len(token_lower) <= 1:
            return True
        
        # Check 5: Token chá»‰ chá»©a kÃ½ tá»± Ä‘áº·c biá»‡t
        if not any(c.isalnum() for c in token):
            return True
        
        return False
    
    def filter(self, tokens):
        """
        Lá»c danh sÃ¡ch tokens, tráº£ vá» tokens cáº§n kiá»ƒm tra chÃ­nh táº£
        
        Args:
            tokens: List cÃ¡c tokens tá»« Táº§ng 2
            
        Returns:
            dict: {
                'tokens_to_check': tokens cáº§n kiá»ƒm tra,
                'whitelisted_tokens': tokens Ä‘Ã£ bá»‹ lá»c,
                'whitelist_count': sá»‘ tokens bá»‹ lá»c
            }
        """
        tokens_to_check = []
        whitelisted_tokens = []
        
        for token in tokens:
            if self.is_whitelisted(token):
                whitelisted_tokens.append(token)
            else:
                tokens_to_check.append(token)
        
        return {
            'tokens_to_check': tokens_to_check,
            'whitelisted_tokens': whitelisted_tokens,
            'whitelist_count': len(whitelisted_tokens)
        }
```

```plaintext
# custom_whitelist.txt
# ThÃªm cÃ¡c tá»« Ä‘áº·c thÃ¹ cá»§a dá»± Ã¡n
# Má»—i tá»« má»™t dÃ²ng, comment báº¯t Ä‘áº§u báº±ng #

# TÃªn riÃªng
huit
ueh
uit
hcmus

# Thuáº­t ngá»¯ má»›i
blockchain
nft
defi
metaverse

# Brand má»›i
cake
timo
tnex
```

```python
def test_whitelist_filter():
    whitelist = WhitelistFilter()
    
    test_cases = [
        # Test brand filtering
        (
            ["vcb", "thong", "bao", "tai", "khoan"],
            {"should_filter": ["vcb"], "should_keep": ["thong", "bao", "tai", "khoan"]}
        ),
        
        # Test jargon filtering
        (
            ["nhap", "otp", "de", "xac", "nhan"],
            {"should_filter": ["otp"], "should_keep": ["nhap", "de", "xac", "nhan"]}
        ),
        
        # Test entity filtering
        (
            ["dang", "nhap", "<URL>", "de", "nhan", "<MONEY>"],
            {"should_filter": ["<URL>", "<MONEY>"], "should_keep": ["dang", "nhap", "de", "nhan"]}
        ),
        
        # Test number filtering
        (
            ["ma", "123456", "het", "han", "24", "gio"],
            {"should_filter": ["123456", "24"], "should_keep": ["ma", "het", "han", "gio"]}
        ),
        
        # Test slang filtering
        (
            ["lh", "ngay", "de", "dc", "ho", "tro"],
            {"should_filter": ["lh", "dc"], "should_keep": ["ngay", "de", "ho", "tro"]}
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 3: WHITELIST FILTERING")
    print("=" * 60)
    
    for i, (tokens, expected) in enumerate(test_cases):
        result = whitelist.filter(tokens)
        
        print(f"\nTest {i+1}:")
        print(f"  Input tokens:      {tokens}")
        print(f"  Tokens to check:   {result['tokens_to_check']}")
        print(f"  Whitelisted:       {result['whitelisted_tokens']}")

# test_whitelist_filter()
```

```plaintext
Input tokens: ["thong", "bao", "ta!", "khoan", "xyz123abc"]

Validation:
- "thong" â†’ Shadow Dict cÃ³ "thÃ´ng" â†’ VALID
- "bao"   â†’ Shadow Dict cÃ³ "bÃ¡o", "bao" â†’ VALID  
- "ta!"   â†’ KhÃ´ng cÃ³ trong cáº£ 2 â†’ MISSPELL
- "khoan" â†’ Shadow Dict cÃ³ "khoáº£n" â†’ VALID
- "xyz123abc" â†’ KhÃ´ng cÃ³ trong cáº£ 2 â†’ MISSPELL

Output: {
    'misspelled_tokens': ["ta!", "xyz123abc"],
    'misspelled_count': 2,
    'total_checked': 5,
    'misspelled_pct': 40.0
}
```

```python
def remove_vietnamese_accents(text):
    """
    Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i Latin gá»‘c
    
    "ngÃ¢n hÃ ng" â†’ "ngan hang"
    "Ä‘iá»u kiá»‡n" â†’ "dieu kien"
    "Ä‘Æ°á»£c"      â†’ "duoc"
    """
    if not isinstance(text, str):
        return text
    
    # Báº£ng chuyá»ƒn Ä‘á»•i Ä‘áº§y Ä‘á»§ cho tiáº¿ng Viá»‡t
    VIETNAMESE_ACCENT_MAP = {
        # a vá»›i dáº¥u
        'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
        'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
        'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
        'Ã€': 'A', 'Ã': 'A', 'áº¢': 'A', 'Ãƒ': 'A', 'áº ': 'A',
        'Ä‚': 'A', 'áº°': 'A', 'áº®': 'A', 'áº²': 'A', 'áº´': 'A', 'áº¶': 'A',
        'Ã‚': 'A', 'áº¦': 'A', 'áº¤': 'A', 'áº¨': 'A', 'áºª': 'A', 'áº¬': 'A',
        
        # Ä‘
        'Ä‘': 'd', 'Ä': 'D',
        
        # e vá»›i dáº¥u
        'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
        'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
        'Ãˆ': 'E', 'Ã‰': 'E', 'áºº': 'E', 'áº¼': 'E', 'áº¸': 'E',
        'ÃŠ': 'E', 'á»€': 'E', 'áº¾': 'E', 'á»‚': 'E', 'á»„': 'E', 'á»†': 'E',
        
        # i vá»›i dáº¥u
        'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
        'ÃŒ': 'I', 'Ã': 'I', 'á»ˆ': 'I', 'Ä¨': 'I', 'á»Š': 'I',
        
        # o vá»›i dáº¥u
        'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
        'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
        'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
        'Ã’': 'O', 'Ã“': 'O', 'á»': 'O', 'Ã•': 'O', 'á»Œ': 'O',
        'Ã”': 'O', 'á»’': 'O', 'á»': 'O', 'á»”': 'O', 'á»–': 'O', 'á»˜': 'O',
        'Æ ': 'O', 'á»œ': 'O', 'á»š': 'O', 'á»': 'O', 'á» ': 'O', 'á»¢': 'O',
        
        # u vá»›i dáº¥u
        'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
        'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
        'Ã™': 'U', 'Ãš': 'U', 'á»¦': 'U', 'Å¨': 'U', 'á»¤': 'U',
        'Æ¯': 'U', 'á»ª': 'U', 'á»¨': 'U', 'á»¬': 'U', 'á»®': 'U', 'á»°': 'U',
        
        # y vá»›i dáº¥u
        'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
        'á»²': 'Y', 'Ã': 'Y', 'á»¶': 'Y', 'á»¸': 'Y', 'á»´': 'Y',
    }
    
    result = []
    for char in text:
        result.append(VIETNAMESE_ACCENT_MAP.get(char, char))
    
    return ''.join(result)
```

```python
import json

class DualLookupValidator:
    """
    Táº§ng 4: Dual-Lookup Validation
    Kiá»ƒm tra tá»« Ä‘iá»ƒn CÃ³ dáº¥u â†’ KhÃ´ng dáº¥u
    """
    
    def __init__(self, words_file_path):
        """
        Args:
            words_file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file words.txt
                            Format: {"text": "tá»«", "source": [...]}
        """
        print("Loading Vietnamese dictionaries...")
        
        # BÆ°á»›c 1: Load vÃ  build Full Dict (cÃ³ dáº¥u)
        self.full_dict = self._build_full_dict(words_file_path)
        print(f"  âœ“ Full Dict: {len(self.full_dict):,} words")
        
        # BÆ°á»›c 2: Build Shadow Dict (khÃ´ng dáº¥u) tá»« Full Dict
        self.shadow_dict = self._build_shadow_dict()
        print(f"  âœ“ Shadow Dict: {len(self.shadow_dict):,} words")
        
        print("âœ“ Dictionaries loaded successfully!")
    
    def _build_full_dict(self, path):
        """
        XÃ¢y dá»±ng tá»« Ä‘iá»ƒn cÃ³ dáº¥u tá»« file words.txt
        """
        full_dict = set()
        
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                try:
                    # Parse JSON
                    entry = json.loads(line)
                    word = entry.get('text', '').strip()
                    
                    if word:
                        # ThÃªm dáº¡ng lowercase
                        full_dict.add(word.lower())
                        
                        # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ gáº¡ch ná»‘i
                        # "ngÃ¢n-hÃ ng" â†’ thÃªm cáº£ "ngÃ¢n hÃ ng" vÃ  "ngÃ¢n_hÃ ng"
                        if '-' in word:
                            full_dict.add(word.lower().replace('-', ' '))
                            full_dict.add(word.lower().replace('-', '_'))
                        
                        # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ khoáº£ng tráº¯ng
                        # "ngÃ¢n hÃ ng" â†’ thÃªm cáº£ "ngÃ¢n_hÃ ng"
                        if ' ' in word:
                            full_dict.add(word.lower().replace(' ', '_'))
                            
                except json.JSONDecodeError:
                    # Náº¿u khÃ´ng pháº£i JSON, thá»­ Ä‘á»c nhÆ° plain text
                    word = line.strip()
                    if word:
                        full_dict.add(word.lower())
        
        return full_dict
    
    def _build_shadow_dict(self):
        """
        XÃ¢y dá»±ng Shadow Dict (khÃ´ng dáº¥u) tá»« Full Dict
        """
        shadow_dict = set()
        
        for word in self.full_dict:
            # Chuyá»ƒn vá» dáº¡ng khÃ´ng dáº¥u
            no_accent = remove_vietnamese_accents(word.lower())
            shadow_dict.add(no_accent)
        
        return shadow_dict
    
    def validate_token(self, token):
        """
        Kiá»ƒm tra má»™t token
        
        Args:
            token: Má»™t token Ä‘Æ¡n láº»
            
        Returns:
            tuple: (is_valid, match_type)
                - is_valid: bool
                - match_type: 'full' | 'shadow' | 'misspell'
        """
        token_lower = token.lower().strip()
        
        # Bá» qua token rá»—ng
        if not token_lower:
            return (True, 'skip')
        
        # CHECK 1: Tá»« Ä‘iá»ƒn cÃ³ dáº¥u (Full Dict)
        if token_lower in self.full_dict:
            return (True, 'full')
        
        # Xá»­ lÃ½ tá»« ghÃ©p vá»›i underscore
        # "ngan_hang" â†’ check cáº£ "ngan hang"
        if '_' in token_lower:
            token_with_space = token_lower.replace('_', ' ')
            if token_with_space in self.full_dict:
                return (True, 'full')
        
        # CHECK 2: Tá»« Ä‘iá»ƒn khÃ´ng dáº¥u (Shadow Dict)
        token_no_accent = remove_vietnamese_accents(token_lower)
        
        if token_no_accent in self.shadow_dict:
            return (True, 'shadow')
        
        # Xá»­ lÃ½ tá»« ghÃ©p
        if '_' in token_no_accent:
            token_with_space = token_no_accent.replace('_', ' ')
            if token_with_space in self.shadow_dict:
                return (True, 'shadow')
        
        # KHÃ”NG match cáº£ 2 â†’ MISSPELL
        return (False, 'misspell')
    
    def validate_tokens(self, tokens):
        """
        Kiá»ƒm tra danh sÃ¡ch tokens
        
        Args:
            tokens: List cÃ¡c tokens cáº§n kiá»ƒm tra
            
        Returns:
            dict: {
                'misspelled_tokens': list cÃ¡c tá»« sai chÃ­nh táº£,
                'misspelled_count': sá»‘ tá»« sai,
                'valid_full_count': sá»‘ tá»« match Full Dict,
                'valid_shadow_count': sá»‘ tá»« match Shadow Dict,
                'total_checked': tá»•ng sá»‘ tá»« Ä‘Ã£ check,
                'misspelled_pct': % tá»« sai
            }
        """
        misspelled_tokens = []
        valid_full_count = 0
        valid_shadow_count = 0
        total_checked = 0
        
        for token in tokens:
            is_valid, match_type = self.validate_token(token)
            
            if match_type == 'skip':
                continue
            
            total_checked += 1
            
            if match_type == 'full':
                valid_full_count += 1
            elif match_type == 'shadow':
                valid_shadow_count += 1
            else:  # misspell
                misspelled_tokens.append(token)
        
        misspelled_count = len(misspelled_tokens)
        misspelled_pct = (misspelled_count / total_checked * 100) if total_checked > 0 else 0.0
        
        return {
            'misspelled_tokens': misspelled_tokens,
            'misspelled_count': misspelled_count,
            'valid_full_count': valid_full_count,
            'valid_shadow_count': valid_shadow_count,
            'total_checked': total_checked,
            'misspelled_pct': round(misspelled_pct, 2)
        }
```

```python
def test_dual_lookup():
    # Giáº£ sá»­ Ä‘Ã£ cÃ³ file words.txt
    validator = DualLookupValidator('data/dicts/words.txt')
    
    test_cases = [
        # Test tá»« cÃ³ dáº¥u â†’ match Full Dict
        (
            ["ngÃ¢n", "hÃ ng", "thÃ´ng", "bÃ¡o"],
            {'expected_full': 4, 'expected_misspell': 0}
        ),
        
        # Test tá»« khÃ´ng dáº¥u â†’ match Shadow Dict
        (
            ["ngan", "hang", "thong", "bao"],
            {'expected_shadow': 4, 'expected_misspell': 0}
        ),
        
        # Test tá»« sai chÃ­nh táº£
        (
            ["ngÃ¢n", "hÃ ng", "xyzabc", "abcdef"],
            {'expected_misspell': 2}
        ),
        
        # Test há»—n há»£p
        (
            ["tai", "khoan", "abc123xyz", "da", "bi", "khoa"],
            {'expected_misspell': 1}  # chá»‰ "abc123xyz" sai
        ),
        
        # Test tá»« ghÃ©p
        (
            ["ngÃ¢n_hÃ ng", "tai_khoan"],
            {'expected_valid': 2}  # cáº£ 2 Ä‘á»u há»£p lá»‡
        ),
    ]
    
    print("=" * 60)
    print("TESTING Táº¦NG 4: DUAL-LOOKUP VALIDATION")
    print("=" * 60)
    
    for i, (tokens, expected) in enumerate(test_cases):
        result = validator.validate_tokens(tokens)
        
        print(f"\nTest {i+1}:")
        print(f"  Input tokens:     {tokens}")
        print(f"  Misspelled:       {result['misspelled_tokens']}")
        print(f"  Full Dict match:  {result['valid_full_count']}")
        print(f"  Shadow Dict match:{result['valid_shadow_count']}")
        print(f"  Misspelled %:     {result['misspelled_pct']}%")

# test_dual_lookup()
```

```python
class MisspellingDetector:
    """
    Pipeline hoÃ n chá»‰nh phÃ¡t hiá»‡n lá»—i chÃ­nh táº£ trong SMS tiáº¿ng Viá»‡t
    Kiáº¿n trÃºc 4 táº§ng:
        Táº§ng 1: Aggressive Masking
        Táº§ng 2: Normalization & Tokenization
        Táº§ng 3: Whitelist Filtering
        Táº§ng 4: Dual-Lookup Validation
    """
    
    def __init__(self, words_file_path, custom_whitelist_path=None):
        """
        Khá»Ÿi táº¡o pipeline
        
        Args:
            words_file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file tá»« Ä‘iá»ƒn tiáº¿ng Viá»‡t
            custom_whitelist_path: (Optional) ÄÆ°á»ng dáº«n Ä‘áº¿n whitelist tÃ¹y chá»‰nh
        """
        print("=" * 60)
        print("INITIALIZING MISSPELLING DETECTOR")
        print("=" * 60)
        
        # Táº§ng 1
        print("\n[Layer 1] Loading Aggressive Masker...")
        self.masker = AggressiveMasker()
        
        # Táº§ng 2
        print("\n[Layer 2] Loading Normalizer & Tokenizer...")
        self.normalizer = NormalizationTokenizer(tokenizer_library='underthesea')
        
        # Táº§ng 3
        print("\n[Layer 3] Loading Whitelist Filter...")
        self.whitelist = WhitelistFilter(custom_whitelist_path)
        
        # Táº§ng 4
        print("\n[Layer 4] Loading Dual-Lookup Validator...")
        self.validator = DualLookupValidator(words_file_path)
        
        print("\n" + "=" * 60)
        print("âœ“ MISSPELLING DETECTOR READY!")
        print("=" * 60)
    
    def extract_features(self, text):
        """
        TrÃ­ch xuáº¥t features lá»—i chÃ­nh táº£ tá»« vÄƒn báº£n
        
        Args:
            text: Chuá»—i vÄƒn báº£n SMS
            
        Returns:
            dict: {
                # Tá»« Táº§ng 1
                'entity_counts': {...},
                
                # Tá»« Táº§ng 2
                'leet_count': int,
                'leet_pct': float,
                
                # Tá»« Táº§ng 4
                'misspelled_count': int,
                'misspelled_pct': float,
                'misspelled_tokens': [...],
                
                # Tá»•ng há»£p
                'total_tokens': int,
            }
        """
        if not isinstance(text, str) or not text.strip():
            return self._empty_result()
        
        # ===== Táº¦NG 1: AGGRESSIVE MASKING =====
        masked_text, entity_counts = self.masker.mask(text)
        
        # ===== Táº¦NG 2: NORMALIZATION & TOKENIZATION =====
        norm_result = self.normalizer.process(masked_text)
        tokens = norm_result['tokens']
        leet_count = norm_result['leet_count']
        
        # ===== Táº¦NG 3: WHITELIST FILTERING =====
        filter_result = self.whitelist.filter(tokens)
        tokens_to_check = filter_result['tokens_to_check']
        
        # ===== Táº¦NG 4: DUAL-LOOKUP VALIDATION =====
        validation_result = self.validator.validate_tokens(tokens_to_check)
        
        # ===== Tá»”NG Há»¢P Káº¾T QUáº¢ =====
        total_tokens = len(tokens)
        leet_pct = (leet_count / len(text) * 100) if len(text) > 0 else 0.0
        
        return {
            # Entity counts tá»« Táº§ng 1
            'entity_counts': entity_counts,
            'url_count': entity_counts.get('url', 0),
            'phone_count': entity_counts.get('phone', 0),
            'money_count': entity_counts.get('money', 0),
            'code_count': entity_counts.get('code', 0),
            
            # Leetspeak tá»« Táº§ng 2
            'leet_count': leet_count,
            'leet_pct': round(leet_pct, 2),
            
            # Whitelist tá»« Táº§ng 3
            'whitelist_count': filter_result['whitelist_count'],
            
            # Misspelling tá»« Táº§ng 4
            'misspelled_count': validation_result['misspelled_count'],
            'misspelled_pct': validation_result['misspelled_pct'],
            'misspelled_tokens': validation_result['misspelled_tokens'],
            
            # Tá»•ng há»£p
            'total_tokens': total_tokens,
            'tokens_checked': validation_result['total_checked'],
        }
    
    def _empty_result(self):
        """Tráº£ vá» káº¿t quáº£ rá»—ng cho vÄƒn báº£n khÃ´ng há»£p lá»‡"""
        return {
            'entity_counts': {},
            'url_count': 0,
            'phone_count': 0,
            'money_count': 0,
            'code_count': 0,
            'leet_count': 0,
            'leet_pct': 0.0,
            'whitelist_count': 0,
            'misspelled_count': 0,
            'misspelled_pct': 0.0,
            'misspelled_tokens': [],
            'total_tokens': 0,
            'tokens_checked': 0,
        }
    
    def process_dataframe(self, df, text_column='content'):
        """
        Xá»­ lÃ½ toÃ n bá»™ DataFrame
        
        Args:
            df: pandas DataFrame
            text_column: TÃªn cá»™t chá»©a vÄƒn báº£n
            
        Returns:
            DataFrame vá»›i cÃ¡c cá»™t features má»›i
        """
        import pandas as pd
        from tqdm import tqdm
        
        print(f"\nProcessing {len(df)} records...")
        
        # Ãp dá»¥ng cho tá»«ng dÃ²ng
        results = []
        for text in tqdm(df[text_column], desc="Extracting features"):
            results.append(self.extract_features(text))
        
        # Chuyá»ƒn thÃ nh DataFrame
        features_df = pd.DataFrame(results)
        
        # Gá»™p vá»›i DataFrame gá»‘c
        result_df = pd.concat([df.reset_index(drop=True), features_df], axis=1)
        
        print(f"âœ“ Processed {len(df)} records")
        
        return result_df
```

```python
# === KHá»I Táº O ===
detector = MisspellingDetector(
    words_file_path='data/dicts/words.txt',
    custom_whitelist_path='data/dicts/custom_whitelist.txt'  # optional
)

# === Xá»¬ LÃ 1 TIN NHáº®N ===
text = "Kh0ng pha! ban dang nhap https://fake-bank.com de nhan 500k. LH: 0901234567"
features = detector.extract_features(text)

print(f"Leet count: {features['leet_count']}")
print(f"Misspelled: {features['misspelled_tokens']}")
print(f"Misspelled %: {features['misspelled_pct']}%")

# === Xá»¬ LÃ DATAFRAME ===
import pandas as pd

df = pd.read_csv('data/dataset.csv')
df_with_features = detector.process_dataframe(df, text_column='content')
df_with_features.to_csv('data/dataset_with_spelling_features.csv', index=False)
```

```plaintext
misspelling_detection/
â”‚
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ layer1_masking.py           # Táº§ng 1: AggressiveMasker
â”‚
â”œâ”€â”€ layer2_normalization.py     # Táº§ng 2: NormalizationTokenizer
â”‚   â”œâ”€â”€ normalize_unicode()
â”‚   â”œâ”€â”€ LeetSpeakDecoder
â”‚   â””â”€â”€ VietnameseTokenizer
â”‚
â”œâ”€â”€ layer3_whitelist.py         # Táº§ng 3: WhitelistFilter
â”‚
â”œâ”€â”€ layer4_validation.py        # Táº§ng 4: DualLookupValidator
â”‚   â”œâ”€â”€ remove_vietnamese_accents()
â”‚   â””â”€â”€ DualLookupValidator
â”‚
â”œâ”€â”€ detector.py                 # Pipeline tÃ­ch há»£p: MisspellingDetector
â”‚
â”œâ”€â”€ utils.py                    # CÃ¡c hÃ m tiá»‡n Ã­ch chung
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_layer1.py
    â”œâ”€â”€ test_layer2.py
    â”œâ”€â”€ test_layer3.py
    â”œâ”€â”€ test_layer4.py
    â””â”€â”€ test_integration.py
```

