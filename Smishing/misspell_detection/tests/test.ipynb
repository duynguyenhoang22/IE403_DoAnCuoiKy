{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a3b163",
   "metadata": {},
   "source": [
    "## Test 1 - Masking Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfdfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b64077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 39 items\n",
      "\n",
      "test_layer1.py::TestURLMasking::test_standard_url_with_https \u001b[32mPASSED\u001b[0m\u001b[32m      [  2%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_standard_url_with_www \u001b[32mPASSED\u001b[0m\u001b[32m        [  5%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_url_shortener_bitly \u001b[32mPASSED\u001b[0m\u001b[32m          [  7%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_aggressive_url_with_spaces \u001b[32mPASSED\u001b[0m\u001b[32m   [ 10%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_spam_tld_icu \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 12%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_spam_tld_vip \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 15%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_multiple_urls \u001b[32mPASSED\u001b[0m\u001b[32m                [ 17%]\u001b[0m\n",
      "test_layer1.py::TestZaloTelegramMasking::test_zalo_link \u001b[32mPASSED\u001b[0m\u001b[32m           [ 20%]\u001b[0m\n",
      "test_layer1.py::TestZaloTelegramMasking::test_telegram_link \u001b[32mPASSED\u001b[0m\u001b[32m       [ 23%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_mobile_standard \u001b[32mPASSED\u001b[0m\u001b[32m            [ 25%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_mobile_with_country_code \u001b[32mPASSED\u001b[0m\u001b[32m   [ 28%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_mobile_with_dots \u001b[32mPASSED\u001b[0m\u001b[32m           [ 30%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_hotline_1900 \u001b[32mPASSED\u001b[0m\u001b[32m               [ 33%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_hotline_with_spaces \u001b[32mPASSED\u001b[0m\u001b[32m        [ 35%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_landline \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_shortcode \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 41%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_k \u001b[32mPASSED\u001b[0m\u001b[32m               [ 43%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_trieu \u001b[32mPASSED\u001b[0m\u001b[32m           [ 46%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_vnd \u001b[32mPASSED\u001b[0m\u001b[32m             [ 48%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_dong \u001b[32mPASSED\u001b[0m\u001b[32m            [ 51%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_multiple_money \u001b[32mPASSED\u001b[0m\u001b[32m             [ 53%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_otp_6_digits \u001b[32mPASSED\u001b[0m\u001b[32m             [ 56%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_otp_4_digits \u001b[32mPASSED\u001b[0m\u001b[32m             [ 58%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_service_code \u001b[32mPASSED\u001b[0m\u001b[32m             [ 61%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_exclude_year \u001b[32mPASSED\u001b[0m\u001b[32m             [ 64%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_exclude_year_2000s \u001b[32mPASSED\u001b[0m\u001b[32m       [ 66%]\u001b[0m\n",
      "test_layer1.py::TestDateTimeMasking::test_date_dd_mm \u001b[32mPASSED\u001b[0m\u001b[32m              [ 69%]\u001b[0m\n",
      "test_layer1.py::TestDateTimeMasking::test_time_hh_mm \u001b[32mPASSED\u001b[0m\u001b[32m              [ 71%]\u001b[0m\n",
      "test_layer1.py::TestDateTimeMasking::test_duration_minutes \u001b[32mPASSED\u001b[0m\u001b[32m        [ 74%]\u001b[0m\n",
      "test_layer1.py::TestEmailMasking::test_standard_email \u001b[32mPASSED\u001b[0m\u001b[32m             [ 76%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_load_dataset \u001b[32mPASSED\u001b[0m\u001b[32m     [ 79%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_mask_real_samples \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_mask_batch_performance \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_entity_counts \u001b[32mPASSED\u001b[0m\u001b[32m    [ 87%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_empty_string \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 89%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_none_handling \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 92%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_unicode_vietnamese \u001b[32mPASSED\u001b[0m\u001b[32m            [ 94%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_gibberish_text \u001b[32mPASSED\u001b[0m\u001b[32m                [ 97%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_combined_entities \u001b[32mPASSED\u001b[0m\u001b[32m             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m39 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 0\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer1.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer1.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae0a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "\n",
      "üîÑ Processing 2,603 rows...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer1_masking_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer1_masking_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize masker\n",
    "masker = AggressiveMasker()\n",
    "\n",
    "# Process all rows\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "    \n",
    "    try:\n",
    "        masked_text, metadata = masker.mask(content)\n",
    "        counts = masker.get_entity_counts(metadata)\n",
    "    except Exception as e:\n",
    "        masked_text = f\"ERROR: {e}\"\n",
    "        metadata = {}\n",
    "        counts = {}\n",
    "    \n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        \"masked_content\": masked_text,\n",
    "        \"url_count\": counts.get(\"url\", 0) + counts.get(\"zalo\", 0) + counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": counts.get(\"hotline\", 0) + counts.get(\"landline\", 0) + \n",
    "                      counts.get(\"mobile\", 0) + counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": counts.get(\"money\", 0),\n",
    "        \"code_count\": counts.get(\"code\", 0),\n",
    "        \"email_count\": counts.get(\"email\", 0),\n",
    "        \"datetime_count\": counts.get(\"datetime\", 0),\n",
    "        \"raw_metadata\": str(metadata),\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040f8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SUMMARY STATISTICS:\n",
      "--------------------------------------------------\n",
      "   URLs detected:      1,192\n",
      "   Phones detected:    2,157\n",
      "   Money detected:     2,748\n",
      "   Codes detected:     1,140\n",
      "   Emails detected:    7\n",
      "   DateTimes detected: 2,742\n",
      "\n",
      "   Rows with entities: 2,285 / 2,603 (87.8%)\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   URLs detected:      {result_df['url_count'].sum():,}\")\n",
    "print(f\"   Phones detected:    {result_df['phone_count'].sum():,}\")\n",
    "print(f\"   Money detected:     {result_df['money_count'].sum():,}\")\n",
    "print(f\"   Codes detected:     {result_df['code_count'].sum():,}\")\n",
    "print(f\"   Emails detected:    {result_df['email_count'].sum():,}\")\n",
    "print(f\"   DateTimes detected: {result_df['datetime_count'].sum():,}\")\n",
    "\n",
    "# Rows with at least one entity\n",
    "has_entity = result_df[['url_count', 'phone_count', 'money_count', 'code_count']].sum(axis=1) > 0\n",
    "print(f\"\\n   Rows with entities: {has_entity.sum():,} / {len(result_df):,} ({has_entity.sum()/len(result_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c945a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE RESULTS (first 10 rows with changes):\n",
      "================================================================================\n",
      "\n",
      "[0] Label: 1\n",
      "   Original: [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõc 17h ng√†y h√¥m nay kh√¥ng thanh...\n",
      "   Masked:   [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõc <TIME> ng√†y h√¥m nay kh√¥ng th...\n",
      "   Counts:   URL=0, Phone=0, Money=2, Code=0\n",
      "\n",
      "[1] Label: 1\n",
      "   Original: [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB 0848836182 vao luc 08:09 27/03...\n",
      "   Masked:   [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB <PHONE> vao luc <TIME> <TIME>....\n",
      "   Counts:   URL=0, Phone=2, Money=1, Code=0\n",
      "\n",
      "[2] Label: 1\n",
      "   Original: Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +19.56 USD. Nh·∫≠n 500.000 VND. Ngay 0...\n",
      "   Masked:   Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +<MONEY>. Nh·∫≠n <MONEY>. Ngay <TIME>....\n",
      "   Counts:   URL=1, Phone=0, Money=2, Code=0\n",
      "\n",
      "[3] Label: 1\n",
      "   Original: B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M·∫≠t kh·∫©u: yk6698 USDT S·ªë d∆∞: 1,...\n",
      "   Masked:   B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M·∫≠t kh·∫©u: yk6698 USDT S·ªë d∆∞: <M...\n",
      "   Counts:   URL=1, Phone=0, Money=1, Code=0\n",
      "\n",
      "[4] Label: 1\n",
      "   Original: [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung thai do cua O/b va nhung nguoi l...\n",
      "   Masked:   [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung thai do cua O/b va nhung nguoi l...\n",
      "   Counts:   URL=0, Phone=1, Money=0, Code=0\n",
      "\n",
      "[5] Label: 1\n",
      "   Original: Nhan thay co hanh vi LOI DUNG TIN NHIEM, CHIEM DOAT TAI SAN. yc tt gap truoc 13g 16/2/2025, neu van ...\n",
      "   Masked:   Nhan thay co hanh vi LOI DUNG TIN NHIEM, CHIEM DOAT TAI SAN. yc tt gap truoc 13g <TIME>, neu van bat...\n",
      "   Counts:   URL=0, Phone=1, Money=0, Code=0\n",
      "\n",
      "[6] Label: 0\n",
      "   Original: H∆∞∆°ng ƒëang l√†m g√¨ v·∫≠y? D·∫°o n√†y c√¥ng vi·ªác th·∫ø n√†o r·ªìi? ...\n",
      "   Masked:   H∆∞∆°ng ƒëang l√†m g√¨ v·∫≠y? D·∫°o n√†y c√¥ng vi·ªác th·∫ø n√†o r·ªìi?...\n",
      "   Counts:   URL=0, Phone=0, Money=0, Code=0\n",
      "\n",
      "[8] Label: 0\n",
      "   Original: Th·∫£o d·∫°o n√†y kho·∫ª kh√¥ng, c√¥ng vi·ªác ·ªïn ch·ª© ? ...\n",
      "   Masked:   Th·∫£o d·∫°o n√†y kho·∫ª kh√¥ng, c√¥ng vi·ªác ·ªïn ch·ª© ?...\n",
      "   Counts:   URL=0, Phone=0, Money=0, Code=0\n",
      "\n",
      "[11] Label: 1\n",
      "   Original: ACB: Tai khoan cua ban da mo dich vu tai chinh toan cau phi dich vu hang thang la 2.000.000VND se bi...\n",
      "   Masked:   ACB: Tai khoan cua ban da mo dich vu tai chinh toan cau phi dich vu hang thang la <MONEY> se bi tru ...\n",
      "   Counts:   URL=0, Phone=0, Money=1, Code=0\n",
      "\n",
      "[12] Label: 1\n",
      "   Original: Vietinbank tran trong thong bao tai khoan cua quy khach hien tai da bi khoa. Dang nhap qua http://ww...\n",
      "   Masked:   Vietinbank tran trong thong bao tai khoan cua quy khach hien tai da bi khoa. Dang nhap qua <URL> de ...\n",
      "   Counts:   URL=1, Phone=0, Money=0, Code=0\n"
     ]
    }
   ],
   "source": [
    "# Show sample results\n",
    "print(\"\\nüìã SAMPLE RESULTS (first 10 rows with changes):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter rows that have changes\n",
    "changed_rows = result_df[result_df['original_content'] != result_df['masked_content']]\n",
    "\n",
    "for _, row in changed_rows.head(10).iterrows():\n",
    "    print(f\"\\n[{row['index']}] Label: {row['label']}\")\n",
    "    print(f\"   Original: {row['original_content'][:100]}...\")\n",
    "    print(f\"   Masked:   {row['masked_content'][:100]}...\")\n",
    "    print(f\"   Counts:   URL={row['url_count']}, Phone={row['phone_count']}, Money={row['money_count']}, Code={row['code_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcc702",
   "metadata": {},
   "source": [
    "## Test Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e60aa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Both dicts loaded: 78,258 words (full), 65,863 words (shadow) from c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\dicts\\words.txt\n"
     ]
    }
   ],
   "source": [
    "import test_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a85d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 46 items\n",
      "\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_0_to_o \u001b[32mPASSED\u001b[0m\u001b[32m             [  2%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_1_to_i \u001b[32mPASSED\u001b[0m\u001b[32m             [  4%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_3_to_e \u001b[32mPASSED\u001b[0m\u001b[32m             [  6%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_4_to_a \u001b[32mPASSED\u001b[0m\u001b[32m             [  8%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_symbol_exclamation_to_i \u001b[32mPASSED\u001b[0m\u001b[32m  [ 10%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_symbol_at_to_a \u001b[32mPASSED\u001b[0m\u001b[32m           [ 13%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_symbol_dollar_to_s \u001b[32mPASSED\u001b[0m\u001b[32m       [ 15%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_char_j_to_i \u001b[32mPASSED\u001b[0m\u001b[32m              [ 17%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_char_f_to_ph \u001b[31mFAILED\u001b[0m\u001b[31m             [ 19%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_combined \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 21%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_heavy_spam \u001b[32mPASSED\u001b[0m\u001b[31m               [ 23%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_apostrophe \u001b[32mPASSED\u001b[0m\u001b[31m  [ 26%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_dash \u001b[32mPASSED\u001b[0m\u001b[31m        [ 28%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_tilde \u001b[32mPASSED\u001b[0m\u001b[31m       [ 30%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_colon \u001b[32mPASSED\u001b[0m\u001b[31m       [ 32%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_dot \u001b[32mPASSED\u001b[0m\u001b[31m         [ 34%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_multiple \u001b[32mPASSED\u001b[0m\u001b[31m    [ 36%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_complex_spam \u001b[32mPASSED\u001b[0m\u001b[31m [ 39%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_url_tag \u001b[32mPASSED\u001b[0m\u001b[31m           [ 41%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_phone_tag \u001b[32mPASSED\u001b[0m\u001b[31m         [ 43%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_money_tag \u001b[32mPASSED\u001b[0m\u001b[31m         [ 45%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_code_tag \u001b[32mPASSED\u001b[0m\u001b[31m          [ 47%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_app_link_tag \u001b[32mPASSED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_multiple_tags \u001b[32mPASSED\u001b[0m\u001b[31m     [ 52%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_tag_with_leet_around \u001b[32mPASSED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_simple \u001b[32mPASSED\u001b[0m\u001b[31m            [ 56%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_lowercase \u001b[32mPASSED\u001b[0m\u001b[31m         [ 58%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_mixed_case \u001b[32mPASSED\u001b[0m\u001b[31m        [ 60%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_with_vietnamese_chars \u001b[32mPASSED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_preserves_tag_case \u001b[32mPASSED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_filters_numbers \u001b[32mPASSED\u001b[0m\u001b[31m   [ 67%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_normalized_text_reconstruction \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_empty_string \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 71%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_none_handling \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 73%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_only_separators \u001b[32mPASSED\u001b[0m\u001b[31m               [ 76%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_unicode_normalization \u001b[32mPASSED\u001b[0m\u001b[31m         [ 78%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_very_long_text \u001b[32mPASSED\u001b[0m\u001b[31m                [ 80%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_gibberish \u001b[32mPASSED\u001b[0m\u001b[31m                     [ 82%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_1 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 84%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_2 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 86%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_3 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 89%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_4 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 91%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_ham_sample \u001b[32mPASSED\u001b[0m\u001b[31m     [ 93%]\u001b[0m\n",
      "test_layer2.py::TestNormalizationResult::test_result_has_all_fields \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "test_layer2.py::TestNormalizationResult::test_result_original_preserved \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "test_layer2.py::TestNormalizationResult::test_result_types \u001b[32mPASSED\u001b[0m\u001b[31m        [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____________________ TestLeetspeak.test_leet_char_f_to_ph _____________________\u001b[0m\n",
      "\n",
      "self = <test_layer2.TestLeetspeak object at 0x00000181CCDD0E50>\n",
      "normalizer = <Smishing.misspell_detection.layer2_normalization.TextNormalizer object at 0x00000181CCDD1450>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_leet_char_f_to_ph\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, normalizer):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"f ‚Üí ph (Vietnamese-specific, 1‚Üí2 char)\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        text = \u001b[33m\"\u001b[39;49;00m\u001b[33mfap ly\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result = normalizer.normalize(text)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mphap\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_layer2.py\u001b[0m:97: AssertionError\n",
      "---------------------------- Captured stdout setup ----------------------------\n",
      "‚úì Both dicts loaded: 78,258 words (full), 65,863 words (shadow) from c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\dicts\\words.txt\n",
      "‚úì Dictionary loaded: 78,258 words (full), 65,863 words (shadow)\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_layer2.py::\u001b[1mTestLeetspeak::test_leet_char_f_to_ph\u001b[0m - AssertionError\n",
      "\u001b[31m======================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m45 passed\u001b[0m\u001b[31m in 22.03s\u001b[0m\u001b[31m ========================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 1\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer1.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer2.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b9fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "‚úì Both dicts loaded: 78,258 words (full), 65,863 words (shadow) from c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\dicts\\words.txt\n",
      "‚úì Dictionary loaded: 78,258 words (full), 65,863 words (shadow)\n",
      "\n",
      "üîÑ Processing 2,603 rows through Layer 1 + Layer 2...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer2_normalization_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 2: APPLY NORMALIZATION TO DATASET\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.misspell_detection.layer2_normalization import TextNormalizer\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer2_normalization_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize processors\n",
    "masker = AggressiveMasker()\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "# Process all rows with FULL PIPELINE: Layer 1 ‚Üí Layer 2\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows through Layer 1 + Layer 2...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "    \n",
    "    try:\n",
    "        # Layer 1: Masking\n",
    "        masked_text, mask_metadata = masker.mask(content)\n",
    "        mask_counts = masker.get_entity_counts(mask_metadata)\n",
    "        \n",
    "        # Layer 2: Normalization (on masked text)\n",
    "        norm_result = normalizer.normalize(masked_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        masked_text = f\"ERROR: {e}\"\n",
    "        mask_counts = {}\n",
    "        norm_result = None\n",
    "    \n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        \"layer1_masked\": masked_text,\n",
    "        \"layer2_normalized\": norm_result.normalized_text if norm_result else \"\",\n",
    "        \"layer2_tokens\": str(norm_result.tokens) if norm_result else \"[]\",\n",
    "        \"token_count\": len(norm_result.tokens) if norm_result else 0,\n",
    "        \"leet_count\": norm_result.leet_count if norm_result else 0,\n",
    "        \"teencode_count\": norm_result.teencode_count if norm_result else 0,\n",
    "        \"visual_leet_count\": norm_result.visual_leet_count if norm_result else 0,\n",
    "        \"symbol_leet_count\": norm_result.symbol_leet_count if norm_result else 0,\n",
    "        \"validated_leet_count\": norm_result.validated_leet_count if norm_result else 0,\n",
    "        \"weighted_leet_score\": norm_result.weighted_leet_score if norm_result else 0.0,\n",
    "        \"separator_count\": norm_result.separator_count if norm_result else 0,\n",
    "        # Layer 1 counts\n",
    "        \"url_count\": mask_counts.get(\"url\", 0) + mask_counts.get(\"zalo\", 0) + mask_counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": mask_counts.get(\"hotline\", 0) + mask_counts.get(\"landline\", 0) + \n",
    "                      mask_counts.get(\"mobile\", 0) + mask_counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": mask_counts.get(\"money\", 0),\n",
    "        \"code_count\": mask_counts.get(\"code\", 0),\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd7cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LAYER 2 SUMMARY STATISTICS:\n",
      "============================================================\n",
      "\n",
      "üìù TOKEN STATISTICS:\n",
      "   Total tokens:      115,391\n",
      "   Avg tokens/row:    44.3\n",
      "   Max tokens/row:    197\n",
      "\n",
      "üî§ LEET DETECTION:\n",
      "   Total leet chars:  13,418\n",
      "   Rows with leet:    2,025 (77.8%)\n",
      "   Avg leet/row:      5.15\n",
      "   Teencode count:    8\n",
      "   Visual leet:      172\n",
      "   Symbol leet:      32\n",
      "   Validated leet:   1,507\n",
      "   Weighted score:   215.80\n",
      "   Avg weighted score/row: 0.0829\n",
      "\n",
      "üìå SEPARATOR DETECTION:\n",
      "   Total separators:  25,241\n",
      "   Rows with sep:     2,542 (97.7%)\n",
      "\n",
      "üìà COMPARISON BY LABEL:\n",
      "------------------------------------------------------------\n",
      "\n",
      "   SPAM (label=1): 278 rows\n",
      "      Avg tokens:    40.1\n",
      "      Avg leet:      3.52\n",
      "      Avg teencode:  0.02\n",
      "      Avg visual:    0.33\n",
      "      Avg symbol:    0.10\n",
      "      Avg validated: 0.92\n",
      "      Avg weighted:  0.4673\n",
      "      Avg separator: 7.45\n",
      "\n",
      "   HAM (label=0): 2,325 rows\n",
      "      Avg tokens:    44.8\n",
      "      Avg leet:      5.35\n",
      "      Avg teencode:  0.00\n",
      "      Avg visual:    0.03\n",
      "      Avg symbol:    0.00\n",
      "      Avg validated: 0.54\n",
      "      Avg weighted:  0.0369\n",
      "      Avg separator: 9.97\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 2: SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä LAYER 2 SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic counts\n",
    "print(f\"\\nüìù TOKEN STATISTICS:\")\n",
    "print(f\"   Total tokens:      {result_df['token_count'].sum():,}\")\n",
    "print(f\"   Avg tokens/row:    {result_df['token_count'].mean():.1f}\")\n",
    "print(f\"   Max tokens/row:    {result_df['token_count'].max()}\")\n",
    "\n",
    "print(f\"\\nüî§ LEET DETECTION:\")\n",
    "print(f\"   Total leet chars:  {result_df['leet_count'].sum():,}\")\n",
    "print(f\"   Rows with leet:    {(result_df['leet_count'] > 0).sum():,} ({(result_df['leet_count'] > 0).sum()/len(result_df)*100:.1f}%)\")\n",
    "print(f\"   Avg leet/row:      {result_df['leet_count'].mean():.2f}\")\n",
    "print(f\"   Teencode count:    {result_df['teencode_count'].sum():,}\")\n",
    "print(f\"   Visual leet:      {result_df['visual_leet_count'].sum():,}\")\n",
    "print(f\"   Symbol leet:      {result_df['symbol_leet_count'].sum():,}\")\n",
    "print(f\"   Validated leet:   {result_df['validated_leet_count'].sum():,}\")\n",
    "print(f\"   Weighted score:   {result_df['weighted_leet_score'].sum():.2f}\")\n",
    "print(f\"   Avg weighted score/row: {result_df['weighted_leet_score'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nüìå SEPARATOR DETECTION:\")\n",
    "print(f\"   Total separators:  {result_df['separator_count'].sum():,}\")\n",
    "print(f\"   Rows with sep:     {(result_df['separator_count'] > 0).sum():,} ({(result_df['separator_count'] > 0).sum()/len(result_df)*100:.1f}%)\")\n",
    "\n",
    "# By label comparison\n",
    "print(f\"\\nüìà COMPARISON BY LABEL:\")\n",
    "print(\"-\" * 60)\n",
    "for label in result_df['label'].unique():\n",
    "    subset = result_df[result_df['label'] == label]\n",
    "    label_name = \"SPAM\" if label == 1 else \"HAM\"\n",
    "    print(f\"\\n   {label_name} (label={label}): {len(subset):,} rows\")\n",
    "    print(f\"      Avg tokens:    {subset['token_count'].mean():.1f}\")\n",
    "    print(f\"      Avg leet:      {subset['leet_count'].mean():.2f}\")\n",
    "    print(f\"      Avg teencode:  {subset['teencode_count'].mean():.2f}\")\n",
    "    print(f\"      Avg visual:    {subset['visual_leet_count'].mean():.2f}\")\n",
    "    print(f\"      Avg symbol:    {subset['symbol_leet_count'].mean():.2f}\")\n",
    "    print(f\"      Avg validated: {subset['validated_leet_count'].mean():.2f}\")\n",
    "    print(f\"      Avg weighted:  {subset['weighted_leet_score'].mean():.4f}\")\n",
    "    print(f\"      Avg separator: {subset['separator_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08e2ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2):\n",
      "================================================================================\n",
      "\n",
      "[2] Label: SPAM\n",
      "   Original:   Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +19.56 USD. Nh·∫≠n...\n",
      "   L1 Masked:  Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +<MONEY>. Nh·∫≠n <...\n",
      "   L2 Normalized: western union tb vietcombank 0071000986547 tr·∫ßn th·ªã lan ref t <MONEY> nh·∫≠n <MONE...\n",
      "   Leet Stats: Total=21, Teencode=0, Visual=0, Symbol=0, Validated=2, Weighted=0.00\n",
      "   Sep: 12, Tokens: 29\n",
      "   Validated leet: 2\n",
      "----------------------------------------\n",
      "\n",
      "[3] Label: SPAM\n",
      "   Original:   B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M·∫≠t kh·∫©u: y...\n",
      "   L1 Masked:  B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M·∫≠t kh·∫©u: y...\n",
      "   L2 Normalized: b·∫Øc t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o t√†i kho·∫£n nay128 m·∫≠t kh·∫©u yk669...\n",
      "   Leet Stats: Total=8, Teencode=0, Visual=0, Symbol=0, Validated=1, Weighted=0.00\n",
      "   Sep: 9, Tokens: 31\n",
      "   Validated leet: 1\n",
      "----------------------------------------\n",
      "\n",
      "[4] Label: SPAM\n",
      "   Original:   [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung thai do cua ...\n",
      "   L1 Masked:  [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung thai do cua ...\n",
      "   L2 Normalized: canh cao lan cuoi chung toi da nhac nho canh cao nhieu lan nhung thai do cua o b...\n",
      "   Leet Stats: Total=2, Teencode=0, Visual=0, Symbol=0, Validated=0, Weighted=0.00\n",
      "   Sep: 11, Tokens: 68\n",
      "   Validated leet: 0\n",
      "----------------------------------------\n",
      "\n",
      "[5] Label: SPAM\n",
      "   Original:   Nhan thay co hanh vi LOI DUNG TIN NHIEM, CHIEM DOAT TAI SAN. yc tt gap truoc 13g...\n",
      "   L1 Masked:  Nhan thay co hanh vi LOI DUNG TIN NHIEM, CHIEM DOAT TAI SAN. yc tt gap truoc 13g...\n",
      "   L2 Normalized: nhan thay co hanh vi loi dung tin nhiem chiem doat tai san yc tt gap truoc 13g <...\n",
      "   Leet Stats: Total=2, Teencode=0, Visual=0, Symbol=0, Validated=0, Weighted=0.00\n",
      "   Sep: 11, Tokens: 69\n",
      "   Validated leet: 0\n",
      "----------------------------------------\n",
      "\n",
      "[9] Label: HAM\n",
      "   Original:   Ch√†o Ph∆∞∆°ng! B·∫°n v√† gia ƒë√¨nh d·∫°o n√†y kho·∫ª ch·ª©?...\n",
      "   L1 Masked:  Ch√†o Ph∆∞∆°ng! B·∫°n v√† gia ƒë√¨nh d·∫°o n√†y kho·∫ª ch·ª©?...\n",
      "   L2 Normalized: ch√†o ph∆∞∆°ng b·∫°n v√† gia ƒë√¨nh d·∫°o n√†y kho·∫ª ch·ª©...\n",
      "   Leet Stats: Total=1, Teencode=0, Visual=0, Symbol=0, Validated=0, Weighted=0.00\n",
      "   Sep: 1, Tokens: 10\n",
      "   Validated leet: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAMPLE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show samples with leet detected\n",
    "leet_samples = result_df[result_df['leet_count'] > 0].head(5)\n",
    "\n",
    "for _, row in leet_samples.iterrows():\n",
    "    print(f\"\\n[{row['index']}] Label: {'SPAM' if row['label']==1 else 'HAM'}\")\n",
    "    print(f\"   Original:   {row['original_content'][:80]}...\")\n",
    "    print(f\"   L1 Masked:  {row['layer1_masked'][:80]}...\")\n",
    "    print(f\"   L2 Normalized: {row['layer2_normalized'][:80]}...\")\n",
    "    print(f\"   Leet Stats: Total={row['leet_count']}, Teencode={row['teencode_count']}, Visual={row['visual_leet_count']}, Symbol={row['symbol_leet_count']}, Validated={row['validated_leet_count']}, Weighted={row['weighted_leet_score']:.2f}\")\n",
    "    print(f\"   Sep: {row['separator_count']}, Tokens: {row['token_count']}\")\n",
    "    print(f\"   Validated leet: {row['validated_leet_count']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45e6e8",
   "metadata": {},
   "source": [
    "## Test Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db32c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 44 items\n",
      "\n",
      "test_layer3.py::TestBrandFiltering::test_bank_brand_vcb \u001b[32mPASSED\u001b[0m\u001b[32m           [  2%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_bank_brand_bidv \u001b[32mPASSED\u001b[0m\u001b[32m          [  4%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_bank_brand_vietinbank \u001b[32mPASSED\u001b[0m\u001b[32m    [  6%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_ewallet_brand_momo \u001b[32mPASSED\u001b[0m\u001b[32m       [  9%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_telco_brand_viettel \u001b[32mPASSED\u001b[0m\u001b[32m      [ 11%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_app_brand_tiktok \u001b[32mPASSED\u001b[0m\u001b[32m         [ 13%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_otp \u001b[32mPASSED\u001b[0m\u001b[32m              [ 15%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_sim \u001b[32mPASSED\u001b[0m\u001b[32m              [ 18%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_4g \u001b[32mPASSED\u001b[0m\u001b[32m               [ 20%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_digibank \u001b[32mPASSED\u001b[0m\u001b[32m         [ 22%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_usdt \u001b[32mPASSED\u001b[0m\u001b[32m             [ 25%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_lh \u001b[32mPASSED\u001b[0m\u001b[32m      [ 27%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_tk \u001b[32mPASSED\u001b[0m\u001b[32m      [ 29%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_cskh \u001b[32mPASSED\u001b[0m\u001b[32m    [ 31%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_bhtn \u001b[32mPASSED\u001b[0m\u001b[32m    [ 34%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_teencode_ko \u001b[31mFAILED\u001b[0m\u001b[31m  [ 36%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_url_uppercase \u001b[32mPASSED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_phone_uppercase \u001b[32mPASSED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_money_lowercase \u001b[32mPASSED\u001b[0m\u001b[31m [ 43%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_app_link \u001b[32mPASSED\u001b[0m\u001b[31m    [ 45%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check1_in_whitelist_set \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check2_pure_digit \u001b[32mPASSED\u001b[0m\u001b[31m   [ 50%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check3_entity_tag \u001b[32mPASSED\u001b[0m\u001b[31m   [ 52%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check4_short_token \u001b[32mPASSED\u001b[0m\u001b[31m  [ 54%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check5_special_chars_only \u001b[32mPASSED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_vietnamese_word_not_whitelisted \u001b[32mPASSED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_mixed_tokens \u001b[32mPASSED\u001b[0m\u001b[31m        [ 61%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_all_whitelisted \u001b[32mPASSED\u001b[0m\u001b[31m     [ 63%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_none_whitelisted \u001b[32mPASSED\u001b[0m\u001b[31m    [ 65%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_whitelist_count \u001b[32mPASSED\u001b[0m\u001b[31m     [ 68%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_original_tokens_preserved \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_has_all_fields \u001b[32mPASSED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_types \u001b[32mPASSED\u001b[0m\u001b[31m            [ 75%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_empty_input \u001b[32mPASSED\u001b[0m\u001b[31m      [ 77%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_is_dataclass \u001b[32mPASSED\u001b[0m\u001b[31m     [ 79%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_empty_list \u001b[32mPASSED\u001b[0m\u001b[31m                    [ 81%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_whitespace_token \u001b[32mPASSED\u001b[0m\u001b[31m              [ 84%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_case_insensitive \u001b[32mPASSED\u001b[0m\u001b[31m              [ 86%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_mixed_alphanumeric \u001b[32mPASSED\u001b[0m\u001b[31m            [ 88%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_unicode_vietnamese \u001b[32mPASSED\u001b[0m\u001b[31m            [ 90%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_smishing_tokens_1 \u001b[32mPASSED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_smishing_tokens_2 \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_smishing_tokens_3 \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_ham_tokens \u001b[32mPASSED\u001b[0m\u001b[31m   [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________ TestSlangAbbreviationFiltering.test_teencode_ko _______________\u001b[0m\n",
      "\n",
      "self = <test_layer3.TestSlangAbbreviationFiltering object at 0x00000181CE69BBF0>\n",
      "whitelist_filter = <Smishing.misspell_detection.layer3_whitelist.WhitelistFilter object at 0x00000181CE5BA2D0>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_teencode_ko\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, whitelist_filter):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"KO (Kh√¥ng) ƒë∆∞·ª£c whitelist\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m whitelist_filter.is_whitelisted(\u001b[33m\"\u001b[39;49;00m\u001b[33mko\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_layer3.py\u001b[0m:124: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_layer3.py::\u001b[1mTestSlangAbbreviationFiltering::test_teencode_ko\u001b[0m - AssertionError\n",
      "\u001b[31m======================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m43 passed\u001b[0m\u001b[31m in 0.21s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 1\n"
     ]
    }
   ],
   "source": [
    "import test_layer3\n",
    "\n",
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer3.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer3.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523002d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "‚úì Both dicts loaded: 78,258 words (full), 65,863 words (shadow) from c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\dicts\\words.txt\n",
      "‚úì Dictionary loaded: 78,258 words (full), 65,863 words (shadow)\n",
      "\n",
      "‚úì Layer 1: AggressiveMasker initialized\n",
      "‚úì Layer 2: TextNormalizer initialized\n",
      "‚úì Layer 3: WhitelistFilter initialized (173 whitelist items)\n",
      "\n",
      "üîÑ Processing 2,603 rows through Layer 1 ‚Üí Layer 2 ‚Üí Layer 3...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer3_whitelist_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL PIPELINE: LAYER 1 ‚Üí LAYER 2 ‚Üí LAYER 3\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.misspell_detection.layer2_normalization import TextNormalizer\n",
    "from Smishing.misspell_detection.layer3_whitelist import WhitelistFilter\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer3_whitelist_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize all processors\n",
    "masker = AggressiveMasker()\n",
    "normalizer = TextNormalizer()\n",
    "whitelist_filter = WhitelistFilter()\n",
    "\n",
    "print(f\"\\n‚úì Layer 1: AggressiveMasker initialized\")\n",
    "print(f\"‚úì Layer 2: TextNormalizer initialized\")\n",
    "print(f\"‚úì Layer 3: WhitelistFilter initialized ({len(whitelist_filter.whitelist)} whitelist items)\")\n",
    "\n",
    "# Process all rows with FULL PIPELINE\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows through Layer 1 ‚Üí Layer 2 ‚Üí Layer 3...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "    \n",
    "    try:\n",
    "        # ===== LAYER 1: MASKING =====\n",
    "        masked_text, mask_metadata = masker.mask(content)\n",
    "        mask_counts = masker.get_entity_counts(mask_metadata)\n",
    "        \n",
    "        # ===== LAYER 2: NORMALIZATION =====\n",
    "        norm_result = normalizer.normalize(masked_text)\n",
    "        tokens = norm_result.tokens\n",
    "        \n",
    "        # ===== LAYER 3: WHITELIST FILTERING =====\n",
    "        whitelist_result = whitelist_filter.filter(tokens)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {idx}: {e}\")\n",
    "        masked_text = content\n",
    "        norm_result = None\n",
    "        whitelist_result = None\n",
    "        tokens = []\n",
    "        mask_counts = {}\n",
    "    \n",
    "    # Build result row\n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        # Layer 1\n",
    "        \"layer1_masked\": masked_text,\n",
    "        \"url_count\": mask_counts.get(\"url\", 0) + mask_counts.get(\"zalo\", 0) + mask_counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": mask_counts.get(\"hotline\", 0) + mask_counts.get(\"landline\", 0) + \n",
    "                      mask_counts.get(\"mobile\", 0) + mask_counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": mask_counts.get(\"money\", 0),\n",
    "        \"code_count\": mask_counts.get(\"code\", 0),\n",
    "        \"bank_acc_count\": mask_counts.get(\"bank_acc\", 0),\n",
    "        # Layer 2\n",
    "        \"layer2_normalized\": norm_result.normalized_text if norm_result else \"\",\n",
    "        \"layer2_tokens\": str(tokens),\n",
    "        \"token_count\": len(tokens),\n",
    "        \"leet_count\": norm_result.leet_count if norm_result else 0,\n",
    "        \"teencode_count\": norm_result.teencode_count if norm_result else 0,\n",
    "        \"visual_leet_count\": norm_result.visual_leet_count if norm_result else 0,\n",
    "        \"symbol_leet_count\": norm_result.symbol_leet_count if norm_result else 0,\n",
    "        \"validated_leet_count\": norm_result.validated_leet_count if norm_result else 0,\n",
    "        \"weighted_leet_score\": norm_result.weighted_leet_score if norm_result else 0.0,\n",
    "        \"separator_count\": norm_result.separator_count if norm_result else 0,\n",
    "        # Layer 3\n",
    "        \"tokens_to_check\": str(whitelist_result.tokens_to_check) if whitelist_result else \"[]\",\n",
    "        \"whitelisted_tokens\": str(whitelist_result.whitelisted_tokens) if whitelist_result else \"[]\",\n",
    "        \"whitelist_count\": whitelist_result.whitelist_count if whitelist_result else 0,\n",
    "        \"tokens_to_check_count\": len(whitelist_result.tokens_to_check) if whitelist_result else 0,\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e1a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FULL PIPELINE SUMMARY STATISTICS:\n",
      "============================================================\n",
      "\n",
      "üîí LAYER 1 - ENTITY MASKING:\n",
      "   URLs detected:      1,192\n",
      "   Phones detected:    2,157\n",
      "   Money detected:     2,748\n",
      "   Codes detected:     1,140\n",
      "   Bank accounts detected: 0\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Total tokens:       115,391\n",
      "   Avg tokens/msg:     44.3\n",
      "   Total leet chars:   13,418\n",
      "   Teencode count:     8\n",
      "   Visual leet:       172\n",
      "   Symbol leet:       32\n",
      "   Validated leet:    1,507\n",
      "   Weighted score:    215.80\n",
      "   Total separators:   25,241\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   Total whitelist:    23,431\n",
      "   Total to check:     91,960\n",
      "   Avg whitelist/msg:  9.00\n",
      "   Avg to check/msg:   35.33\n",
      "\n",
      "üìà FILTERING EFFICIENCY:\n",
      "   Total tokens input:       115,391\n",
      "   Tokens filtered out:      23,431 (20.3%)\n",
      "   Tokens for spell check:   91,960 (79.7%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 3 SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä FULL PIPELINE SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Layer 1 Stats\n",
    "print(\"\\nüîí LAYER 1 - ENTITY MASKING:\")\n",
    "print(f\"   URLs detected:      {result_df['url_count'].sum():,}\")\n",
    "print(f\"   Phones detected:    {result_df['phone_count'].sum():,}\")\n",
    "print(f\"   Money detected:     {result_df['money_count'].sum():,}\")\n",
    "print(f\"   Codes detected:     {result_df['code_count'].sum():,}\")\n",
    "print(f\"   Bank accounts detected: {result_df['bank_acc_count'].sum():,}\")\n",
    "\n",
    "# Layer 2 Stats\n",
    "print(f\"\\nüî§ LAYER 2 - NORMALIZATION:\")\n",
    "print(f\"   Total tokens:       {result_df['token_count'].sum():,}\")\n",
    "print(f\"   Avg tokens/msg:     {result_df['token_count'].mean():.1f}\")\n",
    "print(f\"   Total leet chars:   {result_df['leet_count'].sum():,}\")\n",
    "print(f\"   Teencode count:     {result_df['teencode_count'].sum():,}\")\n",
    "print(f\"   Visual leet:       {result_df['visual_leet_count'].sum():,}\")\n",
    "print(f\"   Symbol leet:       {result_df['symbol_leet_count'].sum():,}\")\n",
    "print(f\"   Validated leet:    {result_df['validated_leet_count'].sum():,}\")\n",
    "print(f\"   Weighted score:    {result_df['weighted_leet_score'].sum():.2f}\")\n",
    "print(f\"   Total separators:   {result_df['separator_count'].sum():,}\")\n",
    "\n",
    "# Layer 3 Stats\n",
    "print(f\"\\nüìã LAYER 3 - WHITELIST FILTERING:\")\n",
    "print(f\"   Total whitelist:    {result_df['whitelist_count'].sum():,}\")\n",
    "print(f\"   Total to check:     {result_df['tokens_to_check_count'].sum():,}\")\n",
    "print(f\"   Avg whitelist/msg:  {result_df['whitelist_count'].mean():.2f}\")\n",
    "print(f\"   Avg to check/msg:   {result_df['tokens_to_check_count'].mean():.2f}\")\n",
    "\n",
    "# Filtering ratio\n",
    "total_tokens = result_df['token_count'].sum()\n",
    "tokens_filtered = result_df['whitelist_count'].sum()\n",
    "tokens_remaining = result_df['tokens_to_check_count'].sum()\n",
    "\n",
    "print(f\"\\nüìà FILTERING EFFICIENCY:\")\n",
    "print(f\"   Total tokens input:       {total_tokens:,}\")\n",
    "print(f\"   Tokens filtered out:      {tokens_filtered:,} ({tokens_filtered/total_tokens*100:.1f}%)\")\n",
    "print(f\"   Tokens for spell check:   {tokens_remaining:,} ({tokens_remaining/total_tokens*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47475d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARISON BY LABEL:\n",
      "============================================================\n",
      "\n",
      "üö® SPAM (label=1): 278 messages\n",
      "----------------------------------------\n",
      "   Avg tokens:           40.1\n",
      "   Avg leet chars:       3.52\n",
      "   Avg teencode:         0.02\n",
      "   Avg visual leet:      0.33\n",
      "   Avg symbol leet:      0.10\n",
      "   Avg validated leet:  0.92\n",
      "   Avg weighted score:   0.4673\n",
      "   Avg separators:       7.45\n",
      "   Avg whitelist count:  5.53\n",
      "   Avg tokens to check:  34.60\n",
      "   Whitelist ratio:      13.8%\n",
      "\n",
      "‚úÖ HAM (label=0): 2,325 messages\n",
      "----------------------------------------\n",
      "   Avg tokens:           44.8\n",
      "   Avg leet chars:       5.35\n",
      "   Avg teencode:         0.00\n",
      "   Avg visual leet:      0.03\n",
      "   Avg symbol leet:      0.00\n",
      "   Avg validated leet:  0.54\n",
      "   Avg weighted score:   0.0369\n",
      "   Avg separators:       9.97\n",
      "   Avg whitelist count:  9.42\n",
      "   Avg tokens to check:  35.42\n",
      "   Whitelist ratio:      21.0%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPARISON BY LABEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä COMPARISON BY LABEL:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in [1, 0]:\n",
    "    subset = result_df[result_df['label'] == label]\n",
    "    label_name = \"SPAM\" if label == 1 else \"HAM\"\n",
    "    \n",
    "    print(f\"\\n{'üö®' if label == 1 else '‚úÖ'} {label_name} (label={label}): {len(subset):,} messages\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Avg tokens:           {subset['token_count'].mean():.1f}\")\n",
    "    print(f\"   Avg leet chars:       {subset['leet_count'].mean():.2f}\")\n",
    "    print(f\"   Avg teencode:         {subset['teencode_count'].mean():.2f}\")\n",
    "    print(f\"   Avg visual leet:      {subset['visual_leet_count'].mean():.2f}\")\n",
    "    print(f\"   Avg symbol leet:      {subset['symbol_leet_count'].mean():.2f}\")\n",
    "    print(f\"   Avg validated leet:  {subset['validated_leet_count'].mean():.2f}\")\n",
    "    print(f\"   Avg weighted score:   {subset['weighted_leet_score'].mean():.4f}\")\n",
    "    print(f\"   Avg separators:       {subset['separator_count'].mean():.2f}\")\n",
    "    print(f\"   Avg whitelist count:  {subset['whitelist_count'].mean():.2f}\")\n",
    "    print(f\"   Avg tokens to check:  {subset['tokens_to_check_count'].mean():.2f}\")\n",
    "    \n",
    "    # Whitelist ratio\n",
    "    total = subset['token_count'].sum()\n",
    "    filtered = subset['whitelist_count'].sum()\n",
    "    print(f\"   Whitelist ratio:      {filtered/total*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4fb247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2 ‚Üí Layer 3):\n",
      "================================================================================\n",
      "\n",
      "[0] Label: SPAM\n",
      "   Original:    [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõ...\n",
      "   L1 Masked:   [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõ...\n",
      "   L2 Tokens:   ['trung', 't√¢m', 'ph√≤ng', 'ch·ªëng', 'gian', 'l·∫≠n', 'ng√¢n', 'h√†ng', '√¥ng...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (5): ['<TIME>', '<MONEY>', 'tp', 'hcm', '<MONEY>']...\n",
      "   ‚úèÔ∏è  To check (83):   ['trung', 't√¢m', 'ph√≤ng', 'ch·ªëng', 'gian', 'l·∫≠n', 'ng√¢n', 'h...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Label: SPAM\n",
      "   Original:    [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB ...\n",
      "   L1 Masked:   [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB ...\n",
      "   L2 Tokens:   ['tb', 'tien', 'ich', 'loi', 'nhan', 'thoai', 'cua', 'viettel', 'quy',...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (9): ['tb', 'viettel', 'tb', '<PHONE>', '<TIME>', '<TIME>', '<TIM...\n",
      "   ‚úèÔ∏è  To check (188):   ['tien', 'ich', 'loi', 'nhan', 'thoai', 'cua', 'quy', 'khach...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] Label: SPAM\n",
      "   Original:    Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +19.56...\n",
      "   L1 Masked:   Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +<MONE...\n",
      "   L2 Tokens:   ['western', 'union', 'tb', 'vietcombank', '0071000986547', 'tr·∫ßn', 'th...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (12): ['tb', 'vietcombank', '0071000986547', 'ref', 't', '<MONEY>'...\n",
      "   ‚úèÔ∏è  To check (17):   ['western', 'union', 'tr·∫ßn', 'th·ªã', 'lan', 'nh·∫≠n', 'ngay', '...\n",
      "   üî§ Leet Info: Total=21, Teencode=0, Visual=0, Symbol=0, Validated=2, Weighted=0.00\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] Label: SPAM\n",
      "   Original:    B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M...\n",
      "   L1 Masked:   B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M...\n",
      "   L2 Tokens:   ['b·∫Øc', 't√†i', 'kho·∫£n', 't√†i', 'ch√≠nh', 'c·ªßa', 'b·∫°n', 'ƒë√£', 'ƒë∆∞·ª£c', 't...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (5): ['usdt', '<MONEY>', '50', '<URL>', '·ªü']...\n",
      "   ‚úèÔ∏è  To check (26):   ['b·∫Øc', 't√†i', 'kho·∫£n', 't√†i', 'ch√≠nh', 'c·ªßa', 'b·∫°n', 'ƒë√£', ...\n",
      "   üî§ Leet Info: Total=8, Teencode=0, Visual=0, Symbol=0, Validated=1, Weighted=0.00\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] Label: SPAM\n",
      "   Original:    [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung th...\n",
      "   L1 Masked:   [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung th...\n",
      "   L2 Tokens:   ['canh', 'cao', 'lan', 'cuoi', 'chung', 'toi', 'da', 'nhac', 'nho', 'c...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (8): ['o', 'b', '<TIME>', 'tt', 'o', 'b', 'lh', '<PHONE>']...\n",
      "   ‚úèÔ∏è  To check (60):   ['canh', 'cao', 'lan', 'cuoi', 'chung', 'toi', 'da', 'nhac',...\n",
      "   üî§ Leet Info: Total=2, Teencode=0, Visual=0, Symbol=0, Validated=0, Weighted=0.00\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAMPLE RESULTS - FULL PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2 ‚Üí Layer 3):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show samples with interesting whitelist filtering\n",
    "samples = result_df[result_df['whitelist_count'] > 0].head(5)\n",
    "\n",
    "for _, row in samples.iterrows():\n",
    "    print(f\"\\n[{row['index']}] Label: {'SPAM' if row['label']==1 else 'HAM'}\")\n",
    "    print(f\"   Original:    {row['original_content'][:70]}...\")\n",
    "    print(f\"   L1 Masked:   {row['layer1_masked'][:70]}...\")\n",
    "    print(f\"   L2 Tokens:   {row['layer2_tokens'][:70]}...\")\n",
    "    print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"   üîç Whitelisted ({row['whitelist_count']}): {row['whitelisted_tokens'][:60]}...\")\n",
    "    print(f\"   ‚úèÔ∏è  To check ({row['tokens_to_check_count']}):   {row['tokens_to_check'][:60]}...\")\n",
    "    \n",
    "    # Show leet information if available\n",
    "    if row['leet_count'] > 0:\n",
    "        print(f\"   üî§ Leet Info: Total={row['leet_count']}, Teencode={row['teencode_count']}, Visual={row['visual_leet_count']}, Symbol={row['symbol_leet_count']}, Validated={row['validated_leet_count']}, Weighted={row['weighted_leet_score']:.2f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf441fb",
   "metadata": {},
   "source": [
    "## Test Layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8267c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 12 items\n",
      "\n",
      "test_layer4.py::TestOOVDetection::test_valid_words_full_dict \u001b[32mPASSED\u001b[0m\u001b[32m      [  8%]\u001b[0m\n",
      "test_layer4.py::TestOOVDetection::test_valid_words_shadow_dict \u001b[32mPASSED\u001b[0m\u001b[32m    [ 16%]\u001b[0m\n",
      "test_layer4.py::TestOOVDetection::test_oov_word \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 25%]\u001b[0m\n",
      "test_layer4.py::TestOOVDetection::test_case_insensitive \u001b[32mPASSED\u001b[0m\u001b[32m           [ 33%]\u001b[0m\n",
      "test_layer4.py::TestOOVDetection::test_ignore_digits_and_short \u001b[32mPASSED\u001b[0m\u001b[32m    [ 41%]\u001b[0m\n",
      "test_layer4.py::TestAdvancedFeatures::test_broken_telex \u001b[32mPASSED\u001b[0m\u001b[32m           [ 50%]\u001b[0m\n",
      "test_layer4.py::TestAdvancedFeatures::test_gibberish \u001b[32mPASSED\u001b[0m\u001b[32m              [ 58%]\u001b[0m\n",
      "test_layer4.py::TestAdvancedFeatures::test_repeated_chars \u001b[32mPASSED\u001b[0m\u001b[32m         [ 66%]\u001b[0m\n",
      "test_layer4.py::TestAdvancedFeatures::test_run_on_words \u001b[32mPASSED\u001b[0m\u001b[32m           [ 75%]\u001b[0m\n",
      "test_layer4.py::TestMetrics::test_density_calculation \u001b[32mPASSED\u001b[0m\u001b[32m             [ 83%]\u001b[0m\n",
      "test_layer4.py::TestMetrics::test_density_with_ignored_tokens \u001b[32mPASSED\u001b[0m\u001b[32m     [ 91%]\u001b[0m\n",
      "test_layer4.py::TestMetrics::test_longest_oov \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m12 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 0\n"
     ]
    }
   ],
   "source": [
    "import test_layer4\n",
    "\n",
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer4.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer4.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3d46aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "‚úì Both dicts loaded: 78,258 words (full), 65,863 words (shadow) from c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\dicts\\words.txt\n",
      "‚úì Dictionary loaded: 78,258 words (full), 65,863 words (shadow)\n",
      "\n",
      "‚úì Layer 1: AggressiveMasker initialized\n",
      "‚úì Layer 2: TextNormalizer initialized\n",
      "‚úì Layer 3: WhitelistFilter initialized (173 whitelist items)\n",
      "‚úì Layer 4: MisspellExtractor initialized\n",
      "\n",
      "üîÑ Processing 2,603 rows through Layer 1 ‚Üí Layer 2 ‚Üí Layer 3 ‚Üí Layer 4...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer4_misspell_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL PIPELINE: LAYER 1 ‚Üí LAYER 2 ‚Üí LAYER 3 ‚Üí LAYER 4\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.misspell_detection.layer2_normalization import TextNormalizer\n",
    "from Smishing.misspell_detection.layer3_whitelist import WhitelistFilter\n",
    "from Smishing.misspell_detection.layer4_misspell import MisspellExtractor\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer4_misspell_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize all processors\n",
    "masker = AggressiveMasker()\n",
    "normalizer = TextNormalizer()\n",
    "whitelist_filter = WhitelistFilter()\n",
    "misspell_extractor = MisspellExtractor(full_dict=normalizer.full_dict, \n",
    "                                       shadow_dict=normalizer.shadow_dict)\n",
    "\n",
    "print(f\"\\n‚úì Layer 1: AggressiveMasker initialized\")\n",
    "print(f\"‚úì Layer 2: TextNormalizer initialized\")\n",
    "print(f\"‚úì Layer 3: WhitelistFilter initialized ({len(whitelist_filter.whitelist)} whitelist items)\")\n",
    "print(f\"‚úì Layer 4: MisspellExtractor initialized\")\n",
    "\n",
    "# Process all rows with FULL PIPELINE\n",
    "\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows through Layer 1 ‚Üí Layer 2 ‚Üí Layer 3 ‚Üí Layer 4...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "\n",
    "    try:\n",
    "        # ===== LAYER 1: MASKING =====\n",
    "        masked_text, mask_metadata = masker.mask(content)\n",
    "        mask_counts = masker.get_entity_counts(mask_metadata)\n",
    "        \n",
    "        # ===== LAYER 2: NORMALIZATION =====\n",
    "        norm_result = normalizer.normalize(masked_text)\n",
    "        tokens = norm_result.tokens\n",
    "        \n",
    "        # ===== LAYER 3: WHITELIST FILTERING =====\n",
    "        whitelist_result = whitelist_filter.filter(tokens)\n",
    "        tokens_to_check = whitelist_result.tokens_to_check\n",
    "        \n",
    "        # ===== LAYER 4: MISPELL EXTRACTION =====\n",
    "        misspell_result = misspell_extractor.extract(tokens_to_check)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {idx}: {e}\")\n",
    "        masked_text = content\n",
    "        norm_result = None\n",
    "        whitelist_result = None\n",
    "        misspell_result = None\n",
    "        tokens = []\n",
    "        tokens_to_check = []\n",
    "        mask_counts = {}\n",
    "        \n",
    "    # Build result row\n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        # Layer 1\n",
    "        \"layer1_masked\": masked_text,\n",
    "        \"url_count\": mask_counts.get(\"url\", 0) + mask_counts.get(\"zalo\", 0) + mask_counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": mask_counts.get(\"hotline\", 0) + mask_counts.get(\"landline\", 0) + \n",
    "                      mask_counts.get(\"mobile\", 0) + mask_counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": mask_counts.get(\"money\", 0),\n",
    "        \"code_count\": mask_counts.get(\"code\", 0),\n",
    "        # Layer 2\n",
    "        \"layer2_normalized\": norm_result.normalized_text if norm_result else \"\",\n",
    "        \"layer2_tokens\": str(tokens),\n",
    "        \"token_count\": len(tokens),\n",
    "        \"leet_count\": norm_result.leet_count if norm_result else 0,\n",
    "        \"teencode_count\": norm_result.teencode_count if norm_result else 0,\n",
    "        \"visual_leet_count\": norm_result.visual_leet_count if norm_result else 0,\n",
    "        \"symbol_leet_count\": norm_result.symbol_leet_count if norm_result else 0,\n",
    "        \"validated_leet_count\": norm_result.validated_leet_count if norm_result else 0,\n",
    "        \"weighted_leet_score\": norm_result.weighted_leet_score if norm_result else 0.0,\n",
    "        \"separator_count\": norm_result.separator_count if norm_result else 0,\n",
    "        # Layer 3\n",
    "        \"tokens_to_check\": str(whitelist_result.tokens_to_check) if whitelist_result else \"[]\",\n",
    "        \"whitelisted_tokens\": str(whitelist_result.whitelisted_tokens) if whitelist_result else \"[]\",\n",
    "        \"whitelist_count\": whitelist_result.whitelist_count if whitelist_result else 0,\n",
    "        \"tokens_to_check_count\": len(whitelist_result.tokens_to_check) if whitelist_result else 0,\n",
    "        # Layer 4\n",
    "        \"oov_count\": misspell_result.oov_count if misspell_result else 0,\n",
    "        \"oov_density\": misspell_result.oov_density if misspell_result else 0.0,\n",
    "        \"broken_telex_count\": misspell_result.broken_telex_count if misspell_result else 0,\n",
    "        \"longest_oov_length\": misspell_result.longest_oov_length if misspell_result else 0,\n",
    "        \"oov_tokens\": str(misspell_result.oov_tokens) if misspell_result else \"[]\",\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb41c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FULL PIPELINE SUMMARY STATISTICS:\n",
      "============================================================\n",
      "\n",
      "üîí LAYER 1 - ENTITY MASKING:\n",
      "   URLs detected:      1,192\n",
      "   Phones detected:    2,157\n",
      "   Money detected:     2,748\n",
      "   Codes detected:     1,140\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Total tokens:       115,391\n",
      "   Avg tokens/msg:     44.3\n",
      "   Total leet chars:   13,418\n",
      "   Teencode count:     8\n",
      "   Visual leet:       172\n",
      "   Symbol leet:       32\n",
      "   Validated leet:    1,507\n",
      "   Weighted score:    215.80\n",
      "   Total separators:   25,241\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   Total whitelist:    23,431\n",
      "   Total to check:     91,960\n",
      "   Avg whitelist/msg:  9.00\n",
      " Filltering_ratio:    20.3%\n",
      "\n",
      "üìã LAYER 4 - MISPELL EXTRACTION:\n",
      "   Total OOV tokens:          3,580\n",
      "   Avg OOV/msg:              1.38\n",
      "   Avg OOV density:          0.08\n",
      "   Total broken telex:       220\n",
      "   Max OOV length:           16\n",
      "   Messages with OOV:       1,421 (54.6\n",
      "\n",
      "üìà VALIDATION EFFICIENCY:\n",
      "   Total tokens checked:     91,960\n",
      "   OOV detected:             3,580 (3.89%)\n",
      "   Valid tokens:             88,380 (96.11%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 4 SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä FULL PIPELINE SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Layer 1 Stats\n",
    "print(\"\\nüîí LAYER 1 - ENTITY MASKING:\")\n",
    "print(f\"   URLs detected:      {result_df['url_count'].sum():,}\")\n",
    "print(f\"   Phones detected:    {result_df['phone_count'].sum():,}\")\n",
    "print(f\"   Money detected:     {result_df['money_count'].sum():,}\")\n",
    "print(f\"   Codes detected:     {result_df['code_count'].sum():,}\")\n",
    "\n",
    "# Layer 2 Stats\n",
    "print(f\"\\nüî§ LAYER 2 - NORMALIZATION:\")\n",
    "print(f\"   Total tokens:       {result_df['token_count'].sum():,}\")\n",
    "print(f\"   Avg tokens/msg:     {result_df['token_count'].mean():.1f}\")\n",
    "print(f\"   Total leet chars:   {result_df['leet_count'].sum():,}\")\n",
    "print(f\"   Teencode count:     {result_df['teencode_count'].sum():,}\")\n",
    "print(f\"   Visual leet:       {result_df['visual_leet_count'].sum():,}\")\n",
    "print(f\"   Symbol leet:       {result_df['symbol_leet_count'].sum():,}\")\n",
    "print(f\"   Validated leet:    {result_df['validated_leet_count'].sum():,}\")\n",
    "print(f\"   Weighted score:    {result_df['weighted_leet_score'].sum():.2f}\")\n",
    "print(f\"   Total separators:   {result_df['separator_count'].sum():,}\")\n",
    "\n",
    "# Layer 3 Stats\n",
    "print(f\"\\nüìã LAYER 3 - WHITELIST FILTERING:\")\n",
    "print(f\"   Total whitelist:    {result_df['whitelist_count'].sum():,}\")\n",
    "print(f\"   Total to check:     {result_df['tokens_to_check_count'].sum():,}\")\n",
    "print(f\"   Avg whitelist/msg:  {result_df['whitelist_count'].mean():.2f}\")\n",
    "print(f\" Filltering_ratio:    {result_df['whitelist_count'].sum() / result_df['token_count'].sum() * 100:.1f}%\")\n",
    "\n",
    "# Layer 4 Stats\n",
    "print(f\"\\nüìã LAYER 4 - MISPELL EXTRACTION:\")\n",
    "print(f\"   Total OOV tokens:          {result_df['oov_count'].sum():,}\")\n",
    "print(f\"   Avg OOV/msg:              {result_df['oov_count'].mean():.2f}\")\n",
    "print(f\"   Avg OOV density:          {result_df['oov_density'].mean():.2f}\")\n",
    "print(f\"   Total broken telex:       {result_df['broken_telex_count'].sum():,}\")\n",
    "print(f\"   Max OOV length:           {result_df['longest_oov_length'].max():,}\")\n",
    "print(f\"   Messages with OOV:       {(result_df['oov_count'] > 0).sum():,} ({(result_df['oov_count'] > 0).sum()/len(result_df)*100:.1f}\")\n",
    "\n",
    "\n",
    "# Validation efficiency\n",
    "total_checked = result_df['tokens_to_check_count'].sum()\n",
    "total_oov = result_df['oov_count'].sum()\n",
    "\n",
    "print(f\"\\nüìà VALIDATION EFFICIENCY:\")\n",
    "print(f\"   Total tokens checked:     {total_checked:,}\")\n",
    "print(f\"   OOV detected:             {total_oov:,} ({total_oov/total_checked*100:.2f}%)\")\n",
    "print(f\"   Valid tokens:             {total_checked - total_oov:,} ({100 - total_oov/total_checked*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb214e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARISON BY LABEL (FULL PIPELINE):\n",
      "============================================================\n",
      "\n",
      "üö® SMISHING (label=1): 278 messages\n",
      "----------------------------------------\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Avg tokens/msg:     40.1\n",
      "   Avg validated leet: 0.92\n",
      "   Avg weighted score: 0.4673\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   Avg whitelist/msg:  5.53\n",
      "   Avg tokens to check/msg:   34.60\n",
      "   Filltering_ratio:   13.8%\n",
      "\n",
      "üìã LAYER 4 - MISPELL EXTRACTION:\n",
      "   Avg OOV/msg:        2.29\n",
      "   Avg OOV density:    0.11\n",
      "   Avg broken telex:   0.16\n",
      "   Max OOV length:     16\n",
      "   Messages with OOV: 194 (69.8%)\n",
      "\n",
      "üîç COMBINED FEATURES:\n",
      "   Avg leet + OOV:      5.81\n",
      "  Avg weight + density: 0.5760\n",
      "\n",
      "‚úÖ LEGIT (label=0): 2,325 messages\n",
      "----------------------------------------\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Avg tokens/msg:     44.8\n",
      "   Avg validated leet: 0.54\n",
      "   Avg weighted score: 0.0369\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   Avg whitelist/msg:  9.42\n",
      "   Avg tokens to check/msg:   35.42\n",
      "   Filltering_ratio:   21.0%\n",
      "\n",
      "üìã LAYER 4 - MISPELL EXTRACTION:\n",
      "   Avg OOV/msg:        1.27\n",
      "   Avg OOV density:    0.07\n",
      "   Avg broken telex:   0.08\n",
      "   Max OOV length:     16\n",
      "   Messages with OOV: 1,227 (52.8%)\n",
      "\n",
      "üîç COMBINED FEATURES:\n",
      "   Avg leet + OOV:      6.62\n",
      "  Avg weight + density: 0.1096\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPARISON BY LABEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä COMPARISON BY LABEL (FULL PIPELINE):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in [1, 0]:\n",
    "    subset = result_df[result_df['label'] == label]\n",
    "    label_name = \"SMISHING\" if label == 1 else \"LEGIT\"\n",
    "    \n",
    "    print(f\"\\n{'üö®' if label == 1 else '‚úÖ'} {label_name} (label={label}): {len(subset):,} messages\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Layer 2 features\n",
    "    print(f\"\\nüî§ LAYER 2 - NORMALIZATION:\")\n",
    "    print(f\"   Avg tokens/msg:     {subset['token_count'].mean():.1f}\")\n",
    "    print(f\"   Avg validated leet: {subset['validated_leet_count'].mean():.2f}\")\n",
    "    print(f\"   Avg weighted score: {subset['weighted_leet_score'].mean():.4f}\")\n",
    "\n",
    "    # Layer 3 features\n",
    "    print(f\"\\nüìã LAYER 3 - WHITELIST FILTERING:\")\n",
    "    print(f\"   Avg whitelist/msg:  {subset['whitelist_count'].mean():.2f}\")\n",
    "    print(f\"   Avg tokens to check/msg:   {subset['tokens_to_check_count'].mean():.2f}\")\n",
    "    print(f\"   Filltering_ratio:   {subset['whitelist_count'].sum() / subset['token_count'].sum() * 100:.1f}%\")\n",
    "\n",
    "    # Layer 4 features\n",
    "    print(f\"\\nüìã LAYER 4 - MISPELL EXTRACTION:\")\n",
    "    print(f\"   Avg OOV/msg:        {subset['oov_count'].mean():.2f}\")\n",
    "    print(f\"   Avg OOV density:    {subset['oov_density'].mean():.2f}\")\n",
    "    print(f\"   Avg broken telex:   {subset['broken_telex_count'].mean():.2f}\")\n",
    "    print(f\"   Max OOV length:     {subset['longest_oov_length'].max():,}\")\n",
    "    print(f\"   Messages with OOV: {(subset['oov_count'] > 0).sum():,} ({(subset['oov_count'] > 0).sum()/len(subset)*100:.1f}%)\")\n",
    "\n",
    "    # Combined features\n",
    "    print(f\"\\nüîç COMBINED FEATURES:\")\n",
    "    print(f\"   Avg leet + OOV:      {subset['leet_count'].mean() + subset['oov_count'].mean():.2f}\")\n",
    "    print(f\"  Avg weight + density: {subset['weighted_leet_score'].mean() + subset['oov_density'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
