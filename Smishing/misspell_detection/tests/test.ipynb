{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a3b163",
   "metadata": {},
   "source": [
    "## Test 1 - Masking Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfdfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b64077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 39 items\n",
      "\n",
      "test_layer1.py::TestURLMasking::test_standard_url_with_https \u001b[32mPASSED\u001b[0m\u001b[32m      [  2%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_standard_url_with_www \u001b[32mPASSED\u001b[0m\u001b[32m        [  5%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_url_shortener_bitly \u001b[32mPASSED\u001b[0m\u001b[32m          [  7%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_aggressive_url_with_spaces \u001b[32mPASSED\u001b[0m\u001b[32m   [ 10%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_spam_tld_icu \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 12%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_spam_tld_vip \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 15%]\u001b[0m\n",
      "test_layer1.py::TestURLMasking::test_multiple_urls \u001b[32mPASSED\u001b[0m\u001b[32m                [ 17%]\u001b[0m\n",
      "test_layer1.py::TestZaloTelegramMasking::test_zalo_link \u001b[32mPASSED\u001b[0m\u001b[32m           [ 20%]\u001b[0m\n",
      "test_layer1.py::TestZaloTelegramMasking::test_telegram_link \u001b[32mPASSED\u001b[0m\u001b[32m       [ 23%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_mobile_standard \u001b[32mPASSED\u001b[0m\u001b[32m            [ 25%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_mobile_with_country_code \u001b[32mPASSED\u001b[0m\u001b[32m   [ 28%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_mobile_with_dots \u001b[32mPASSED\u001b[0m\u001b[32m           [ 30%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_hotline_1900 \u001b[32mPASSED\u001b[0m\u001b[32m               [ 33%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_hotline_with_spaces \u001b[32mPASSED\u001b[0m\u001b[32m        [ 35%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_landline \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "test_layer1.py::TestPhoneMasking::test_shortcode \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 41%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_k \u001b[32mPASSED\u001b[0m\u001b[32m               [ 43%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_trieu \u001b[32mPASSED\u001b[0m\u001b[32m           [ 46%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_vnd \u001b[32mPASSED\u001b[0m\u001b[32m             [ 48%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_money_with_dong \u001b[32mPASSED\u001b[0m\u001b[32m            [ 51%]\u001b[0m\n",
      "test_layer1.py::TestMoneyMasking::test_multiple_money \u001b[32mPASSED\u001b[0m\u001b[32m             [ 53%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_otp_6_digits \u001b[32mPASSED\u001b[0m\u001b[32m             [ 56%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_otp_4_digits \u001b[32mPASSED\u001b[0m\u001b[32m             [ 58%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_service_code \u001b[32mPASSED\u001b[0m\u001b[32m             [ 61%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_exclude_year \u001b[32mPASSED\u001b[0m\u001b[32m             [ 64%]\u001b[0m\n",
      "test_layer1.py::TestCodeOTPMasking::test_exclude_year_2000s \u001b[32mPASSED\u001b[0m\u001b[32m       [ 66%]\u001b[0m\n",
      "test_layer1.py::TestDateTimeMasking::test_date_dd_mm \u001b[32mPASSED\u001b[0m\u001b[32m              [ 69%]\u001b[0m\n",
      "test_layer1.py::TestDateTimeMasking::test_time_hh_mm \u001b[32mPASSED\u001b[0m\u001b[32m              [ 71%]\u001b[0m\n",
      "test_layer1.py::TestDateTimeMasking::test_duration_minutes \u001b[32mPASSED\u001b[0m\u001b[32m        [ 74%]\u001b[0m\n",
      "test_layer1.py::TestEmailMasking::test_standard_email \u001b[32mPASSED\u001b[0m\u001b[32m             [ 76%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_load_dataset \u001b[32mPASSED\u001b[0m\u001b[32m     [ 79%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_mask_real_samples \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_mask_batch_performance \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "test_layer1.py::TestIntegrationWithDataset::test_entity_counts \u001b[32mPASSED\u001b[0m\u001b[32m    [ 87%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_empty_string \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 89%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_none_handling \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 92%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_unicode_vietnamese \u001b[32mPASSED\u001b[0m\u001b[32m            [ 94%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_gibberish_text \u001b[32mPASSED\u001b[0m\u001b[32m                [ 97%]\u001b[0m\n",
      "test_layer1.py::TestEdgeCases::test_combined_entities \u001b[32mPASSED\u001b[0m\u001b[32m             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m39 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 0\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer1.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer1.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae0a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "\n",
      "üîÑ Processing 2,603 rows...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer1_masking_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer1_masking_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize masker\n",
    "masker = AggressiveMasker()\n",
    "\n",
    "# Process all rows\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "    \n",
    "    try:\n",
    "        masked_text, metadata = masker.mask(content)\n",
    "        counts = masker.get_entity_counts(metadata)\n",
    "    except Exception as e:\n",
    "        masked_text = f\"ERROR: {e}\"\n",
    "        metadata = {}\n",
    "        counts = {}\n",
    "    \n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        \"masked_content\": masked_text,\n",
    "        \"url_count\": counts.get(\"url\", 0) + counts.get(\"zalo\", 0) + counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": counts.get(\"hotline\", 0) + counts.get(\"landline\", 0) + \n",
    "                      counts.get(\"mobile\", 0) + counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": counts.get(\"money\", 0),\n",
    "        \"code_count\": counts.get(\"code\", 0),\n",
    "        \"email_count\": counts.get(\"email\", 0),\n",
    "        \"datetime_count\": counts.get(\"datetime\", 0),\n",
    "        \"raw_metadata\": str(metadata),\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040f8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SUMMARY STATISTICS:\n",
      "--------------------------------------------------\n",
      "   URLs detected:      1,395\n",
      "   Phones detected:    2,151\n",
      "   Money detected:     2,952\n",
      "   Codes detected:     1,087\n",
      "   Emails detected:    7\n",
      "   DateTimes detected: 2,742\n",
      "\n",
      "   Rows with entities: 2,310 / 2,603 (88.7%)\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   URLs detected:      {result_df['url_count'].sum():,}\")\n",
    "print(f\"   Phones detected:    {result_df['phone_count'].sum():,}\")\n",
    "print(f\"   Money detected:     {result_df['money_count'].sum():,}\")\n",
    "print(f\"   Codes detected:     {result_df['code_count'].sum():,}\")\n",
    "print(f\"   Emails detected:    {result_df['email_count'].sum():,}\")\n",
    "print(f\"   DateTimes detected: {result_df['datetime_count'].sum():,}\")\n",
    "\n",
    "# Rows with at least one entity\n",
    "has_entity = result_df[['url_count', 'phone_count', 'money_count', 'code_count']].sum(axis=1) > 0\n",
    "print(f\"\\n   Rows with entities: {has_entity.sum():,} / {len(result_df):,} ({has_entity.sum()/len(result_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c945a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE RESULTS (first 10 rows with changes):\n",
      "================================================================================\n",
      "\n",
      "[0] Label: 1\n",
      "   Original: [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõc 17h ng√†y h√¥m nay kh√¥ng thanh...\n",
      "   Masked:   [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõc <TIME> ng√†y h√¥m nay kh√¥ng th...\n",
      "   Counts:   URL=0, Phone=0, Money=2, Code=0\n",
      "\n",
      "[1] Label: 1\n",
      "   Original: [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB 0848836182 vao luc 08:09 27/03...\n",
      "   Masked:   [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB <PHONE> vao luc <TIME> <TIME>....\n",
      "   Counts:   URL=0, Phone=2, Money=1, Code=0\n",
      "\n",
      "[2] Label: 1\n",
      "   Original: Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +19.56 USD. Nh·∫≠n 500.000 VND. Ngay 0...\n",
      "   Masked:   Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +<MONEY>. Nh·∫≠n <MONEY>. Ngay <TIME>....\n",
      "   Counts:   URL=1, Phone=0, Money=2, Code=0\n",
      "\n",
      "[3] Label: 1\n",
      "   Original: B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M·∫≠t kh·∫©u: yk6698 USDT S·ªë d∆∞: 1,...\n",
      "   Masked:   B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M·∫≠t kh·∫©u: yk6698 USDT S·ªë d∆∞: <M...\n",
      "   Counts:   URL=1, Phone=0, Money=1, Code=0\n",
      "\n",
      "[4] Label: 1\n",
      "   Original: [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung thai do cua O/b va nhung nguoi l...\n",
      "   Masked:   [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung thai do cua O/b va nhung nguoi l...\n",
      "   Counts:   URL=0, Phone=1, Money=0, Code=0\n",
      "\n",
      "[5] Label: 1\n",
      "   Original: Nhan thay co hanh vi LOI DUNG TIN NHIEM, CHIEM DOAT TAI SAN. yc tt gap truoc 13g 16/2/2025, neu van ...\n",
      "   Masked:   Nhan thay co hanh vi LOI DUNG TIN NHIEM, CHIEM DOAT TAI SAN. yc tt gap truoc 13g <TIME>, neu van bat...\n",
      "   Counts:   URL=0, Phone=1, Money=0, Code=0\n",
      "\n",
      "[11] Label: 1\n",
      "   Original: ACB: Tai khoan cua ban da mo dich vu tai chinh toan cau phi dich vu hang thang la 2.000.000VND se bi...\n",
      "   Masked:   ACB: Tai khoan cua ban da mo dich vu tai chinh toan cau phi dich vu hang thang la <MONEY> se bi tru ...\n",
      "   Counts:   URL=1, Phone=0, Money=1, Code=0\n",
      "\n",
      "[12] Label: 1\n",
      "   Original: Vietinbank tran trong thong bao tai khoan cua quy khach hien tai da bi khoa. Dang nhap qua http://ww...\n",
      "   Masked:   Vietinbank tran trong thong bao tai khoan cua quy khach hien tai da bi khoa. Dang nhap qua <URL> de ...\n",
      "   Counts:   URL=1, Phone=0, Money=0, Code=0\n",
      "\n",
      "[13] Label: 1\n",
      "   Original: Nh·ªù nh·∫Øn l·∫°i Tr·∫ßn VƒÉn ƒê·ª©c 24/05/2004. SƒêT: 0912345678, ra thanh to√°n kho·∫£n n·ª£ 2.040.000 ngay l·∫≠p t·ª©c...\n",
      "   Masked:   Nh·ªù nh·∫Øn l·∫°i Tr·∫ßn VƒÉn ƒê·ª©c <TIME>. SƒêT: <PHONE>, ra thanh to√°n kho·∫£n n·ª£ <MONEY> ngay l·∫≠p t·ª©c tr∆∞·ªõc kh...\n",
      "   Counts:   URL=0, Phone=2, Money=1, Code=0\n",
      "\n",
      "[14] Label: 0\n",
      "   Original: Em H√† day. Goi lai vao so 0978123456 em co viec gap....\n",
      "   Masked:   Em H√† day. Goi lai vao so <PHONE> em co viec gap....\n",
      "   Counts:   URL=0, Phone=1, Money=0, Code=0\n"
     ]
    }
   ],
   "source": [
    "# Show sample results\n",
    "print(\"\\nüìã SAMPLE RESULTS (first 10 rows with changes):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter rows that have changes\n",
    "changed_rows = result_df[result_df['original_content'] != result_df['masked_content']]\n",
    "\n",
    "for _, row in changed_rows.head(10).iterrows():\n",
    "    print(f\"\\n[{row['index']}] Label: {row['label']}\")\n",
    "    print(f\"   Original: {row['original_content'][:100]}...\")\n",
    "    print(f\"   Masked:   {row['masked_content'][:100]}...\")\n",
    "    print(f\"   Counts:   URL={row['url_count']}, Phone={row['phone_count']}, Money={row['money_count']}, Code={row['code_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcc702",
   "metadata": {},
   "source": [
    "## Test Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e60aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a85d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 46 items\n",
      "\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_0_to_o \u001b[31mFAILED\u001b[0m\u001b[31m             [  2%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_1_to_i \u001b[32mPASSED\u001b[0m\u001b[31m             [  4%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_3_to_e \u001b[32mPASSED\u001b[0m\u001b[31m             [  6%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_digit_4_to_a \u001b[32mPASSED\u001b[0m\u001b[31m             [  8%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_symbol_exclamation_to_i \u001b[32mPASSED\u001b[0m\u001b[31m  [ 10%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_symbol_at_to_a \u001b[32mPASSED\u001b[0m\u001b[31m           [ 13%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_symbol_dollar_to_s \u001b[32mPASSED\u001b[0m\u001b[31m       [ 15%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_char_j_to_i \u001b[32mPASSED\u001b[0m\u001b[31m              [ 17%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_char_f_to_ph \u001b[32mPASSED\u001b[0m\u001b[31m             [ 19%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_combined \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 21%]\u001b[0m\n",
      "test_layer2.py::TestLeetspeak::test_leet_heavy_spam \u001b[32mPASSED\u001b[0m\u001b[31m               [ 23%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_apostrophe \u001b[32mPASSED\u001b[0m\u001b[31m  [ 26%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_dash \u001b[32mPASSED\u001b[0m\u001b[31m        [ 28%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_tilde \u001b[32mPASSED\u001b[0m\u001b[31m       [ 30%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_colon \u001b[32mPASSED\u001b[0m\u001b[31m       [ 32%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_dot \u001b[32mPASSED\u001b[0m\u001b[31m         [ 34%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_multiple \u001b[32mPASSED\u001b[0m\u001b[31m    [ 36%]\u001b[0m\n",
      "test_layer2.py::TestSeparatorCleaning::test_separator_complex_spam \u001b[32mPASSED\u001b[0m\u001b[31m [ 39%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_url_tag \u001b[32mPASSED\u001b[0m\u001b[31m           [ 41%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_phone_tag \u001b[32mPASSED\u001b[0m\u001b[31m         [ 43%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_money_tag \u001b[32mPASSED\u001b[0m\u001b[31m         [ 45%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_code_tag \u001b[32mPASSED\u001b[0m\u001b[31m          [ 47%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_app_link_tag \u001b[32mPASSED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_multiple_tags \u001b[32mPASSED\u001b[0m\u001b[31m     [ 52%]\u001b[0m\n",
      "test_layer2.py::TestTagProtection::test_protect_tag_with_leet_around \u001b[32mPASSED\u001b[0m\u001b[31m [ 54%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_simple \u001b[32mPASSED\u001b[0m\u001b[31m            [ 56%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_lowercase \u001b[32mPASSED\u001b[0m\u001b[31m         [ 58%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_mixed_case \u001b[32mPASSED\u001b[0m\u001b[31m        [ 60%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_with_vietnamese_chars \u001b[32mPASSED\u001b[0m\u001b[31m [ 63%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_preserves_tag_case \u001b[32mPASSED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_tokenize_filters_numbers \u001b[32mPASSED\u001b[0m\u001b[31m   [ 67%]\u001b[0m\n",
      "test_layer2.py::TestTokenization::test_normalized_text_reconstruction \u001b[32mPASSED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_empty_string \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 71%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_none_handling \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 73%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_only_separators \u001b[32mPASSED\u001b[0m\u001b[31m               [ 76%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_unicode_normalization \u001b[32mPASSED\u001b[0m\u001b[31m         [ 78%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_very_long_text \u001b[32mPASSED\u001b[0m\u001b[31m                [ 80%]\u001b[0m\n",
      "test_layer2.py::TestEdgeCases::test_gibberish \u001b[32mPASSED\u001b[0m\u001b[31m                     [ 82%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_1 \u001b[31mFAILED\u001b[0m\u001b[31m  [ 84%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_2 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 86%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_3 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 89%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_spam_sample_4 \u001b[32mPASSED\u001b[0m\u001b[31m  [ 91%]\u001b[0m\n",
      "test_layer2.py::TestIntegrationRealData::test_real_ham_sample \u001b[32mPASSED\u001b[0m\u001b[31m     [ 93%]\u001b[0m\n",
      "test_layer2.py::TestNormalizationResult::test_result_has_all_fields \u001b[32mPASSED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "test_layer2.py::TestNormalizationResult::test_result_original_preserved \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "test_layer2.py::TestNormalizationResult::test_result_types \u001b[32mPASSED\u001b[0m\u001b[31m        [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____________________ TestLeetspeak.test_leet_digit_0_to_o _____________________\u001b[0m\n",
      "\n",
      "self = <test_layer2.TestLeetspeak object at 0x0000028342E074D0>\n",
      "normalizer = <Smishing.misspell_detection.layer2_normalization.TextNormalizer object at 0x0000028342BAB770>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_leet_digit_0_to_o\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, normalizer):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"0 ‚Üí o\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        text = \u001b[33m\"\u001b[39;49;00m\u001b[33mkh0ng\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result = normalizer.normalize(text)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mkhong\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result.leet_count >= \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_layer2.py\u001b[0m:46: AssertionError\n",
      "\u001b[31m\u001b[1m______________________ TestLeetspeak.test_leet_combined _______________________\u001b[0m\n",
      "\n",
      "self = <test_layer2.TestLeetspeak object at 0x0000028342D236B0>\n",
      "normalizer = <Smishing.misspell_detection.layer2_normalization.TextNormalizer object at 0x0000028342579350>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_leet_combined\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, normalizer):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Multiple leet chars in one text\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        text = \u001b[33m\"\u001b[39;49;00m\u001b[33mkh0ng co d!eu k!en\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result = normalizer.normalize(text)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mkhong\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdieu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mkien\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result.leet_count >= \u001b[94m3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_layer2.py\u001b[0m:106: AssertionError\n",
      "\u001b[31m\u001b[1m_______________ TestIntegrationRealData.test_real_spam_sample_1 _______________\u001b[0m\n",
      "\n",
      "self = <test_layer2.TestIntegrationRealData object at 0x0000028342E68190>\n",
      "normalizer = <Smishing.misspell_detection.layer2_normalization.TextNormalizer object at 0x0000028342ECBED0>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_real_spam_sample_1\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, normalizer):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Sample spam th·ª±c t·∫ø #1\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        text = \u001b[33m\"\u001b[39;49;00m\u001b[33mOng/(Ba) da du d!eu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mk!en NHAN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTIEN h0 tro\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result = normalizer.normalize(text)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Verify decoded correctly\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdieu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mkien\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mho\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m result.normalized_text\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result.leet_count >= \u001b[94m3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_layer2.py\u001b[0m:353: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_layer2.py::\u001b[1mTestLeetspeak::test_leet_digit_0_to_o\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m test_layer2.py::\u001b[1mTestLeetspeak::test_leet_combined\u001b[0m - AssertionError\n",
      "\u001b[31mFAILED\u001b[0m test_layer2.py::\u001b[1mTestIntegrationRealData::test_real_spam_sample_1\u001b[0m - AssertionError\n",
      "\u001b[31m======================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m43 passed\u001b[0m\u001b[31m in 0.48s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 1\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer1.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer2.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b9fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "\n",
      "üîÑ Processing 2,603 rows through Layer 1 + Layer 2...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer2_normalization_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 2: APPLY NORMALIZATION TO DATASET\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.misspell_detection.layer2_normalization import TextNormalizer\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer2_normalization_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize processors\n",
    "masker = AggressiveMasker()\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "# Process all rows with FULL PIPELINE: Layer 1 ‚Üí Layer 2\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows through Layer 1 + Layer 2...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "    \n",
    "    try:\n",
    "        # Layer 1: Masking\n",
    "        masked_text, mask_metadata = masker.mask(content)\n",
    "        mask_counts = masker.get_entity_counts(mask_metadata)\n",
    "        \n",
    "        # Layer 2: Normalization (on masked text)\n",
    "        norm_result = normalizer.normalize(masked_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        masked_text = f\"ERROR: {e}\"\n",
    "        mask_counts = {}\n",
    "        norm_result = None\n",
    "    \n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        \"layer1_masked\": masked_text,\n",
    "        \"layer2_normalized\": norm_result.normalized_text if norm_result else \"\",\n",
    "        \"layer2_tokens\": str(norm_result.tokens) if norm_result else \"[]\",\n",
    "        \"token_count\": len(norm_result.tokens) if norm_result else 0,\n",
    "        \"leet_count\": norm_result.leet_count if norm_result else 0,\n",
    "        \"leet_word_count\": norm_result.leet_word_count if norm_result else 0,\n",
    "        \"leet_density\": norm_result.leet_density if norm_result else 0.0,\n",
    "        \"leet_words\": json.dumps(norm_result.leet_words, ensure_ascii=False) if norm_result and norm_result.leet_words else \"[]\",\n",
    "        \"leet_patterns_used\": json.dumps(norm_result.leet_patterns_used, ensure_ascii=False) if norm_result and norm_result.leet_patterns_used else \"{}\",\n",
    "        \"separator_count\": norm_result.separator_count if norm_result else 0,\n",
    "        # Layer 1 counts\n",
    "        \"url_count\": mask_counts.get(\"url\", 0) + mask_counts.get(\"zalo\", 0) + mask_counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": mask_counts.get(\"hotline\", 0) + mask_counts.get(\"landline\", 0) + \n",
    "                      mask_counts.get(\"mobile\", 0) + mask_counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": mask_counts.get(\"money\", 0),\n",
    "        \"code_count\": mask_counts.get(\"code\", 0),\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd7cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LAYER 2 SUMMARY STATISTICS:\n",
      "============================================================\n",
      "\n",
      "üìù TOKEN STATISTICS:\n",
      "   Total tokens:      114,904\n",
      "   Avg tokens/row:    44.1\n",
      "   Max tokens/row:    197\n",
      "\n",
      "üî§ LEET DETECTION:\n",
      "   Total leet chars:  19\n",
      "   Rows with leet:    18 (0.7%)\n",
      "   Avg leet/row:      0.01\n",
      "   Total leet words:  19\n",
      "   Avg leet words/row: 0.01\n",
      "   Avg leet density:  0.0001\n",
      "\n",
      "üìå SEPARATOR DETECTION:\n",
      "   Total separators:  24,360\n",
      "   Rows with sep:     2,541 (97.6%)\n",
      "\n",
      "üìà COMPARISON BY LABEL:\n",
      "------------------------------------------------------------\n",
      "\n",
      "   SPAM (label=1): 278 rows\n",
      "      Avg tokens:    40.0\n",
      "      Avg leet:      0.06\n",
      "      Avg leet words: 0.06\n",
      "      Avg leet density: 0.0005\n",
      "      Avg separator: 7.21\n",
      "\n",
      "   HAM (label=0): 2,325 rows\n",
      "      Avg tokens:    44.6\n",
      "      Avg leet:      0.00\n",
      "      Avg leet words: 0.00\n",
      "      Avg leet density: 0.0000\n",
      "      Avg separator: 9.62\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 2: SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä LAYER 2 SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic counts\n",
    "print(f\"\\nüìù TOKEN STATISTICS:\")\n",
    "print(f\"   Total tokens:      {result_df['token_count'].sum():,}\")\n",
    "print(f\"   Avg tokens/row:    {result_df['token_count'].mean():.1f}\")\n",
    "print(f\"   Max tokens/row:    {result_df['token_count'].max()}\")\n",
    "\n",
    "print(f\"\\nüî§ LEET DETECTION:\")\n",
    "print(f\"   Total leet chars:  {result_df['leet_count'].sum():,}\")\n",
    "print(f\"   Rows with leet:    {(result_df['leet_count'] > 0).sum():,} ({(result_df['leet_count'] > 0).sum()/len(result_df)*100:.1f}%)\")\n",
    "print(f\"   Avg leet/row:      {result_df['leet_count'].mean():.2f}\")\n",
    "print(f\"   Total leet words:  {result_df['leet_word_count'].sum():,}\")\n",
    "print(f\"   Avg leet words/row: {result_df['leet_word_count'].mean():.2f}\")\n",
    "print(f\"   Avg leet density:  {result_df['leet_density'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nüìå SEPARATOR DETECTION:\")\n",
    "print(f\"   Total separators:  {result_df['separator_count'].sum():,}\")\n",
    "print(f\"   Rows with sep:     {(result_df['separator_count'] > 0).sum():,} ({(result_df['separator_count'] > 0).sum()/len(result_df)*100:.1f}%)\")\n",
    "\n",
    "# By label comparison\n",
    "print(f\"\\nüìà COMPARISON BY LABEL:\")\n",
    "print(\"-\" * 60)\n",
    "for label in result_df['label'].unique():\n",
    "    subset = result_df[result_df['label'] == label]\n",
    "    label_name = \"SPAM\" if label == 1 else \"HAM\"\n",
    "    print(f\"\\n   {label_name} (label={label}): {len(subset):,} rows\")\n",
    "    print(f\"      Avg tokens:    {subset['token_count'].mean():.1f}\")\n",
    "    print(f\"      Avg leet:      {subset['leet_count'].mean():.2f}\")\n",
    "    print(f\"      Avg leet words: {subset['leet_word_count'].mean():.2f}\")\n",
    "    print(f\"      Avg leet density: {subset['leet_density'].mean():.4f}\")\n",
    "    print(f\"      Avg separator: {subset['separator_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08e2ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2):\n",
      "================================================================================\n",
      "\n",
      "[16] Label: SPAM\n",
      "   Original:   Th0ng ba0:BIDV nang cap he thong. Vui l0ng dang nhap https://b0dv.xyz va nang ca...\n",
      "   L1 Masked:  Th0ng ba0:BIDV nang cap he thong. Vui l0ng dang nhap <URL> va nang cap. Neu kh0n...\n",
      "   L2 Normalized: thong bao bidv nang cap he thong vui long dang nhap <URL> va nang cap neu khong ...\n",
      "   Leet: 1, Leet Words: 1, Density: 0.0093, Sep: 4, Tokens: 23\n",
      "   Leet Words Found: 1\n",
      "      - 'ba0' -> 'bao' (1 leet chars)\n",
      "----------------------------------------\n",
      "\n",
      "[29] Label: SPAM\n",
      "   Original:   LENH TRUY NA: Can cu tai lieu thu thap duoc, ngay 09/03/2022 Co quan canh sat di...\n",
      "   L1 Masked:  LENH TRUY NA: Can cu tai lieu thu thap duoc, ngay <TIME> Co quan canh sat dieu t...\n",
      "   L2 Normalized: lenh truy na can cu tai lieu thu thap duoc ngay <TIME> co quan canh sat dieu tra...\n",
      "   Leet: 1, Leet Words: 1, Density: 0.0034, Sep: 6, Tokens: 64\n",
      "   Leet Words Found: 1\n",
      "      - 'N0' -> 'No' (1 leet chars)\n",
      "----------------------------------------\n",
      "\n",
      "[35] Label: SPAM\n",
      "   Original:   Ong(Ba) da du d!eu k!en NHAN T1EN h0 tro tu quy BHTN. Bam vao www.mvndc.icude la...\n",
      "   L1 Masked:  Ong(Ba) da du d!eu k!en NHAN T1EN h0 tro tu quy BHTN. Bam vao www.mvndc.icude la...\n",
      "   L2 Normalized: ong ba da du dieu kien nhan tien ho tro tu quy bhtn bam vao www mvndc icude lay ...\n",
      "   Leet: 1, Leet Words: 1, Density: 0.0083, Sep: 7, Tokens: 27\n",
      "   Leet Words Found: 1\n",
      "      - 'h0' -> 'ho' (1 leet chars)\n",
      "----------------------------------------\n",
      "\n",
      "[55] Label: SPAM\n",
      "   Original:   Em la nu sinh vien 21 tuoi MU0N KIEM chut Tjen trang TRAI CU0C S0NG EM C0 the de...\n",
      "   L1 Masked:  Em la nu sinh vien 21 tuoi MU0N KIEM chut Tjen trang TRAI CU0C S0NG EM <CODE> th...\n",
      "   L2 Normalized: em la nu sinh vien 21 tuoi muon kiem chut tien trang trai cuoc song em <CODE> th...\n",
      "   Leet: 1, Leet Words: 1, Density: 0.0065, Sep: 1, Tokens: 35\n",
      "   Leet Words Found: 1\n",
      "      - 'ZAL0' -> 'ZALo' (1 leet chars)\n",
      "----------------------------------------\n",
      "\n",
      "[56] Label: SPAM\n",
      "   Original:   Anh co can GAI LAM tjnh K0? E di lam them kiem tien tieu vat EM 20 T DANG NG0N, ...\n",
      "   L1 Masked:  Anh co can GAI LAM tjnh K0? E di lam them kiem tien tieu vat EM 20 T DANG NG0N, ...\n",
      "   L2 Normalized: anh co can gai lam tinh ko e di lam them kiem tien tieu vat em 20 t dang ngon bo...\n",
      "   Leet: 1, Leet Words: 1, Density: 0.0069, Sep: 2, Tokens: 32\n",
      "   Leet Words Found: 1\n",
      "      - 'K0' -> 'Ko' (1 leet chars)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAMPLE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\nüìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show samples with leet detected\n",
    "leet_samples = result_df[result_df['leet_count'] > 0].head(5)\n",
    "\n",
    "for _, row in leet_samples.iterrows():\n",
    "    print(f\"\\n[{row['index']}] Label: {'SPAM' if row['label']==1 else 'HAM'}\")\n",
    "    print(f\"   Original:   {row['original_content'][:80]}...\")\n",
    "    print(f\"   L1 Masked:  {row['layer1_masked'][:80]}...\")\n",
    "    print(f\"   L2 Normalized: {row['layer2_normalized'][:80]}...\")\n",
    "    print(f\"   Leet: {row['leet_count']}, Leet Words: {row['leet_word_count']}, Density: {row['leet_density']:.4f}, Sep: {row['separator_count']}, Tokens: {row['token_count']}\")\n",
    "    \n",
    "    # Show leet words if available\n",
    "    if row['leet_words'] and row['leet_words'] != \"[]\":\n",
    "        try:\n",
    "            leet_words = json.loads(row['leet_words'])\n",
    "            if leet_words:\n",
    "                print(f\"   Leet Words Found: {len(leet_words)}\")\n",
    "                for i, word_info in enumerate(leet_words[:3]):  # Show first 3\n",
    "                    print(f\"      - '{word_info.get('original', '')}' -> '{word_info.get('decoded', '')}' ({word_info.get('leet_chars', 0)} leet chars)\")\n",
    "        except:\n",
    "            pass\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45e6e8",
   "metadata": {},
   "source": [
    "## Test Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db32c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- c:\\Python313\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\n",
      "plugins: anyio-4.12.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 44 items\n",
      "\n",
      "test_layer3.py::TestBrandFiltering::test_bank_brand_vcb \u001b[32mPASSED\u001b[0m\u001b[32m           [  2%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_bank_brand_bidv \u001b[32mPASSED\u001b[0m\u001b[32m          [  4%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_bank_brand_vietinbank \u001b[32mPASSED\u001b[0m\u001b[32m    [  6%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_ewallet_brand_momo \u001b[32mPASSED\u001b[0m\u001b[32m       [  9%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_telco_brand_viettel \u001b[32mPASSED\u001b[0m\u001b[32m      [ 11%]\u001b[0m\n",
      "test_layer3.py::TestBrandFiltering::test_app_brand_tiktok \u001b[32mPASSED\u001b[0m\u001b[32m         [ 13%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_otp \u001b[32mPASSED\u001b[0m\u001b[32m              [ 15%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_sim \u001b[32mPASSED\u001b[0m\u001b[32m              [ 18%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_4g \u001b[32mPASSED\u001b[0m\u001b[32m               [ 20%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_digibank \u001b[32mPASSED\u001b[0m\u001b[32m         [ 22%]\u001b[0m\n",
      "test_layer3.py::TestJargonFiltering::test_jargon_usdt \u001b[32mPASSED\u001b[0m\u001b[32m             [ 25%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_lh \u001b[32mPASSED\u001b[0m\u001b[32m      [ 27%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_tk \u001b[32mPASSED\u001b[0m\u001b[32m      [ 29%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_cskh \u001b[32mPASSED\u001b[0m\u001b[32m    [ 31%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_abbr_bhtn \u001b[32mPASSED\u001b[0m\u001b[32m    [ 34%]\u001b[0m\n",
      "test_layer3.py::TestSlangAbbreviationFiltering::test_teencode_ko \u001b[32mPASSED\u001b[0m\u001b[32m  [ 36%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_url_uppercase \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_phone_uppercase \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_money_lowercase \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "test_layer3.py::TestEntityTokenFiltering::test_entity_app_link \u001b[32mPASSED\u001b[0m\u001b[32m    [ 45%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check1_in_whitelist_set \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check2_pure_digit \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check3_entity_tag \u001b[32mPASSED\u001b[0m\u001b[32m   [ 52%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check4_short_token \u001b[32mPASSED\u001b[0m\u001b[32m  [ 54%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_check5_special_chars_only \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "test_layer3.py::TestIsWhitelistedChecks::test_vietnamese_word_not_whitelisted \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_mixed_tokens \u001b[32mPASSED\u001b[0m\u001b[32m        [ 61%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_all_whitelisted \u001b[32mPASSED\u001b[0m\u001b[32m     [ 63%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_none_whitelisted \u001b[32mPASSED\u001b[0m\u001b[32m    [ 65%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_whitelist_count \u001b[32mPASSED\u001b[0m\u001b[32m     [ 68%]\u001b[0m\n",
      "test_layer3.py::TestFilterMethod::test_filter_original_tokens_preserved \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_has_all_fields \u001b[32mPASSED\u001b[0m\u001b[32m   [ 72%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_types \u001b[32mPASSED\u001b[0m\u001b[32m            [ 75%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_empty_input \u001b[32mPASSED\u001b[0m\u001b[32m      [ 77%]\u001b[0m\n",
      "test_layer3.py::TestWhitelistResult::test_result_is_dataclass \u001b[32mPASSED\u001b[0m\u001b[32m     [ 79%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_empty_list \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 81%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_whitespace_token \u001b[32mPASSED\u001b[0m\u001b[32m              [ 84%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_case_insensitive \u001b[32mPASSED\u001b[0m\u001b[32m              [ 86%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_mixed_alphanumeric \u001b[32mPASSED\u001b[0m\u001b[32m            [ 88%]\u001b[0m\n",
      "test_layer3.py::TestEdgeCases::test_unicode_vietnamese \u001b[32mPASSED\u001b[0m\u001b[32m            [ 90%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_smishing_tokens_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_smishing_tokens_2 \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_smishing_tokens_3 \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "test_layer3.py::TestIntegrationWithLayer2::test_real_ham_tokens \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m44 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "K·∫øt qu·∫£ pytest: 0\n"
     ]
    }
   ],
   "source": [
    "import test_layer3\n",
    "\n",
    "# Ch·∫°y to√†n b·ªô tests c·ªßa test_layer3.py b·∫±ng pytest\n",
    "import pytest\n",
    "\n",
    "result = pytest.main([\"test_layer3.py\", \"-v\"])\n",
    "print(\"K·∫øt qu·∫£ pytest:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523002d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: c:\\IE403\\IE403_DoAnCuoiKy\\data\\dataset.csv\n",
      "‚úÖ Loaded 2,603 rows from dataset.csv (standard parser)\n",
      "‚úÖ Loaded 2,603 rows\n",
      "\n",
      "‚úì Layer 1: AggressiveMasker initialized\n",
      "‚úì Layer 2: TextNormalizer initialized\n",
      "‚úì Layer 3: WhitelistFilter initialized (124 whitelist items)\n",
      "\n",
      "üîÑ Processing 2,603 rows through Layer 1 ‚Üí Layer 2 ‚Üí Layer 3...\n",
      "   Processed 500 / 2,603 rows...\n",
      "   Processed 1,000 / 2,603 rows...\n",
      "   Processed 1,500 / 2,603 rows...\n",
      "   Processed 2,000 / 2,603 rows...\n",
      "   Processed 2,500 / 2,603 rows...\n",
      "\n",
      "‚úÖ Results saved to: c:\\IE403\\IE403_DoAnCuoiKy\\Smishing\\misspell_detection\\tests\\layer3_whitelist_results.csv\n",
      "   Total rows: 2,603\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL PIPELINE: LAYER 1 ‚Üí LAYER 2 ‚Üí LAYER 3\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path.cwd().parent.parent.parent  # IE403_DoAnCuoiKy/\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "from Smishing.preprocessing.layer1_masking import AggressiveMasker\n",
    "from Smishing.misspell_detection.layer2_normalization import TextNormalizer\n",
    "from Smishing.misspell_detection.layer3_whitelist import WhitelistFilter\n",
    "from Smishing.data_loader import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = ROOT_DIR / \"data\" / \"dataset.csv\"\n",
    "OUTPUT_PATH = Path.cwd() / \"layer3_whitelist_results.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset from: {DATA_PATH}\")\n",
    "df = load_dataset(DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "\n",
    "# Initialize all processors\n",
    "masker = AggressiveMasker()\n",
    "normalizer = TextNormalizer()\n",
    "whitelist_filter = WhitelistFilter()\n",
    "\n",
    "print(f\"\\n‚úì Layer 1: AggressiveMasker initialized\")\n",
    "print(f\"‚úì Layer 2: TextNormalizer initialized\")\n",
    "print(f\"‚úì Layer 3: WhitelistFilter initialized ({len(whitelist_filter.whitelist)} whitelist items)\")\n",
    "\n",
    "# Process all rows with FULL PIPELINE\n",
    "print(f\"\\nüîÑ Processing {len(df):,} rows through Layer 1 ‚Üí Layer 2 ‚Üí Layer 3...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    content = str(row.get(\"content\", \"\"))\n",
    "    label = row.get(\"label\", \"\")\n",
    "    \n",
    "    try:\n",
    "        # ===== LAYER 1: MASKING =====\n",
    "        masked_text, mask_metadata = masker.mask(content)\n",
    "        mask_counts = masker.get_entity_counts(mask_metadata)\n",
    "        \n",
    "        # ===== LAYER 2: NORMALIZATION =====\n",
    "        norm_result = normalizer.normalize(masked_text)\n",
    "        tokens = norm_result.tokens\n",
    "        \n",
    "        # ===== LAYER 3: WHITELIST FILTERING =====\n",
    "        whitelist_result = whitelist_filter.filter(tokens)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {idx}: {e}\")\n",
    "        masked_text = content\n",
    "        norm_result = None\n",
    "        whitelist_result = None\n",
    "        tokens = []\n",
    "        mask_counts = {}\n",
    "    \n",
    "    # Build result row\n",
    "    result = {\n",
    "        \"index\": idx,\n",
    "        \"label\": label,\n",
    "        \"original_content\": content,\n",
    "        # Layer 1\n",
    "        \"layer1_masked\": masked_text,\n",
    "        \"url_count\": mask_counts.get(\"url\", 0) + mask_counts.get(\"zalo\", 0) + mask_counts.get(\"telegram\", 0),\n",
    "        \"phone_count\": mask_counts.get(\"hotline\", 0) + mask_counts.get(\"landline\", 0) + \n",
    "                      mask_counts.get(\"mobile\", 0) + mask_counts.get(\"shortcode\", 0),\n",
    "        \"money_count\": mask_counts.get(\"money\", 0),\n",
    "        \"code_count\": mask_counts.get(\"code\", 0),\n",
    "        # Layer 2\n",
    "        \"layer2_normalized\": norm_result.normalized_text if norm_result else \"\",\n",
    "        \"layer2_tokens\": str(tokens),\n",
    "        \"token_count\": len(tokens),\n",
    "        \"leet_count\": norm_result.leet_count if norm_result else 0,\n",
    "        \"leet_word_count\": norm_result.leet_word_count if norm_result else 0,\n",
    "        \"leet_density\": norm_result.leet_density if norm_result else 0.0,\n",
    "        \"leet_words\": json.dumps(norm_result.leet_words, ensure_ascii=False) if norm_result and norm_result.leet_words else \"[]\",\n",
    "        \"leet_patterns_used\": json.dumps(norm_result.leet_patterns_used, ensure_ascii=False) if norm_result and norm_result.leet_patterns_used else \"{}\",\n",
    "        \"separator_count\": norm_result.separator_count if norm_result else 0,\n",
    "        # Layer 3\n",
    "        \"tokens_to_check\": str(whitelist_result.tokens_to_check) if whitelist_result else \"[]\",\n",
    "        \"whitelisted_tokens\": str(whitelist_result.whitelisted_tokens) if whitelist_result else \"[]\",\n",
    "        \"whitelist_count\": whitelist_result.whitelist_count if whitelist_result else 0,\n",
    "        \"tokens_to_check_count\": len(whitelist_result.tokens_to_check) if whitelist_result else 0,\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"   Processed {idx + 1:,} / {len(df):,} rows...\")\n",
    "\n",
    "# Save results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {OUTPUT_PATH}\")\n",
    "print(f\"   Total rows: {len(result_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e1a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FULL PIPELINE SUMMARY STATISTICS:\n",
      "============================================================\n",
      "\n",
      "üîí LAYER 1 - ENTITY MASKING:\n",
      "   URLs detected:      1,395\n",
      "   Phones detected:    2,151\n",
      "   Money detected:     2,952\n",
      "   Codes detected:     1,087\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Total tokens:       114,904\n",
      "   Avg tokens/msg:     44.1\n",
      "   Total leet chars:   19\n",
      "   Total leet words:   19\n",
      "   Avg leet words/msg: 0.01\n",
      "   Avg leet density:   0.0001\n",
      "   Total separators:   24,360\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   Total whitelist:    20,789\n",
      "   Total to check:     94,115\n",
      "   Avg whitelist/msg:  7.99\n",
      "   Avg to check/msg:   36.16\n",
      "\n",
      "üìà FILTERING EFFICIENCY:\n",
      "   Total tokens input:       114,904\n",
      "   Tokens filtered out:      20,789 (18.1%)\n",
      "   Tokens for spell check:   94,115 (81.9%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LAYER 3 SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä FULL PIPELINE SUMMARY STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Layer 1 Stats\n",
    "print(\"\\nüîí LAYER 1 - ENTITY MASKING:\")\n",
    "print(f\"   URLs detected:      {result_df['url_count'].sum():,}\")\n",
    "print(f\"   Phones detected:    {result_df['phone_count'].sum():,}\")\n",
    "print(f\"   Money detected:     {result_df['money_count'].sum():,}\")\n",
    "print(f\"   Codes detected:     {result_df['code_count'].sum():,}\")\n",
    "\n",
    "# Layer 2 Stats\n",
    "print(f\"\\nüî§ LAYER 2 - NORMALIZATION:\")\n",
    "print(f\"   Total tokens:       {result_df['token_count'].sum():,}\")\n",
    "print(f\"   Avg tokens/msg:     {result_df['token_count'].mean():.1f}\")\n",
    "print(f\"   Total leet chars:   {result_df['leet_count'].sum():,}\")\n",
    "print(f\"   Total leet words:   {result_df['leet_word_count'].sum():,}\")\n",
    "print(f\"   Avg leet words/msg: {result_df['leet_word_count'].mean():.2f}\")\n",
    "print(f\"   Avg leet density:   {result_df['leet_density'].mean():.4f}\")\n",
    "print(f\"   Total separators:   {result_df['separator_count'].sum():,}\")\n",
    "\n",
    "# Layer 3 Stats\n",
    "print(f\"\\nüìã LAYER 3 - WHITELIST FILTERING:\")\n",
    "print(f\"   Total whitelist:    {result_df['whitelist_count'].sum():,}\")\n",
    "print(f\"   Total to check:     {result_df['tokens_to_check_count'].sum():,}\")\n",
    "print(f\"   Avg whitelist/msg:  {result_df['whitelist_count'].mean():.2f}\")\n",
    "print(f\"   Avg to check/msg:   {result_df['tokens_to_check_count'].mean():.2f}\")\n",
    "\n",
    "# Filtering ratio\n",
    "total_tokens = result_df['token_count'].sum()\n",
    "tokens_filtered = result_df['whitelist_count'].sum()\n",
    "tokens_remaining = result_df['tokens_to_check_count'].sum()\n",
    "\n",
    "print(f\"\\nüìà FILTERING EFFICIENCY:\")\n",
    "print(f\"   Total tokens input:       {total_tokens:,}\")\n",
    "print(f\"   Tokens filtered out:      {tokens_filtered:,} ({tokens_filtered/total_tokens*100:.1f}%)\")\n",
    "print(f\"   Tokens for spell check:   {tokens_remaining:,} ({tokens_remaining/total_tokens*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47475d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARISON BY LABEL:\n",
      "============================================================\n",
      "\n",
      "üö® SPAM (label=1): 278 messages\n",
      "----------------------------------------\n",
      "   Avg tokens:           40.0\n",
      "   Avg leet chars:       0.06\n",
      "   Avg leet words:       0.06\n",
      "   Avg leet density:     0.0005\n",
      "   Avg separators:       7.21\n",
      "   Avg whitelist count:  5.27\n",
      "   Avg tokens to check:  34.75\n",
      "   Whitelist ratio:      13.2%\n",
      "\n",
      "‚úÖ HAM (label=0): 2,325 messages\n",
      "----------------------------------------\n",
      "   Avg tokens:           44.6\n",
      "   Avg leet chars:       0.00\n",
      "   Avg leet words:       0.00\n",
      "   Avg leet density:     0.0000\n",
      "   Avg separators:       9.62\n",
      "   Avg whitelist count:  8.31\n",
      "   Avg tokens to check:  36.32\n",
      "   Whitelist ratio:      18.6%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPARISON BY LABEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä COMPARISON BY LABEL:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in [1, 0]:\n",
    "    subset = result_df[result_df['label'] == label]\n",
    "    label_name = \"SPAM\" if label == 1 else \"HAM\"\n",
    "    \n",
    "    print(f\"\\n{'üö®' if label == 1 else '‚úÖ'} {label_name} (label={label}): {len(subset):,} messages\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Avg tokens:           {subset['token_count'].mean():.1f}\")\n",
    "    print(f\"   Avg leet chars:       {subset['leet_count'].mean():.2f}\")\n",
    "    print(f\"   Avg leet words:       {subset['leet_word_count'].mean():.2f}\")\n",
    "    print(f\"   Avg leet density:     {subset['leet_density'].mean():.4f}\")\n",
    "    print(f\"   Avg separators:       {subset['separator_count'].mean():.2f}\")\n",
    "    print(f\"   Avg whitelist count:  {subset['whitelist_count'].mean():.2f}\")\n",
    "    print(f\"   Avg tokens to check:  {subset['tokens_to_check_count'].mean():.2f}\")\n",
    "    \n",
    "    # Whitelist ratio\n",
    "    total = subset['token_count'].sum()\n",
    "    filtered = subset['whitelist_count'].sum()\n",
    "    print(f\"   Whitelist ratio:      {filtered/total*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4fb247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2 ‚Üí Layer 3):\n",
      "================================================================================\n",
      "\n",
      "[0] Label: SPAM\n",
      "   Original:    [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõ...\n",
      "   L1 Masked:   [TRUNG T√ÇM PH√íNG CH·ªêNG GIAN L·∫¨N NG√ÇN H√ÄNG] √îng/B√† Nguy·ªÖn VƒÉn Minh Tr∆∞·ªõ...\n",
      "   L2 Tokens:   ['trung', 't√¢m', 'ph√≤ng', 'ch·ªëng', 'gian', 'l·∫≠n', 'ng√¢n', 'h√†ng', '√¥ng...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (5): ['<TIME>', '<MONEY>', 'tp', 'hcm', '<MONEY>']...\n",
      "   ‚úèÔ∏è  To check (83):   ['trung', 't√¢m', 'ph√≤ng', 'ch·ªëng', 'gian', 'l·∫≠n', 'ng√¢n', 'h...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Label: SPAM\n",
      "   Original:    [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB ...\n",
      "   L1 Masked:   [TB] Tien ich Loi nhan thoai cua Viettel: Quy khach co loi nhan tu TB ...\n",
      "   L2 Tokens:   ['tb', 'tien', 'ich', 'loi', 'nhan', 'thoai', 'cua', 'viettel', 'quy',...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (9): ['tb', 'viettel', 'tb', '<PHONE>', '<TIME>', '<TIME>', '<TIM...\n",
      "   ‚úèÔ∏è  To check (188):   ['tien', 'ich', 'loi', 'nhan', 'thoai', 'cua', 'quy', 'khach...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] Label: SPAM\n",
      "   Original:    Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +19.56...\n",
      "   L1 Masked:   Western Union TB: Vietcombank: 0071000986547. Tr·∫ßn Th·ªã Lan. Ref +<MONE...\n",
      "   L2 Tokens:   ['western', 'union', 'tb', 'vietcombank', '0071000986547', 'tr·∫ßn', 'th...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (11): ['tb', 'vietcombank', '0071000986547', 'ref', '<MONEY>', '<M...\n",
      "   ‚úèÔ∏è  To check (17):   ['western', 'union', 'tr·∫ßn', 'th·ªã', 'lan', 'nh·∫≠n', 'ngay', '...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] Label: SPAM\n",
      "   Original:    B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M...\n",
      "   L1 Masked:   B·∫Øc, t√†i kho·∫£n t√†i ch√≠nh c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c th√™m v√†o. T√†i kho·∫£n: Nay128 M...\n",
      "   L2 Tokens:   ['b·∫Øc', 't√†i', 'kho·∫£n', 't√†i', 'ch√≠nh', 'c·ªßa', 'b·∫°n', 'ƒë√£', 'ƒë∆∞·ª£c', 't...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (5): ['usdt', '<MONEY>', '50', '<URL>', '·ªü']...\n",
      "   ‚úèÔ∏è  To check (26):   ['b·∫Øc', 't√†i', 'kho·∫£n', 't√†i', 'ch√≠nh', 'c·ªßa', 'b·∫°n', 'ƒë√£', ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] Label: SPAM\n",
      "   Original:    [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung th...\n",
      "   L1 Masked:   [CANH CAO LAN CUOI]:Chung toi da nhac nho, canh cao nhieu lan nhung th...\n",
      "   L2 Tokens:   ['canh', 'cao', 'lan', 'cuoi', 'chung', 'toi', 'da', 'nhac', 'nho', 'c...\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üîç Whitelisted (8): ['o', 'b', '<TIME>', 'tt', 'o', 'b', 'lh', '<PHONE>']...\n",
      "   ‚úèÔ∏è  To check (60):   ['canh', 'cao', 'lan', 'cuoi', 'chung', 'toi', 'da', 'nhac',...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAMPLE RESULTS - FULL PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\nüìã SAMPLE RESULTS (Layer 1 ‚Üí Layer 2 ‚Üí Layer 3):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show samples with interesting whitelist filtering\n",
    "samples = result_df[result_df['whitelist_count'] > 0].head(5)\n",
    "\n",
    "for _, row in samples.iterrows():\n",
    "    print(f\"\\n[{row['index']}] Label: {'SPAM' if row['label']==1 else 'HAM'}\")\n",
    "    print(f\"   Original:    {row['original_content'][:70]}...\")\n",
    "    print(f\"   L1 Masked:   {row['layer1_masked'][:70]}...\")\n",
    "    print(f\"   L2 Tokens:   {row['layer2_tokens'][:70]}...\")\n",
    "    print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"   üîç Whitelisted ({row['whitelist_count']}): {row['whitelisted_tokens'][:60]}...\")\n",
    "    print(f\"   ‚úèÔ∏è  To check ({row['tokens_to_check_count']}):   {row['tokens_to_check'][:60]}...\")\n",
    "    \n",
    "    # Show leet information if available\n",
    "    if row['leet_count'] > 0:\n",
    "        print(f\"   üî§ Leet Info: {row['leet_count']} chars, {row['leet_word_count']} words, density: {row['leet_density']:.4f}\")\n",
    "        if row['leet_words'] and row['leet_words'] != \"[]\":\n",
    "            try:\n",
    "                leet_words = json.loads(row['leet_words'])\n",
    "                if leet_words:\n",
    "                    print(f\"      Leet words: {', '.join([f\"\\'{w.get('original', '')}\\'‚Üí\\'{w.get('decoded', '')}\\'\" for w in leet_words[:3]])}\")\n",
    "            except:\n",
    "                pass\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523b8864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üì® ORIGINAL INPUT:\n",
      "   VCB: T√†i kho·∫£n c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t OTP. Truy c·∫≠p https://vcb-fake.com\n",
      "\n",
      "üîí LAYER 1 - MASKING:\n",
      "   VCB: T√†i kho·∫£n c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t OTP. Truy c·∫≠p <URL>\n",
      "   Entities: {'url': ['https://vcb-fake.com']}\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Text: vcb t√†i kho·∫£n c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t otp truy c·∫≠p <URL>\n",
      "   Tokens: ['vcb', 't√†i', 'kho·∫£n', 'c·ªßa', 'b·∫°n', 'ƒë√£', 'ƒë∆∞·ª£c', 'k√≠ch', 'ho·∫°t', 'otp', 'truy', 'c·∫≠p', '<URL>']\n",
      "   Leet: 0, Sep: 2\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   ‚úÖ Whitelisted (3): ['vcb', 'otp', '<URL>']\n",
      "   ‚úèÔ∏è  To check (10):    ['t√†i', 'kho·∫£n', 'c·ªßa', 'b·∫°n', 'ƒë√£', 'ƒë∆∞·ª£c', 'k√≠ch', 'ho·∫°t', 'truy', 'c·∫≠p']\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üì® ORIGINAL INPUT:\n",
      "   Th0ng ba0: BIDV nang cap he thong. Vui l0ng dang nhap ngay!\n",
      "\n",
      "üîí LAYER 1 - MASKING:\n",
      "   Th0ng ba0: BIDV nang cap he thong. Vui l0ng dang nhap ngay!\n",
      "   Entities: {}\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Text: thong bao bidv nang cap he thong vui long dang nhap ngay\n",
      "   Tokens: ['thong', 'bao', 'bidv', 'nang', 'cap', 'he', 'thong', 'vui', 'long', 'dang', 'nhap', 'ngay']\n",
      "   Leet: 1, Sep: 3\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   ‚úÖ Whitelisted (1): ['bidv']\n",
      "   ‚úèÔ∏è  To check (11):    ['thong', 'bao', 'nang', 'cap', 'he', 'thong', 'vui', 'long', 'dang', 'nhap', 'ngay']\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üì® ORIGINAL INPUT:\n",
      "   Ch√†o b·∫°n, d·∫°o n√†y kh·ªèe kh√¥ng? L√¢u r·ªìi kh√¥ng g·∫∑p.\n",
      "\n",
      "üîí LAYER 1 - MASKING:\n",
      "   Ch√†o b·∫°n, d·∫°o n√†y kh·ªèe kh√¥ng? L√¢u r·ªìi kh√¥ng g·∫∑p.\n",
      "   Entities: {}\n",
      "\n",
      "üî§ LAYER 2 - NORMALIZATION:\n",
      "   Text: ch√†o b·∫°n d·∫°o n√†y kh·ªèe kh√¥ng l√¢u r·ªìi kh√¥ng g·∫∑p\n",
      "   Tokens: ['ch√†o', 'b·∫°n', 'd·∫°o', 'n√†y', 'kh·ªèe', 'kh√¥ng', 'l√¢u', 'r·ªìi', 'kh√¥ng', 'g·∫∑p']\n",
      "   Leet: 0, Sep: 2\n",
      "\n",
      "üìã LAYER 3 - WHITELIST FILTERING:\n",
      "   ‚úÖ Whitelisted (0): []\n",
      "   ‚úèÔ∏è  To check (10):    ['ch√†o', 'b·∫°n', 'd·∫°o', 'n√†y', 'kh·ªèe', 'kh√¥ng', 'l√¢u', 'r·ªìi', 'kh√¥ng', 'g·∫∑p']\n",
      "======================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VISUAL: SINGLE MESSAGE FLOW\n",
    "# ============================================================\n",
    "\n",
    "def show_pipeline_flow(text, masker, normalizer, whitelist_filter):\n",
    "    \"\"\"Hi·ªÉn th·ªã chi ti·∫øt flow x·ª≠ l√Ω 1 message\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üì® ORIGINAL INPUT:\")\n",
    "    print(f\"   {text}\")\n",
    "    \n",
    "    # Layer 1\n",
    "    masked, meta = masker.mask(text)\n",
    "    print(\"\\nüîí LAYER 1 - MASKING:\")\n",
    "    print(f\"   {masked}\")\n",
    "    print(f\"   Entities: {meta}\")\n",
    "    \n",
    "    # Layer 2\n",
    "    norm = normalizer.normalize(masked)\n",
    "    print(\"\\nüî§ LAYER 2 - NORMALIZATION:\")\n",
    "    print(f\"   Text: {norm.normalized_text}\")\n",
    "    print(f\"   Tokens: {norm.tokens}\")\n",
    "    print(f\"   Leet: {norm.leet_count}, Sep: {norm.separator_count}\")\n",
    "    \n",
    "    # Layer 3\n",
    "    result = whitelist_filter.filter(norm.tokens)\n",
    "    print(\"\\nüìã LAYER 3 - WHITELIST FILTERING:\")\n",
    "    print(f\"   ‚úÖ Whitelisted ({result.whitelist_count}): {result.whitelisted_tokens}\")\n",
    "    print(f\"   ‚úèÔ∏è  To check ({len(result.tokens_to_check)}):    {result.tokens_to_check}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Test v·ªõi m·ªôt s·ªë samples\n",
    "test_messages = [\n",
    "    \"VCB: T√†i kho·∫£n c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t OTP. Truy c·∫≠p https://vcb-fake.com\",\n",
    "    \"Th0ng ba0: BIDV nang cap he thong. Vui l0ng dang nhap ngay!\",\n",
    "    \"Ch√†o b·∫°n, d·∫°o n√†y kh·ªèe kh√¥ng? L√¢u r·ªìi kh√¥ng g·∫∑p.\",\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    show_pipeline_flow(msg, masker, normalizer, whitelist_filter)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
